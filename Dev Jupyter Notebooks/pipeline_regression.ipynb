{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as cp\n",
    "import torch\n",
    "import hcp_utils as hcp # https://rmldj.github.io/hcp-utils/\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matlab.engine\n",
    "import os\n",
    "import json\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.linalg import eigh, svd, logm\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from nilearn import image as nimg\n",
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "from pyriemann.utils.tangentspace import tangent_space, untangent_space, log_map_riemann, unupper\n",
    "from pyriemann.utils.distance import distance_riemann, distance\n",
    "from pyriemann.utils.base import logm, expm\n",
    "from concurrent.futures import ProcessPoolExecutor, TimeoutError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your settings\n",
    "settings = {\n",
    "    \"phenotype\": \"ReadEng_AgeAdj\",\n",
    "    \"percentile\": 0.2,\n",
    "    \"outputfolder\": \"regression_pmat\",\n",
    "    \"n_folds\": 5,\n",
    "    \"TanSVM_C\": 1,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"Tangent_Class\": True,\n",
    "    \"metric\": \"logeuclid\"\n",
    "}\n",
    "\n",
    "# Ensure the output folder exists\n",
    "outputfolder = settings[\"outputfolder\"]\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)\n",
    "\n",
    "# Define the path for the settings file\n",
    "settings_filepath = os.path.join(outputfolder, \"settings.json\")\n",
    "\n",
    "# Save the settings to a JSON file\n",
    "with open(settings_filepath, \"w\") as f:\n",
    "    json.dump(settings, f, indent=4)\n",
    "\n",
    "print(f\"Settings have been saved to {settings_filepath}\")\n",
    "# Define the output folder\n",
    "phenotype = settings[\"phenotype\"]\n",
    "percentile = settings[\"percentile\"]\n",
    "n_folds = settings[\"n_folds\"]\n",
    "TanSVM_C = settings[\"TanSVM_C\"]\n",
    "random_state = settings[\"random_state\"]\n",
    "n_filters_per_group = settings[\"n_filters_per_group\"]\n",
    "Tangent_Class = settings[\"Tangent_Class\"]\n",
    "# Pyriemannian Mean https://github.com/pyRiemann/pyRiemann/blob/master/pyriemann/utils/mean.py#L633 Metric for mean estimation, can be: \"ale\", \"alm\", \"euclid\", \"harmonic\", \"identity\", \"kullback_sym\", \"logdet\", \"logeuclid\", \"riemann\", \"wasserstein\", or a callable function.\n",
    "# https://link.springer.com/article/10.1007/s12021-020-09473-9 <---- best descriptions/plots\n",
    "# Geometric means in a novel vector space structure on symmetric positive-definite matrices <https://epubs.siam.org/doi/abs/10.1137/050637996?journalCode=sjmael>`_\n",
    "metric = settings[\"metric\"]\n",
    "\n",
    "def load_array_from_outputfolder(filename):\n",
    "    filepath = os.path.join(outputfolder, filename)\n",
    "    return np.load(filepath)\n",
    "# Function to save an array to the output folder\n",
    "def save_array_to_outputfolder(filename, array):\n",
    "    filepath = os.path.join(outputfolder, filename)\n",
    "    np.save(filepath, array)\n",
    "\n",
    "def save_text_results(text, filename=\"results.txt\"):\n",
    "    \"\"\"Save text results to a file.\"\"\"\n",
    "    filepath = os.path.join(outputfolder, filename)\n",
    "    with open(filepath, \"a\") as f:  # Using 'a' to append results to the file\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory and Processor Usage/Limits Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\n",
    "#Open terminal for job\n",
    "# srun --jobid=68974 --overlap --pty /bin/bash \n",
    "\n",
    "# #SLURM RAM\n",
    "!cgget -r memory.limit_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "#SLURM VM\n",
    "!cgget -r memory.memsw.limit_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "#SLURM USAGE\n",
    "!cgget -r memory.memsw.usage_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "!echo \"SLURM_JOB_ID: $SLURM_JOB_ID\"\n",
    "!echo \"SLURM_JOB_NAME: $SLURM_JOB_NAME\"\n",
    "!echo \"SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST\"\n",
    "!echo \"SLURM_MEM_PER_NODE: $SLURM_MEM_PER_NODE\"\n",
    "!echo \"SLURM_CPUS_ON_NODE: $SLURM_CPUS_ON_NODE\"\n",
    "!echo \"SLURM_MEM_PER_CPU: $SLURM_MEM_PER_CPU\"\n",
    "\n",
    "!free -h\n",
    "\n",
    "import resource\n",
    "\n",
    "# Get the soft and hard limits of virtual memory (address space)\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "print(f\"Soft limit: {soft / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Hard limit: {hard / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "# Get the soft and hard limits of the data segment (physical memory usage)\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_DATA)\n",
    "print(f\"Soft limit: {soft / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Hard limit: {hard / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "#TORQUE Virtual Memory\n",
    "# !cgget -r memory.memsw.limit_in_bytes /torque/$PBS_JOBID\n",
    "\n",
    "# #TORQUE RAM\n",
    "# !cgget -r memory.limit_in_bytes /torque/$PBS_JOBID\n",
    "\n",
    "# #TORQUE USAGE\n",
    "# !cgget -r memory.memsw.usage_in_bytes /torque/$PBS_JOBID\n",
    "# print(int(os.environ['PBS_NP']))\n",
    "!nvidia-smi\n",
    "\n",
    "def gpu_mem():\n",
    "    # Memory usage information\n",
    "    print(f\"Total memory available: {(torch.cuda.get_device_properties('cuda').total_memory / 1024**3):.2f} GB\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "def cpu_mem():\n",
    "   # Display memory information\n",
    "    print(f\"Total Memory: { psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available Memory: { psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used Memory: { psutil.virtual_memory().used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory Usage: { psutil.virtual_memory().percent}%\")\n",
    "\n",
    "gpu_mem()\n",
    "cpu_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Paths & Parcellated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupA_parcellated_array = load_array_from_outputfolder(\"groupA_parcellated_array.npy\")\n",
    "groupB_parcellated_array = load_array_from_outputfolder(\"groupB_parcellated_array.npy\")\n",
    "groupA_paths_filtered = load_array_from_outputfolder(\"groupA_paths_filtered.npy\")\n",
    "groupB_paths_filtered = load_array_from_outputfolder(\"groupB_paths_filtered.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def analyze_residuals(y_pred, y_true):\n",
    "    residuals = y_true - y_pred\n",
    "\n",
    "    # Manually calculate R^2 score using y_true mean\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)  # Sum of squares of residuals\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares (mean of y_true)\n",
    "    r_squared = 1 - (ss_res / ss_tot)  # R^2 score\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(y_pred, residuals, color=\"blue\", label=\"Residuals\")\n",
    "    plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "    plt.title(f\"Residuals vs Fitted Values (R^2 = {r_squared})\")\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(y_true, residuals, color=\"red\", label=\"Residuals\")\n",
    "    plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "    plt.title(f\"Residuals vs True Values (R^2 = {r_squared})\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    fig = sm.graphics.qqplot(residuals, dist=stats.norm, fit=True, line=\"45\")\n",
    "    plt.title(\"QQ Plot of Standardized Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', color='purple')\n",
    "    plt.title(\"Histogram of Residuals\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    return r_squared\n",
    "\n",
    "\n",
    "def FKT_proj(data, filters, method=\"basic\", alpha=1, beta=0, l1_ratio=0.5, lambda1=.01, lambda2=.01):\n",
    "    S = (data @ filters)\n",
    "    \n",
    "    if method == \"basic\":\n",
    "        proj = (np.linalg.pinv(S)@ data)\n",
    "    elif method == \"covs\":\n",
    "        cov_est_scm = Covariances(estimator='scm')\n",
    "        s_cov = cov_est_scm.transform(S.T[np.newaxis,:,:])[0,:,:]\n",
    "        data_cov = cov_est_scm.transform(data.T[np.newaxis,:,:])[0,:,:]\n",
    "        proj = (data_cov @ filters @ np.linalg.inv(s_cov)).T\n",
    "    elif method == \"linreg\":\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(S, data)\n",
    "        proj = reg.coef_.T\n",
    "    elif method == \"grouplassolinreg\":\n",
    "        reg = MultiTaskLasso(alpha=alpha)  # Using 5-fold cross-validation\n",
    "        reg.fit(S, data)\n",
    "        proj = reg.coef_.T\n",
    "    elif method == \"lassolinreg\":\n",
    "        reg = Lasso(alpha=alpha)  # Using 5-fold cross-validation\n",
    "        reg.fit(S, data)\n",
    "        proj = reg.coef_.T\n",
    "    elif method == \"elasticlinreg\":\n",
    "        reg = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        reg.fit(S, data)\n",
    "        proj = reg.coef_.T\n",
    "    elif method == \"growl\":\n",
    "        # Proximal Operator for GrOWL targeting columns\n",
    "        def prox_growl(V, lambda1, lambda2, tau):\n",
    "            p, r = V.shape\n",
    "            norms = np.linalg.norm(V, axis=0)  # Norms of columns\n",
    "            indices = np.argsort(-norms)  # Sort indices by descending norms\n",
    "            weights = lambda1 + lambda2 * np.linspace(1, 0, r)  # Weights decrease\n",
    "            V_new = np.zeros_like(V)\n",
    "            for i in range(r):\n",
    "                idx = indices[i]\n",
    "                if norms[idx] > weights[i] * tau:\n",
    "                    V_new[:, idx] = (1 - tau * weights[i] / norms[idx]) * V[:, idx]\n",
    "            return V_new\n",
    "        \n",
    "        # Initialization\n",
    "        B = np.zeros((filters.shape[1], data.shape[1]))\n",
    "        \n",
    "        # Optimization Loop\n",
    "        max_iter = 100\n",
    "        learning_rate = 0.01\n",
    "        for _ in range(max_iter):\n",
    "            gradient = S.T @ (S @ B - data)\n",
    "            B -= learning_rate * gradient\n",
    "            B = prox_growl(B, lambda1, lambda2, tau=learning_rate)\n",
    "            if np.linalg.norm(gradient) < 1e-1:\n",
    "                break\n",
    "        \n",
    "        proj = B.T\n",
    "    \n",
    "    return proj\n",
    "\n",
    "# Combine group A and B data and paths\n",
    "def combine_groups(groupA_parcellated_array, groupB_parcellated_array, groupA_paths_filtered, groupB_paths_filtered):\n",
    "    # Combine data arrays\n",
    "    combined_data = np.concatenate((groupA_parcellated_array, groupB_parcellated_array), axis=0)\n",
    "\n",
    "    # Combine paths arrays\n",
    "    combined_paths = np.concatenate((groupA_paths_filtered, groupB_paths_filtered), axis=0)\n",
    "\n",
    "    return combined_data, combined_paths\n",
    "\n",
    "# Extract subject IDs from the combined paths\n",
    "def extract_subject_ids(combined_paths):\n",
    "    subject_ids = np.array([re.search(r'/(\\d+)/', path[0]).group(1) for path in combined_paths])\n",
    "    return np.array(subject_ids)\n",
    "\n",
    "def extract_phenotype(subids,phenotype):\n",
    "    file_path_restricted = '../HCP/RESTRICTED_zainsou_8_6_2024_2_11_21.csv'\n",
    "    file_path_unrestricted = '../HCP/unrestricted_zainsou_8_2_2024_6_13_22.csv'\n",
    "\n",
    "    try:\n",
    "        data_r = pd.read_csv(file_path_restricted)\n",
    "        data_ur = pd.read_csv(file_path_unrestricted)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path_restricted} or {file_path_unrestricted}\")\n",
    "        raise\n",
    "\n",
    "    # Combine restricted and unrestricted data on Subject ID\n",
    "    data = pd.merge(data_r, data_ur, on='Subject', how='outer')\n",
    "\n",
    "    # Convert Subject IDs to string for consistency\n",
    "    data['Subject'] = data['Subject'].astype(str)\n",
    "    subids = subids.astype(str)\n",
    "\n",
    "    # Filter data for training subjects\n",
    "    train_data = data[data['Subject'].isin(subids)]\n",
    "    # Ensure the order matches the training data\n",
    "    train_data = train_data.set_index('Subject').loc[subids].reset_index()\n",
    "    pheno_score = train_data[phenotype]\n",
    "    return pheno_score\n",
    "\n",
    "# Regress out age from predictors\n",
    "def regress_out_age(predictor, age):\n",
    "    reg = Ridge(alpha=1)\n",
    "    # reg = LinearRegression()\n",
    "\n",
    "    reg.fit(age.reshape(-1, 1), predictor)  # Age is the independent variable\n",
    "\n",
    "    # reg = LinearSVR(C=1,fit_intercept=False)\n",
    "    # reg.fit(predictor, age)  # Age is the independent variable\n",
    "\n",
    "    return reg\n",
    "\n",
    "def preproc(train, test,method=\"zscore\"):\n",
    "    if method == \"zscore\":\n",
    "        scaler = StandardScaler()\n",
    "        train_zscore = scaler.fit_transform(train)\n",
    "        test_zscore = scaler.transform(test)\n",
    "    else:\n",
    "        mean = train.mean(axis=0)\n",
    "        train_zscore = train - mean\n",
    "        test_zscore = test - mean\n",
    "    return train_zscore, test_zscore \n",
    "\n",
    "# Helper function to plot regression results\n",
    "def plot_predictions(true_values, predicted_values, title, train_or_test):\n",
    "    plt.scatter(true_values, predicted_values, label=f'{train_or_test} Predictions', color='blue', alpha=0.6)\n",
    "    \n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def regression(train, test, age_train, age_test, values_train, values_test,pre=\"center\"):\n",
    "    if pre == \"center\":\n",
    "        # Mean center the tangent space data\n",
    "        mean = np.mean(train, axis=0)\n",
    "        train_centered = train - mean\n",
    "        test_centered = test - mean\n",
    "    elif pre == \"znorm\":\n",
    "        train_centered,  test_centered = preproc(train,test)\n",
    "    else:\n",
    "        train_centered = train\n",
    "        test_centered = test\n",
    "\n",
    "\n",
    "    # Regress out age using only the training data\n",
    "    age_reg = regress_out_age(train_centered, age_train)\n",
    "    # tan_train_centered = tan_train_centered - (tan_train_centered@np.linalg.pinv(age_reg.coef_[np.newaxis,:]))@age_reg.coef_[np.newaxis,:]\n",
    "    # tan_test_centered = tan_test_centered - (tan_test_centered@np.linalg.pinv(age_reg.coef_[np.newaxis,:]))@age_reg.coef_[np.newaxis,:]\n",
    "    train_centered = train_centered - age_reg.predict(age_train.reshape(-1, 1))\n",
    "    test_centered = test_centered - age_reg.predict(age_test.reshape(-1, 1))\n",
    "\n",
    "    # Choose the regression model\n",
    "    # reg_model = LinearSVR(C=1,fit_intercept=True)\n",
    "    # reg_model = LinearSVR(C=1)\n",
    "    # reg_model = LinearSVR(C=.01,loss='squared_epsilon_insensitive')\n",
    "    # reg_model = Lasso(alpha=0.001,fit_intercept=False)\n",
    "    reg_model = Ridge(alpha=1)\n",
    "    # reg_model = ElasticNet(alpha=1, l1_ratio=0.0001)\n",
    "    # reg_model = LinearRegression(fit_intercept=False)\n",
    "    # reg_model = LassoCV(cv=5)  # Use 5-fold cross-validation within each fold to tune Lasso\n",
    "    # reg_model = SVR(kernel=\"poly\")\n",
    "    reg_model.fit(train_centered, values_train)\n",
    "\n",
    "    # Get the predicted values from the regularized model\n",
    "    predictions_train = reg_model.predict(train_centered)\n",
    "    # Adjust for the regularization bias using Zou & Hastie's method\n",
    "    # Regress the true values on the predicted values\n",
    "    kappa_reg = LinearRegression(fit_intercept=False)\n",
    "    kappa_reg.fit(predictions_train.reshape(-1, 1), values_train)\n",
    "    kappa = kappa_reg.coef_[0]\n",
    "    predictions_train_adj = kappa*predictions_train\n",
    "\n",
    "    predictions = kappa*reg_model.predict(test_centered)\n",
    "    # Plot predictions vs true values for test fold\n",
    "    test_score = analyze_residuals(predictions, values_test)\n",
    "    plot_predictions(values_test, predictions,f'Test Predictions vs True Values - Fold R²: {test_score}', 'Test')\n",
    "\n",
    "    train_score = analyze_residuals(predictions_train_adj, values_train)\n",
    "    plot_predictions(values_train, predictions_train_adj, f'Train Predictions vs True Values - Fold R²: {train_score}', 'Train')\n",
    "\n",
    "    fold_corr, _ = pearsonr(values_test, predictions)\n",
    "\n",
    "    return train_centered, reg_model.coef_, test_score, fold_corr\n",
    "\n",
    "\n",
    "# Tangent space regression with centering\n",
    "def tan_regression_folds(data, values, age, metric=\"riemann\", pre=\"znorm\", n_splits=5):\n",
    "    # Binning the continuous target variable for stratified splits\n",
    "    n_bins = 5  # Adjust this based on your target distribution\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    values_binned = binner.fit_transform(values.reshape(-1, 1))\n",
    "\n",
    "    cov_est = Covariances(estimator='lwf')\n",
    "    # cov_est = Covariances(estimator='corr')\n",
    "    covs = cov_est.transform(np.transpose(data, (0, 2, 1)))\n",
    "    \n",
    "    # Initialize Stratified K-Fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10)\n",
    "    \n",
    "    fold_scores = []\n",
    "    fold_corrs = []\n",
    "    filter_fold_scores = []\n",
    "    filter_fold_corrs = []\n",
    "\n",
    "    count = 0\n",
    "    # Perform Stratified K-Fold Cross-Validation\n",
    "    for train_index, test_index in kf.split(data, values_binned):\n",
    "        print(\"Fold\", count)\n",
    "        count += 1\n",
    "        train, test = data[train_index], data[test_index]\n",
    "\n",
    "        values_train, values_test = values[train_index], values[test_index]\n",
    "        values_train, values_test = preproc(values_train[:,np.newaxis],values_test[:,np.newaxis], method=\"center\")\n",
    "        values_train = values_train[:,0]\n",
    "        values_test = values_test[:,0]\n",
    "\n",
    "        # Plot the histogram of training data and calculated weights\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Plot histogram of training target values\n",
    "        plt.hist(values_train, bins=n_bins, alpha=0.7, color='blue', edgecolor='black', label='Train Data')\n",
    "        plt.title(f'Train Data Histogram and Weights - Fold {count}')\n",
    "        plt.xlabel('Target Value')\n",
    "        plt.ylabel('Frequency / Weight')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        age_train, age_test = age[train_index], age[test_index]\n",
    "        age_train, age_test = preproc(age_train[:,np.newaxis], age_test[:,np.newaxis], method=\"center\")\n",
    "        age_train = age_train[:,0]\n",
    "        age_test = age_test[:,0]\n",
    "\n",
    "        train_covs, test_covs = covs[train_index], covs[test_index]\n",
    "        train_mean = mean_covariance(train_covs, metric=metric)\n",
    "        tan_train = tangent_space(train_covs,train_mean,metric=metric)\n",
    "        tan_test = tangent_space(test_covs,train_mean,metric=metric)\n",
    "\n",
    "        tan_train_centered, model_coefs, test_score, fold_corr = regression(tan_train, tan_test, age_train, age_test, values_train, values_test,pre=pre)\n",
    "        fold_scores.append(test_score)\n",
    "        fold_corrs.append(fold_corr)\n",
    "        print(f\"Fold Test R² Score: {test_score}\")\n",
    "        print(f\"Fold Test R Score: {fold_corr}\")\n",
    "\n",
    "        # (tan_train@np.linalg.pinv(reg_model.coef_))@reg_model.coef_\n",
    "        # hauf_coef = FKT_proj(tan_train,reg_model.coef_[:,np.newaxis],method=\"basic\")\n",
    "        # hauf_coef = FKT_proj(tan_train,reg_model.coef_[:,np.newaxis],method=\"covs\")\n",
    "        hauf_coef = FKT_proj(tan_train_centered, model_coefs[:,np.newaxis],method=\"linreg\").T[:,0]\n",
    "\n",
    "        weights_matrix = untangent_space(hauf_coef,train_mean,metric=metric)\n",
    "        # weights_matrix = untangent_space(reg_model.coef_,train_mean,metric=metric)\n",
    "        eigs, filters_all = eigh(weights_matrix,train_mean)\n",
    "        # plt.scatter(range(0,eigs.shape[0]),np.abs(eigs))\n",
    "        # plt.show()\n",
    "        iter_test_score_reduced = 0\n",
    "        iter_test_corr_reduced = 0\n",
    "        test_score_reduced = 0\n",
    "        test_corr_reduced = 0\n",
    "        n_filters = 0\n",
    "        while test_score_reduced <=  np.abs(iter_test_score_reduced):\n",
    "            test_score_reduced = iter_test_score_reduced\n",
    "            test_corr_reduced = iter_test_corr_reduced\n",
    "            n_filters += 1\n",
    "            inds = np.argsort(np.abs(eigs))[-n_filters:]\n",
    "            filters = filters_all[:,inds]\n",
    "\n",
    "            train_transformed = train @ filters\n",
    "            test_transformed = test @ filters\n",
    "\n",
    "            train_transformed_cov = cov_est.transform(np.transpose(train_transformed, (0, 2, 1)))\n",
    "            test_transformed_cov = cov_est.transform(np.transpose(test_transformed, (0, 2, 1)))\n",
    "\n",
    "            reduced_mean = mean_covariance(train_transformed_cov, metric=metric)\n",
    "            tangent_transform_train = tangent_space(train_transformed_cov, reduced_mean, metric=metric)\n",
    "            tangent_transform_test = tangent_space(test_transformed_cov, reduced_mean, metric=metric)\n",
    "\n",
    "            _, _, iter_test_score_reduced, iter_test_corr_reduced, = regression(tangent_transform_train, tangent_transform_test, age_train, age_test, values_train, values_test,pre=pre)\n",
    "                \n",
    "        filter_fold_scores.append(test_score_reduced)\n",
    "        filter_fold_corrs.append(test_corr_reduced)\n",
    "        print(n_filters)\n",
    "        print(f\"Fold Reduced Test R² Score: {test_score_reduced}\")\n",
    "        print(f\"Fold Reduced Test R Score: {test_corr_reduced}\")\n",
    "\n",
    "\n",
    "    # Output the average R² score across all folds\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    mean_corr = np.mean(fold_corrs)\n",
    "\n",
    "    mean_filter_score = np.mean(filter_fold_scores)\n",
    "    mean_filter_corr = np.mean(filter_fold_corrs)\n",
    "\n",
    "    print(f\"Mean R² Score across {n_splits} folds: {mean_score}\")\n",
    "    print(f\"Mean R Score across {n_splits} folds: {mean_corr}\")\n",
    "\n",
    "    print(f\"Mean R² Filter Score across {n_splits} folds: {mean_filter_score}\")\n",
    "    print(f\"Mean R Filter Score across {n_splits} folds: {mean_filter_corr}\")\n",
    "    return mean_score, mean_corr, mean_filter_score, mean_filter_corr, fold_scores, fold_corrs, filter_fold_scores, filter_fold_corrs\n",
    "\n",
    "\n",
    "\n",
    "def tan_quantile_regression_folds(data, values, age, metric=\"riemann\", pre=\"znorm\", n_splits=5):\n",
    "    # Binning the continuous target variable for stratified splits\n",
    "    n_bins = 3  # Adjust this based on your target distribution\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "    values_binned = binner.fit_transform(values.reshape(-1, 1))\n",
    "\n",
    "    cov_est = Covariances(estimator='lwf')\n",
    "    # cov_est = Covariances(estimator='corr')\n",
    "    covs = cov_est.transform(np.transpose(data, (0, 2, 1)))\n",
    "    \n",
    "    # Initialize Stratified K-Fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10)\n",
    "    \n",
    "    for quantile in range(0,n_bins):\n",
    "        fold_scores = []\n",
    "        fold_corrs = []\n",
    "        filter_fold_scores = []\n",
    "        filter_fold_corrs = []\n",
    "\n",
    "        count = 0\n",
    "        # Perform Stratified K-Fold Cross-Validation\n",
    "        for train_index, test_index in kf.split(data, values_binned):\n",
    "            print(\"Fold\", count)\n",
    "            count += 1\n",
    "            \n",
    "            group_train_ind = np.where(values_binned[train_index] == quantile)[0]\n",
    "            group_test_ind = np.where(values_binned[test_index] == quantile)[0]\n",
    "            \n",
    "            train, test = data[train_index][group_train_ind], data[test_index][group_test_ind]\n",
    "\n",
    "            values_train, values_test = values[train_index][group_train_ind], values[test_index][group_test_ind]\n",
    "            values_train, values_test = preproc(values_train[:,np.newaxis],values_test[:,np.newaxis], method=\"center\")\n",
    "            values_train = values_train[:,0]\n",
    "            values_test = values_test[:,0]\n",
    "\n",
    "\n",
    "            # Plot the histogram of training data and calculated weights\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            # Plot histogram of training target values\n",
    "            plt.hist(values_train, bins=n_bins, alpha=0.7, color='blue', edgecolor='black', label='Train Data')\n",
    "            plt.title(f'Train Data Histogram and Weights - Fold {count}')\n",
    "            plt.xlabel('Target Value')\n",
    "            plt.ylabel('Frequency / Weight')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            age_train, age_test = age[train_index][group_train_ind], age[test_index][group_test_ind]\n",
    "            age_train, age_test = preproc(age_train[:,np.newaxis], age_test[:,np.newaxis], method=\"center\")\n",
    "            age_train = age_train[:,0]\n",
    "            age_test = age_test[:,0]\n",
    "\n",
    "            train_covs, test_covs = covs[train_index][group_train_ind], covs[test_index][group_test_ind]\n",
    "            train_mean = mean_covariance(train_covs, metric=metric)\n",
    "            tan_train = tangent_space(train_covs,train_mean,metric=metric)\n",
    "            tan_test = tangent_space(test_covs,train_mean,metric=metric)\n",
    "\n",
    "            print(tan_train.shape,tan_test.shape,age_train.shape,age_test.shape,values_train.shape,values_test.shape)\n",
    "            tan_train_centered, model_coefs, test_score, fold_corr = regression(tan_train, tan_test, age_train, age_test, values_train, values_test,pre=pre)\n",
    "            fold_scores.append(test_score)\n",
    "            fold_corrs.append(fold_corr)\n",
    "            print(f\"Fold Test R² Score: {test_score}\")\n",
    "            print(f\"Fold Test R Score: {fold_corr}\")\n",
    "\n",
    "            # (tan_train@np.linalg.pinv(reg_model.coef_))@reg_model.coef_\n",
    "            # hauf_coef = FKT_proj(tan_train,reg_model.coef_[:,np.newaxis],method=\"basic\")\n",
    "            # hauf_coef = FKT_proj(tan_train,reg_model.coef_[:,np.newaxis],method=\"covs\")\n",
    "            hauf_coef = FKT_proj(tan_train_centered, model_coefs[:,np.newaxis],method=\"linreg\")\n",
    "\n",
    "            weights_matrix = untangent_space(hauf_coef.T[:,0],train_mean,metric=metric)\n",
    "            # weights_matrix = untangent_space(reg_model.coef_,train_mean,metric=metric)\n",
    "            eigs, filters_all = eigh(weights_matrix,train_mean)\n",
    "            # plt.scatter(range(0,eigs.shape[0]),np.abs(eigs))\n",
    "            # plt.show()\n",
    "\n",
    "            n_filters = 5\n",
    "            inds = np.argsort(np.abs(eigs))[-n_filters:]\n",
    "            filters = filters_all[:,inds]\n",
    "\n",
    "            train_transformed = train @ filters\n",
    "            test_transformed = test @ filters\n",
    "\n",
    "            train_transformed_cov = cov_est.transform(np.transpose(train_transformed, (0, 2, 1)))\n",
    "            test_transformed_cov = cov_est.transform(np.transpose(test_transformed, (0, 2, 1)))\n",
    "\n",
    "            reduced_mean = mean_covariance(train_transformed_cov, metric=metric)\n",
    "            tangent_transform_train = tangent_space(train_transformed_cov, reduced_mean, metric=metric)\n",
    "            tangent_transform_test = tangent_space(test_transformed_cov, reduced_mean, metric=metric)\n",
    "\n",
    "            _, _, test_score_reduced, test_corr_reduced, = regression(tangent_transform_train, tangent_transform_test, age_train, age_test, values_train, values_test,pre=pre)\n",
    "                    \n",
    "            filter_fold_scores.append(test_score_reduced)\n",
    "            filter_fold_corrs.append(test_corr_reduced)\n",
    "            print(n_filters)\n",
    "            print(f\"Fold Reduced Test R² Score: {test_score_reduced}\")\n",
    "            print(f\"Fold Reduced Test R Score: {test_corr_reduced}\")\n",
    "\n",
    "\n",
    "        # Output the average R² score across all folds\n",
    "        mean_score = np.mean(fold_scores)\n",
    "        mean_corr = np.mean(fold_corrs)\n",
    "\n",
    "        mean_filter_score = np.mean(filter_fold_scores)\n",
    "        mean_filter_corr = np.mean(filter_fold_corrs)\n",
    "\n",
    "        print(f\"Mean R² Score across {n_splits} folds: {mean_score}\")\n",
    "        print(f\"Mean R Score across {n_splits} folds: {mean_corr}\")\n",
    "\n",
    "        print(f\"Mean R² Filter Score across {n_splits} folds: {mean_filter_score}\")\n",
    "        print(f\"Mean R Filter Score across {n_splits} folds: {mean_filter_corr}\")\n",
    "\n",
    "def tan_quantile_regression_II(data, values, age, metric=\"riemann\", pre=\"znorm\", n_splits=5):\n",
    "    # Binning the continuous target variable for stratified splits\n",
    "    n_bins = 3  # Adjust this based on your target distribution\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "    values_binned = binner.fit_transform(values.reshape(-1, 1))\n",
    "\n",
    "    cov_est = Covariances(estimator='lwf')\n",
    "    # cov_est = Covariances(estimator='corr')\n",
    "    covs = cov_est.transform(np.transpose(data, (0, 2, 1)))\n",
    "    \n",
    "    # Initialize Stratified K-Fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10)\n",
    "    \n",
    "    fold_scores = []\n",
    "    fold_corrs = []\n",
    "    filter_fold_scores = []\n",
    "    filter_fold_corrs = []\n",
    "\n",
    "    count = 0\n",
    "    # Perform Stratified K-Fold Cross-Validation\n",
    "    for train_index, test_index in kf.split(data, values_binned):\n",
    "        print(\"Fold\", count)\n",
    "        count += 1\n",
    "        filters = []\n",
    "        values_train, values_test = values[train_index], values[test_index]\n",
    "        values_train, values_test = preproc(values_train[:,np.newaxis],values_test[:,np.newaxis], method=\"center\")\n",
    "        values_train = values_train[:,0]\n",
    "        values_test = values_test[:,0]\n",
    "\n",
    "        # Plot the histogram of training data and calculated weights\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Plot histogram of training target values\n",
    "        plt.hist(values_train, bins=n_bins, alpha=0.7, color='blue', edgecolor='black', label='Train Data')\n",
    "        plt.title(f'Train Data Histogram and Weights - Fold {count}')\n",
    "        plt.xlabel('Target Value')\n",
    "        plt.ylabel('Frequency / Weight')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        age_train, age_test = age[train_index], age[test_index]\n",
    "        age_train, age_test = preproc(age_train[:,np.newaxis], age_test[:,np.newaxis], method=\"center\")\n",
    "        age_train = age_train[:,0]\n",
    "        age_test = age_test[:,0]\n",
    "        \n",
    "        train, test = data[train_index], data[test_index]\n",
    "\n",
    "        train_covs, test_covs = covs[train_index], covs[test_index]\n",
    "        for quantile in range(0,n_bins):\n",
    "            group_train_ind = np.where(values_binned[train_index] == quantile)[0]\n",
    "            group_test_ind = np.where(values_binned[test_index] == quantile)[0]\n",
    "            \n",
    "            values_train_quantile, values_test_quantile, age_train_quantile, age_test_quantile, train_quantile, test_quantile = values_train[group_train_ind], values_test[group_test_ind], age_train[group_train_ind], age_test[group_test_ind], train[group_train_ind], test[group_test_ind]\n",
    "            train_covs_quantile, test_covs_quantile = train_covs[group_train_ind], test_covs[group_test_ind]\n",
    "            train_mean_quantile = mean_covariance(train_covs_quantile, metric=metric)\n",
    "            tan_train_quantile = tangent_space(train_covs_quantile,train_mean_quantile,metric=metric)\n",
    "            tan_test_quantile = tangent_space(test_covs_quantile,train_mean_quantile,metric=metric)\n",
    "\n",
    "            tan_train_centered_quantile, model_coefs, test_score, fold_corr = regression(tan_train_quantile, tan_test_quantile, age_train_quantile, age_test_quantile, values_train_quantile, values_test_quantile,pre=pre)\n",
    "            fold_scores.append(test_score)\n",
    "            fold_corrs.append(fold_corr)\n",
    "            print(f\"Fold Test R² Score: {test_score}\")\n",
    "            print(f\"Fold Test R Score: {fold_corr}\")\n",
    "\n",
    "            # (tan_train@np.linalg.pinv(reg_model.coef_))@reg_model.coef_\n",
    "            # hauf_coef = FKT_proj(tan_train,reg_model.coef_[:,np.newaxis],method=\"basic\")\n",
    "            # hauf_coef = FKT_proj(tan_train,reg_model.coef_[:,np.newaxis],method=\"covs\")\n",
    "            hauf_coef = FKT_proj(tan_train_centered_quantile, model_coefs[:,np.newaxis],method=\"linreg\")\n",
    "\n",
    "            weights_matrix = untangent_space(hauf_coef.T[:,0],train_mean_quantile,metric=metric)\n",
    "            # weights_matrix = untangent_space(reg_model.coef_,train_mean,metric=metric)\n",
    "            eigs, filters_all = eigh(weights_matrix,train_mean_quantile)\n",
    "            # plt.scatter(range(0,eigs.shape[0]),np.abs(eigs))\n",
    "            # plt.show()\n",
    "\n",
    "            n_filters = 3\n",
    "            inds = np.argsort(np.abs(eigs))[-n_filters:]\n",
    "            quantile_filters = filters_all[:,inds]\n",
    "            filters.append(quantile_filters)\n",
    "\n",
    "        train_mean = mean_covariance(train_covs, metric=metric)\n",
    "        tan_train = tangent_space(train_covs,train_mean,metric=metric)\n",
    "        tan_test = tangent_space(test_covs,train_mean,metric=metric)\n",
    "        \n",
    "        filters = np.concatenate(filters, axis=1)  # Concatenate along the filter dimension\n",
    "        train_transformed = train @ filters\n",
    "        test_transformed = test @ filters\n",
    "\n",
    "        train_transformed_cov = cov_est.transform(np.transpose(train_transformed, (0, 2, 1)))\n",
    "        test_transformed_cov = cov_est.transform(np.transpose(test_transformed, (0, 2, 1)))\n",
    "\n",
    "        reduced_mean = mean_covariance(train_transformed_cov, metric=metric)\n",
    "        tangent_transform_train = tangent_space(train_transformed_cov, reduced_mean, metric=metric)\n",
    "        tangent_transform_test = tangent_space(test_transformed_cov, reduced_mean, metric=metric)\n",
    "\n",
    "        _, _, test_score_reduced, test_corr_reduced, = regression(tangent_transform_train, tangent_transform_test, age_train, age_test, values_train, values_test,pre=pre)\n",
    "\n",
    "        filter_fold_scores.append(test_score_reduced)\n",
    "        filter_fold_corrs.append(test_corr_reduced)\n",
    "        print(n_filters)\n",
    "        print(f\"Fold Reduced Test R² Score: {test_score_reduced}\")\n",
    "        print(f\"Fold Reduced Test R Score: {test_corr_reduced}\")\n",
    "\n",
    "\n",
    "    # Output the average R² score across all folds\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    mean_corr = np.mean(fold_corrs)\n",
    "\n",
    "    mean_filter_score = np.mean(filter_fold_scores)\n",
    "    mean_filter_corr = np.mean(filter_fold_corrs)\n",
    "\n",
    "    print(f\"Mean R² Score across {n_splits} folds: {mean_score}\")\n",
    "    print(f\"Mean R Score across {n_splits} folds: {mean_corr}\")\n",
    "\n",
    "    print(f\"Mean R² Filter Score across {n_splits} folds: {mean_filter_score}\")\n",
    "    print(f\"Mean R Filter Score across {n_splits} folds: {mean_filter_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 1 Fold and Filters for Rest of Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, combined_paths = combine_groups(groupA_parcellated_array, groupB_parcellated_array, groupA_paths_filtered, groupB_paths_filtered)\n",
    "subject_ids = extract_subject_ids(combined_paths)\n",
    "values = np.array(extract_phenotype(subject_ids,phenotype))\n",
    "nan_mask = ~np.isnan(values)  # This will create a boolean mask, True where values are not NaN\n",
    "data, combined_paths, subject_ids, values = data[nan_mask], combined_paths[nan_mask], subject_ids[nan_mask], values[nan_mask]\n",
    "age = np.array(extract_phenotype(subject_ids, 'Age_in_Yrs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tan_quantile_regression_II(data,values,age, metric=metric,pre=\"center\",n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score, mean_corr, mean_filter_score, mean_filter_corr, fold_scores, fold_corrs, filter_fold_scores, filter_fold_corrs = tan_regression_folds(data,values,age, metric=metric,pre=\"center\",n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 3  # Adjust this based on your target distribution\n",
    "binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "values_binned = binner.fit_transform(values.reshape(-1, 1))\n",
    "\n",
    "# Initialize Stratified K-Fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=10)\n",
    "splits = list(kf.split(data, values_binned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "fold_outputfolder = f\"fold_{fold}\"\n",
    "if not os.path.exists(os.path.join(outputfolder, f\"fold_{fold}\")):\n",
    "    os.makedirs(os.path.join(outputfolder, f\"fold_{fold}\"))\n",
    "if not os.path.exists(os.path.join(outputfolder, f\"fold_{fold}\", \"results\")):\n",
    "    os.makedirs(os.path.join(outputfolder, f\"fold_{fold}\", \"results\"))\n",
    "\n",
    "train_index = splits[fold][0]\n",
    "test_index = splits[fold][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data[train_index], data[test_index]\n",
    "combined_paths_train, combined_paths_test = combined_paths[train_index], combined_paths[test_index]\n",
    "\n",
    "values_train, values_test = values[train_index], values[test_index]\n",
    "values_train, values_test = preproc(values_train[:,np.newaxis],values_test[:,np.newaxis], method=\"center\")\n",
    "values_train = values_train[:,0]\n",
    "values_test = values_test[:,0]\n",
    "\n",
    "age_train, age_test = age[train_index], age[test_index]\n",
    "age_train, age_test = preproc(age_train[:,np.newaxis], age_test[:,np.newaxis], method=\"center\")\n",
    "age_train = age_train[:,0]\n",
    "age_test = age_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group0_train_ind = np.where(values_binned[train_index] == 0)[0]\n",
    "group1_train_ind = np.where(values_binned[train_index] == 1)[0]\n",
    "group2_train_ind = np.where(values_binned[train_index] == 2)[0]\n",
    "\n",
    "\n",
    "group0_train = train[group0_train_ind]\n",
    "group0_values_train = values_train[group0_train_ind]\n",
    "group0_age_train = age_train[group0_train_ind]\n",
    "print(group0_values_train.shape)\n",
    "print(group0_train.shape)\n",
    "print(group0_age_train.shape)\n",
    "\n",
    "group1_train = train[group1_train_ind]\n",
    "group1_values_train = values_train[group1_train_ind]\n",
    "group1_age_train = age_train[group1_train_ind]\n",
    "print(group1_values_train.shape)\n",
    "print(group1_train.shape)\n",
    "print(group1_age_train.shape)\n",
    "\n",
    "group2_train = train[group2_train_ind]\n",
    "group2_values_train = values_train[group2_train_ind]\n",
    "group2_age_train = age_train[group2_train_ind]\n",
    "print(group2_values_train.shape)\n",
    "print(group2_train.shape)\n",
    "print(group2_age_train.shape)\n",
    "\n",
    "# Plot the histogram for each group's 'values_train'\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(group0_values_train, bins=10, alpha=0.5, label=\"Group 0 (Values = 0)\")\n",
    "plt.hist(group1_values_train, bins=10, alpha=0.5, label=\"Group 1 (Values = 1)\")\n",
    "plt.hist(group2_values_train, bins=10, alpha=0.5, label=\"Group 2 (Values = 2)\")\n",
    "\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Values for Different Groups')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_est = Covariances(estimator='lwf')\n",
    "train_covs = cov_est.transform(np.transpose(train, (0, 2, 1)))\n",
    "train_mean = mean_covariance(train_covs, metric=metric)\n",
    "tan_train = tangent_space(train_covs,train_mean,metric=metric)\n",
    "\n",
    "tan_mean = np.mean(tan_train, axis=0)\n",
    "tan_train_centered = tan_train - tan_mean\n",
    "\n",
    "age_reg = regress_out_age(tan_train_centered, age_train)\n",
    "tan_train_centered = tan_train_centered - age_reg.predict(age_train.reshape(-1, 1))\n",
    "\n",
    "reg_model = Ridge(alpha=1)\n",
    "reg_model.fit(tan_train_centered, values_train)\n",
    "\n",
    "hauf_coef = FKT_proj(tan_train_centered,reg_model.coef_[:,np.newaxis],method=\"linreg\")\n",
    "\n",
    "weights_matrix = untangent_space(hauf_coef.T[:,0],train_mean,metric=metric)\n",
    "eigs, filters_all = eigh(weights_matrix,train_mean)\n",
    "inds = np.argsort(np.abs(eigs))[-4:]\n",
    "filters_parcellated = filters_all[:,inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyriemann.utils.tangentspace import log_map_riemann, log_map_euclid, log_map_logeuclid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riem_covs = tangent_space(train_covs[:10], train_mean,metric=\"riemann\")\n",
    "# logeuc_covs = log_map_logeuclid(train_covs[:10], train_mean)\n",
    "# euc_covs = log_map_euclid(train_covs[:10], train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(riem_covs.T)\n",
    "print(correlation_matrix.shape)\n",
    "\n",
    "# sns.heatmap((riem_covs.T@riem_covs))\n",
    "# plt.show()\n",
    "# sns.heatmap((logeuc_covs[0].T@logeuc_covs[0]))\n",
    "# plt.show()\n",
    "# sns.heatmap((euc_covs[0].T@euc_covs[0]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the upper triangular part without the diagonal (as they are self-correlations)\n",
    "upper_tri_indices = np.triu_indices_from(correlation_matrix, k=1)\n",
    "average_correlation = np.mean(np.abs(correlation_matrix[upper_tri_indices]))\n",
    "\n",
    "print(\"Average correlation between projected matrices:\", average_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "metrics = [\"euclid\", \"logeuclid\", \"riemann\"]\n",
    "A = train_covs[0]\n",
    "B = train_covs[1]\n",
    "\n",
    "# Set precision level for numpy printing (16 decimal places)\n",
    "np.set_printoptions(precision=16, suppress=True)\n",
    "\n",
    "# Print shapes of A and B\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "\n",
    "# Print determinants of A and B with high precision (16 decimal places)\n",
    "print(f\"Determinant of A: {np.linalg.det(A):.16f}\")\n",
    "print(f\"Determinant of B: {np.linalg.det(B):.16f}\")\n",
    "\n",
    "for met in metrics:\n",
    "    print(f\"\\nTesting metric: {met}\")\n",
    "    test = 1  # To check if all operations succeed\n",
    "\n",
    "    try:\n",
    "        # Calculate distance with high precision\n",
    "        dist = distance(A, B, metric=met)\n",
    "        print(f\"Distance ({met}): {dist:.16f}\")\n",
    "    except Exception as e:\n",
    "        test = 0\n",
    "        print(f\"{met} failed during distance calculation: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Calculate mean covariance\n",
    "        av = mean_covariance(np.array([A, B]), metric=met)\n",
    "        print(f\"Mean covariance ({met}) calculated successfully.\")\n",
    "        # Eigenvalue decomposition of the mean covariance matrix\n",
    "        eigenvalues, _ = np.linalg.eigh(av)\n",
    "\n",
    "        # Check if any eigenvalues are zero (or very close to zero)\n",
    "        if np.any(eigenvalues <= 1e-10):\n",
    "            print(\"Matrix is singular or nearly singular.\")\n",
    "        else:\n",
    "            print(\"Matrix is positive definite.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        test = 0\n",
    "        print(f\"{met} failed during mean covariance calculation: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Convert to tangent space\n",
    "        tanA = tangent_space(A, av, metric=met)\n",
    "        tanB = tangent_space(B, av, metric=met)\n",
    "        print(f\"Tangent space transformations ({met}) successful.\")\n",
    "    except Exception as e:\n",
    "        test = 0\n",
    "        print(f\"{met} failed during tangent space transformation: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Convert back from tangent space\n",
    "        untanA = untangent_space(tanA, av, metric=met)\n",
    "        untanB = untangent_space(tanB, av, metric=met)\n",
    "        print(f\"Untangent space transformations ({met}) successful.\")\n",
    "    except Exception as e:\n",
    "        test = 0\n",
    "        print(f\"{met} failed during untangent space transformation: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Print determinants of the mean covariance and eigenvalue decompositions with high precision\n",
    "        print(f\"Determinant of mean covariance: {np.linalg.det(av):.16f}\")\n",
    "        _, _ = eigh(A, av)\n",
    "        _, _ = eigh(B, av)\n",
    "        _, _ = eigh(untanA, av)\n",
    "        _, _ = eigh(untanB, av)\n",
    "        print(f\"GEVD ({met}) successful.\")\n",
    "    except Exception as e:\n",
    "        test = 0\n",
    "        print(f\"{met} failed during GEVD calculation: {e}\")\n",
    "\n",
    "    if test == 1:\n",
    "        print(f\"{met} passed all tests.\")\n",
    "    else:\n",
    "        print(f\"{met} encountered errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migp(subs, batch_size=2, m=4800):\n",
    "    W_gpu = None\n",
    "    for batch_start in range(0, len(subs), batch_size):\n",
    "        # Select the current batch of subjects\n",
    "        batch_subs = subs[batch_start:batch_start + batch_size]\n",
    "        batch_paths = [path for sublist in batch_subs for path in sublist]\n",
    "\n",
    "        concatenated_data = []\n",
    "\n",
    "        for task in batch_paths:\n",
    "            X = nib.load(task).get_fdata()\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            # print(Xn.mean(axis=0).mean())\n",
    "            # print(Xn.std(axis=0).mean())\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "            \n",
    "        try:\n",
    "            # Concatenate data along the first axis using numpy\n",
    "            batch = np.concatenate(concatenated_data, axis=0)\n",
    "            batch = hcp.normalize(batch - batch.mean(axis=1,keepdims=True))\n",
    "            del concatenated_data\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Convert to torch tensor and move to GPU\n",
    "                batch_gpu = torch.tensor(batch, dtype=torch.float32, device=\"cuda\")\n",
    "                del batch\n",
    "                if torch.isnan(batch_gpu).any():\n",
    "                    print(\"NaNs detected in the batch data. Aborting SVD operation.\")\n",
    "                    del batch_gpu\n",
    "                    torch.cuda.empty_cache()\n",
    "                    return None\n",
    "                if W_gpu is None:\n",
    "                    combined_data_gpu = batch_gpu\n",
    "                else:\n",
    "                    combined_data_gpu = torch.cat([W_gpu, batch_gpu], dim=0)\n",
    "                del batch_gpu\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # # Calculate size in GB\n",
    "                # size_in_gb = combined_data_gpu.element_size() * combined_data_gpu.nelement() / (1024**3)\n",
    "                # print(f\"Size of the array: {size_in_gb:.2f} GB\")\n",
    "                # cpu_mem()\n",
    "                # gpu_mem()\n",
    "                # Perform SVD on the GPU\n",
    "                # Check for NaNs in the data\n",
    "\n",
    "                # _, S_gpu, Vh_gpu = torch.linalg.svd(combined_data_gpu, full_matrices=False)\n",
    "                _, Q = torch.linalg.eigh(combined_data_gpu@combined_data_gpu.T)\n",
    "                # cpu_mem()\n",
    "                # gpu_mem()\n",
    "                # Compute the updated W on the GPU\n",
    "                # W_gpu = torch.diag(S_gpu[:m]) @ Vh_gpu[:m, :]\n",
    "                # Returned in Ascending order\n",
    "                W_gpu = Q[:, -m:].T@combined_data_gpu\n",
    "                del Q, combined_data_gpu  # Free up GPU memory\n",
    "                torch.cuda.empty_cache()\n",
    "                print(batch_start, \"done\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed during GPU processing: {e}\")\n",
    "            if \"combined_data_gpu\" in locals():\n",
    "                del combined_data_gpu\n",
    "            if \"Q\" in locals():\n",
    "                del Q\n",
    "            if \"W_gpu\" in locals():\n",
    "                del W_gpu\n",
    "            torch.cuda.empty_cache()\n",
    "            return None\n",
    "\n",
    "    # Transfer W back to CPU only at the end\n",
    "    W = W_gpu.cpu().numpy()\n",
    "    del W_gpu  # Free up GPU memory\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubs = migp((combined_paths_train))\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,'reducedsubs.npy'), reducedsubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubs = load_array_from_outputfolder(os.path.join(fold_outputfolder,'reducedsubs.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_haufe(sub,pinv_TF):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub:\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "\n",
    "        # Concatenate data along the first axis\n",
    "        subject = np.concatenate(concatenated_data, axis=0)\n",
    "        del concatenated_data  # Explicitly delete the concatenated data list\n",
    "\n",
    "        Xp = hcp.normalize(subject - subject.mean(axis=1, keepdims=True))\n",
    "        del subject\n",
    "        Xpf = pinv_TF@Xp\n",
    "        del Xp\n",
    "        return Xpf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None\n",
    "\n",
    "\n",
    "def haufe_transform(F, parcellated,paths):\n",
    "    \n",
    "    # Ensure the tensors are on the correct device\n",
    "    pinv_TF = np.linalg.pinv(parcellated.reshape(-1,parcellated.shape[-1]) @ np.linalg.pinv(F.T))\n",
    "\n",
    "\n",
    "    # pinv_TF_list = pinv_TF.reshape(len(paths),F.shape[1],pinv_TF.shape[0])\n",
    "    pinv_TF_list = (np.array_split(pinv_TF, len(paths), axis=1))\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=(int(os.cpu_count()*.5))) as executor:\n",
    "        # Use map to process subjects in parallel\n",
    "        blocks = np.array(list(executor.map(process_subject_haufe, paths,pinv_TF_list)))\n",
    "        print(blocks.shape)\n",
    "        return (blocks.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_transform = haufe_transform(filters_parcellated,train,combined_paths_train)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"filters_transform.npy\"), filters_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_transform = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"filters_transform.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filters_parcellated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(hcp.unparcellate(filters_parcellated[:,0], hcp.mmp)), threshold=np.percentile(np.abs(filters_parcellated[:,0]), 50), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(filters_transform[0,:]), threshold=np.percentile(np.abs(filters_transform[0,:]), 90), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthonormalize Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthonormalize_filters(W):\n",
    "    print(W.shape)\n",
    "    \n",
    "    # Perform QR decomposition to orthonormalize the filters\n",
    "    Q, _ = np.linalg.qr(W)\n",
    "    \n",
    "    print(Q.shape)\n",
    "\n",
    "    # Verify that the inner product between the two orthonormalized vectors is 0 (orthogonality)\n",
    "    print(f'Inner product between Q[:, 0] and Q[:, 1]: {np.dot(Q[:, 0].T, Q[:, 1])} (should be 0)')\n",
    "    \n",
    "    # Verify that the inner product within each vector is 1 (normalization)\n",
    "    print(f'Norm of Q[:, 0]: {np.dot(Q[:, 0].T, Q[:, 0])} (should be 1)')\n",
    "    print(f'Norm of Q[:, 1]: {np.dot(Q[:, 1].T, Q[:, 1])} (should be 1)')\n",
    "    \n",
    "    return Q\n",
    "# Example usage\n",
    "\n",
    "filters = orthonormalize_filters(filters_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_pca_dim(Data=None,eigs=None,N=None):\n",
    "   # Start MATLAB engine\n",
    "    eng = matlab.engine.start_matlab()\n",
    "    \n",
    "    # Add the path to the MATLAB function\n",
    "    eng.addpath(\"/project/3022057.01/IFA/melodic\", nargout=0)\n",
    "    \n",
    "    if Data is not None:\n",
    "      # Call the MATLAB function\n",
    "      prob = eng.pca_dim(matlab.double(Data))\n",
    "      eig_vectors = np.array(prob['E'])\n",
    "    else:\n",
    "      prob = eng.pca_dim_eigs(matlab.double(eigs.tolist()), matlab.double([N]))\n",
    "\n",
    "    # Extract and convert each variable\n",
    "    lap = np.array(prob['lap']).flatten().reshape(-1, 1)\n",
    "    bic = np.array(prob['bic']).flatten().reshape(-1, 1)\n",
    "    rrn = np.array(prob['rrn']).flatten().reshape(-1, 1)\n",
    "    AIC = np.array(prob['AIC']).flatten().reshape(-1, 1)\n",
    "    MDL = np.array(prob['MDL']).flatten().reshape(-1, 1)\n",
    "    eig = np.array(prob['eig']).flatten()\n",
    "    orig_eig = np.array(prob['orig_eig']).flatten()\n",
    "    leig = np.array(prob['leig']).flatten()\n",
    "\n",
    "    # Stop MATLAB engine\n",
    "    eng.eval('clearvars', nargout=0)\n",
    "    eng.quit()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.arange(len(eig)),eig,label=\"Adjusted Eigenspectrum\")\n",
    "    plt.scatter(np.arange(len(orig_eig)),orig_eig,label=\"Eigenspectrum\")\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.legend()\n",
    "    plt.title('Scree Plot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Use SimpleImputer to handle any missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    lap = imputer.fit_transform(lap)\n",
    "    bic = imputer.fit_transform(bic)\n",
    "    rrn = imputer.fit_transform(rrn)\n",
    "    AIC = imputer.fit_transform(AIC)\n",
    "    MDL = imputer.fit_transform(MDL)\n",
    "    \n",
    "    # Use StandardScaler to standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    lap_std = scaler.fit_transform(lap)\n",
    "    bic_std = scaler.fit_transform(bic)\n",
    "    rrn_std = scaler.fit_transform(rrn)\n",
    "    AIC_std = scaler.fit_transform(AIC)\n",
    "    MDL_std = scaler.fit_transform(MDL)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.arange(len(lap_std)), lap_std, label='Laplacian')\n",
    "    plt.scatter(np.arange(len(bic_std)), bic_std, label='BIC')\n",
    "    plt.scatter(np.arange(len(rrn_std)), rrn_std, label='RRN')\n",
    "    plt.scatter(np.arange(len(AIC_std)), AIC_std, label='AIC')\n",
    "    plt.scatter(np.arange(len(MDL_std)), MDL_std, label='MDL')\n",
    "    \n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Standardized Value')\n",
    "    plt.legend()\n",
    "    plt.title('Scatter Plot of Standardized Eigenvalues and Model Order Selection Values')\n",
    "    plt.show()\n",
    "   \n",
    "    return np.argmax(rrn_std)+1\n",
    "\n",
    "def get_n_and_some(data):\n",
    "    # Check the shape of the data and determine the axis for mean subtraction\n",
    "\n",
    "    # Move data to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gpu = data.to(device, dtype=torch.float32)\n",
    "    groupN = data_gpu.shape[1] - 1\n",
    "\n",
    "    # Subtract the mean along the specified axis\n",
    "    data_centered = data_gpu - torch.mean(data_gpu, dim=1, keepdim=True)\n",
    "    del data_gpu  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    # Perform SVD decomposition\n",
    "    _, d, v = torch.svd(data_centered)\n",
    "    del data_centered  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert singular values to eigenvalues\n",
    "    e = (d ** 2) / groupN\n",
    "\n",
    "    # Move eigenvalues to CPU and convert to NumPy array\n",
    "    e_np = e.cpu().numpy()\n",
    "    del e, d  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Determine the number of components\n",
    "    n_components = torch.tensor(call_pca_dim(eigs=e_np, N=groupN),device=device,dtype=torch.int32)\n",
    "\n",
    "    return n_components, v.T\n",
    "\n",
    "def PPCA(data, filters=None, threshold=1.6, niters=10, n=-1):\n",
    "    n_components = -1\n",
    "    n_prev = -2\n",
    "    i = 0\n",
    "\n",
    "    # Move data to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gpu = torch.tensor(data,device=device, dtype=torch.float32)\n",
    "\n",
    "    while n_components != n_prev and i < niters:\n",
    "        n_prev = n_components\n",
    "        if filters is not None:\n",
    "            basis_gpu =  torch.tensor(filters.T,device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            n_components, vt = get_n_and_some(data_gpu)\n",
    "            if n <= 0:\n",
    "                basis_gpu = vt[:n_components, :]\n",
    "            else:\n",
    "                print(n)\n",
    "                basis_gpu = vt[:n, :]\n",
    "            del vt\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(n_prev, n_components)\n",
    "\n",
    "        # Estimate noise and residual standard deviation\n",
    "        est_noise = data_gpu - (data_gpu @ torch.linalg.pinv(basis_gpu)) @ basis_gpu\n",
    "        est_residual_std = torch.std(est_noise,dim=0,correction=torch.linalg.matrix_rank(basis_gpu))\n",
    "        del est_noise\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Normalize the data\n",
    "        data_gpu = (data_gpu / est_residual_std)\n",
    "        i += 1\n",
    "\n",
    "    data = data_gpu.cpu().numpy()\n",
    "    basis = basis_gpu.cpu().numpy()\n",
    "    # del data_gpu, basis_gpu, est_residual_std\n",
    "    del data_gpu, basis_gpu\n",
    "    torch.cuda.empty_cache()\n",
    "    return data, basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_data_VN, vt = PPCA(reducedsubs.copy(), threshold=0.0, niters=1)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"subs_data_VN.npy\"), subs_data_VN)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"vt.npy\"), vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns are samples i.e. XXT is the covariance matrix formed\n",
    "def whiten(X,n_components, method=\"SVD\", visualize=False):\n",
    "    # -1 to account for demean\n",
    "    n_samples = X.shape[-1]-1\n",
    "    X_mean = X.mean(axis=-1)\n",
    "    X -= X_mean[:, np.newaxis]\n",
    "\n",
    "    if method == \"SVD\":\n",
    "        u, d = svd(X, full_matrices=False, check_finite=False)[:2]\n",
    "        # Give consistent eigenvectors for both svd solvers\n",
    "        # u *= np.sign(u[0])\n",
    "        K = (u / d).T[:n_components]  # see (6.33) p.140\n",
    "        del u, d\n",
    "        whitening_matrix = np.sqrt(n_samples)*K\n",
    "    elif method == \"Cholesky\":\n",
    "    # Does not Orthogonalize, just has unit covariance\n",
    "        # Step 2: Perform Cholesky decomposition\n",
    "        L = np.linalg.cholesky(np.cov(X,ddof=1))\n",
    "        # Step 3:\n",
    "        whitening_matrix = np.linalg.inv(L)\n",
    "    elif method == \"InvCov\":\n",
    "        # Calculate the covariance matrix of the centered data\n",
    "        cov_matrix = np.cov(X)\n",
    "        # Perform eigenvalue decomposition of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "        # Calculate the whitening matrix\n",
    "        D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n",
    "        whitening_matrix = eigvecs @ D_inv_sqrt @ eigvecs.T\n",
    "   \n",
    "    whitened_data = whitening_matrix@X\n",
    "\n",
    "    return whitened_data, whitening_matrix\n",
    "\n",
    "# Combine Basis\n",
    "combined_spatial = np.vstack((vt,filters.T))\n",
    "\n",
    "# Whiten\n",
    "whitened_basis, whitening_matrix_pre = whiten(combined_spatial,n_components=combined_spatial.shape[0],method=\"InvCov\",visualize=True)\n",
    "subs_data_com_VN, _ = PPCA(reducedsubs.copy(), filters=whitened_basis.T, threshold=0.0, niters=1)\n",
    "\n",
    "# tempbasis = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))@subs_data_com_VN\n",
    "# whitened_basis, _ = whiten(tempbasis,n_components=tempbasis.shape[0],method=\"InvCov\",visualize=True)\n",
    "\n",
    "# for i in range(0,3):\n",
    "#     # Readjust the MiGP data based on the new basis\n",
    "#     subs_data_com_VN, _ = PPCA(subs_data_com_VN.copy(), filters=whitened_basis.T, threshold=0.0, niters=1)\n",
    "\n",
    "#     # Recalculate the basis via Haufe transform based on adjusted MIGP data\n",
    "#     tempbasis = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))@subs_data_com_VN\n",
    "\n",
    "#     # Rewhiten the basis\n",
    "#     whitened_basis, whitening_matrix = whiten(tempbasis,n_components=combined_spatial.shape[0],method=\"InvCov\",visualize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICA(data,whitened_data):\n",
    "    ica = FastICA(whiten=False)\n",
    "    # Takes in array-like of shape (n_samples, n_features) and returns ndarray of shape (n_samples, n_components)\n",
    "    IFA_components = ica.fit_transform(whitened_data.T).T\n",
    "    A = data@np.linalg.pinv(IFA_components)\n",
    "    W = np.linalg.pinv(A)\n",
    "    print(\"The combined unmixing matrix correctly calculates the components: \", np.allclose(W@data, IFA_components))\n",
    "    print(\"The combined mixing matrix correctly reconstructs the low rank data_demean: \", np.allclose(A@IFA_components, A@(W@data)))\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    # Heat map for the combined unmixing matrix\n",
    "    sns.heatmap(W@data, cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Combined Unmixing Matrix (W @ data)')\n",
    "    axes[0].set_xlabel('Components')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "\n",
    "    # Heat map for the IFA components\n",
    "    sns.heatmap(IFA_components, cmap='viridis', ax=axes[1])\n",
    "    axes[1].set_title('IFA Components')\n",
    "    axes[1].set_xlabel('Components')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return IFA_components, A, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_components_combined, A_combined, W_combined = ICA(subs_data_com_VN,whitened_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_data_VN_more, vtmore = PPCA(reducedsubs.copy(), threshold=0.0, niters=1,n=vt.shape[0]+filters.shape[1])\n",
    "vtmorewhiten,_ = whiten(vtmore,n_components=vtmore.shape[0],method=\"SVD\")\n",
    "subs_data_VN_more, _ = PPCA(reducedsubs.copy(), filters=vtmorewhiten.T, threshold=0.0, niters=1)\n",
    "\n",
    "raw_components_major_more, A_major_more, W_major_more = ICA(subs_data_VN_more,vtmorewhiten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"raw_components_combined.npy\"), raw_components_combined)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"A_combined.npy\"), A_combined)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"W_combined.npy\"), W_combined)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"raw_components_major_more.npy\"), raw_components_major_more)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"A_major_more.npy\"), A_major_more)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"W_major_more.npy\"), W_major_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_projection(W,data, visualize=True):\n",
    "\n",
    "    Signals = np.linalg.pinv(W)@(W@data)\n",
    "    Residuals = data - Signals\n",
    "    residual_std = np.std(Residuals,axis=0,ddof=np.linalg.matrix_rank(W))\n",
    "    # Trace of I-pinv(W)(W) is equal to the nullity (n-m gvien n > m) of the reconstructed matrix \n",
    "    # trace = data.shape[0] - np.linalg.matrix_rank(W)\n",
    "    # residual_std2 = (np.einsum('ij,ij->j', Residuals, Residuals)/(trace))**.5\n",
    "\n",
    "\n",
    "    if visualize:\n",
    "        n=1000\n",
    "        plt.figure()\n",
    "        plt.plot(Signals[:n,0:1])\n",
    "        plt.plot(Residuals[:n,0:1])\n",
    "        # plt.plot(data[:n,0:1])\n",
    "        # plt.plot(data[:n,0:1] - (Signals[:n,0:1]+Residuals[:n,0:1]))\n",
    "        plt.legend(['Signal','Noise', 'Data' ,'Reconstruction Error'])\n",
    "        plt.title(\"Calculations based on pinv(W)W Projection Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(range(0,residual_std.shape[0]), residual_std)\n",
    "        plt.title(\"Noise std Per Voxel based on pinv(W)W Projection Matrix\")\n",
    "        plt.show()\n",
    "    return residual_std\n",
    "\n",
    "\n",
    "def threshold_and_visualize(data, W, components,visualize=False):\n",
    "    \n",
    "    voxel_noise = noise_projection(W,data)[:, np.newaxis]\n",
    "    z_scores_array = np.zeros_like(components)\n",
    "    z_scores = np.zeros_like(components)\n",
    "\n",
    "    # Process each filter individually\n",
    "    for i in range(components.shape[1]):\n",
    "        z_score = ((components[:, i:i+1]))/voxel_noise\n",
    "        # P(Z < -z \\text{ or } Z > z) = (1 - \\text{CDF}(z)) + (1 - \\text{CDF}(z)) = 2 \\times (1 - \\text{CDF}(z))\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(z_score)))\n",
    "        # Apply multiple comparisons correction for the current filter https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "        reject, pvals_corrected, _, _ = multipletests(p_values.flatten(), alpha=0.05, method='fdr_bh')\n",
    "        masked_comp = z_score*(reject[:,np.newaxis])\n",
    "        # print(masked_comp, reject[:,np.newaxis],z_score)\n",
    "        z_scores_array[:, i:i+1] = masked_comp        \n",
    "        z_scores[:,i:i+1] = z_score\n",
    "\n",
    "       # Skip the iteration if there are no significant values\n",
    "        if not np.any(reject) and visualize:\n",
    "            print(f'Component {i} did not contain any significant values')\n",
    "            plt.figure()\n",
    "            plt.hist(z_score, bins=30, color='blue', alpha=0.7)\n",
    "            plt.title(f\"Histogram for Filter {i} NO SIGNIFICANT VALUES\")\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "        else:\n",
    "            if visualize:\n",
    "                # Create a figure and axes for subplots (1 row of 2 plots per filter)\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "                ax_hist1 = axes[0]\n",
    "                ax_img = axes[1]\n",
    "\n",
    "                # Plot the histogram of the current filter\n",
    "                ax_hist1.hist(z_score, bins=30, color='blue', alpha=0.7)\n",
    "                ax_hist1.set_title(f\"Histogram for Filter {i}\")\n",
    "                ax_hist1.set_xlabel('Value')\n",
    "                ax_hist1.set_ylabel('Frequency')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    # Heat map for the combined unmixing matrix\n",
    "    sns.heatmap(z_scores, cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('z_score')\n",
    "    axes[0].set_xlabel('Components')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "\n",
    "    # Heat map for the IFA components\n",
    "    sns.heatmap(z_scores_array, cmap='viridis', ax=axes[1])\n",
    "    axes[1].set_title('z_score thresh')\n",
    "    axes[1].set_xlabel('Components')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return z_scores, z_scores_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh, z_scores_thresh = threshold_and_visualize(subs_data_com_VN, W_combined, raw_components_combined.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh_major_more, z_scores_thresh_major_more = threshold_and_visualize(subs_data_VN_more, W_major_more, raw_components_major_more.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh.npy\"), z_scores_unthresh)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh.npy\"), z_scores_thresh)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh_major_more.npy\"), z_scores_unthresh_major_more)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh_major_more.npy\"), z_scores_thresh_major_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh.npy\"))\n",
    "z_scores_thresh = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh.npy\"))\n",
    "z_scores_unthresh_major_more = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh_major_more.npy\"))\n",
    "z_scores_thresh_major_more = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh_major_more.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00115/full\n",
    "\n",
    "def calculate_netmat_and_spatial_map(Xn, z_maps):\n",
    "    \"\"\"\n",
    "    Calculate the network matrix (netmat) and spatial map for a given subject and z_maps.\n",
    "    \n",
    "    Parameters:\n",
    "    Xn (array): Time x Grayordinates normalized data matrix (Time x V)\n",
    "    z_maps (array): Grayordinates x Components map (V x C)\n",
    "\n",
    "    Returns:\n",
    "    netmat (array): Components x Components network matrix (C x C)\n",
    "    spatial_map (array): Components x Grayordinates matrix (C x V)\n",
    "    \"\"\"\n",
    "    # Time x Components\n",
    "    # Demean the regressors (z_maps)\n",
    "    z_maps_demeaned = z_maps - z_maps.mean(axis=0, keepdims=True)  # Demean the columns of z_maps (V x C)\n",
    "    \n",
    "    # Time x Components\n",
    "    A = (Xn @ np.linalg.pinv(z_maps_demeaned.T))  # A is Time x Components (T x C)\n",
    "   \n",
    "    \n",
    "    # Normalized Time x Components matrix\n",
    "    An = hcp.normalize(A)  # An is Time x Components (T x C)\n",
    "    del A\n",
    "\n",
    "    # Components x Components network matrix\n",
    "    netmat = (An.T @ An) / (Xn.shape[0] - 1)  # Netmat is Components x Components (C x C)\n",
    "\n",
    "    # Components x Grayordinates spatial map\n",
    "    spatial_map = np.linalg.pinv(An) @ Xn  # Spatial map is Components x Grayordinates (C x V)\n",
    "\n",
    "    return An, netmat, spatial_map\n",
    "\n",
    "def dual_regress_sub(sub_path, z_maps_1, z_maps_2):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub_path:\n",
    "            # Load and preprocess each task\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)  # Grayordinates x Time (V x T)\n",
    "            Xn = hcp.normalize(X - X.mean(axis=1, keepdims=True))  # Normalizing (V x T)\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "        \n",
    "        # Concatenate data along the first axis (all tasks into one big matrix)\n",
    "        subject = np.concatenate(concatenated_data, axis=0)  # Time x Grayordinates (T x V)\n",
    "        del concatenated_data\n",
    "        \n",
    "        # Normalize the concatenated data\n",
    "        Xn = hcp.normalize(subject - subject.mean(axis=1,keepdims=True))  # Time x Grayordinates normalized data (T x V)\n",
    "        del subject\n",
    "        \n",
    "        # Calculate netmat and spatial map for the first set of z_maps\n",
    "        An_1, netmat_1, spatial_map_1 = calculate_netmat_and_spatial_map(Xn, z_maps_1)\n",
    "\n",
    "        # Calculate netmat and spatial map for the second set of z_maps\n",
    "        An_2, netmat_2, spatial_map_2 = calculate_netmat_and_spatial_map(Xn, z_maps_2)\n",
    "\n",
    "        return (An_1, netmat_1, spatial_map_1), (An_2, netmat_2, spatial_map_2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def dual_regress(group_paths, z_maps_1, z_maps_2):\n",
    "    # Use partial to avoid duplicating z_maps in memory\n",
    "    with ProcessPoolExecutor(max_workers=int(os.cpu_count() * 0.7)) as executor:\n",
    "        # Create a partial function that \"binds\" the z_maps_1 and z_maps_2 without duplicating them\n",
    "        partial_func = partial(dual_regress_sub, z_maps_1=z_maps_1, z_maps_2=z_maps_2)\n",
    "\n",
    "        # Pass the subject paths to the executor without copying z_maps\n",
    "        results = list(executor.map(partial_func, group_paths))\n",
    "        \n",
    "        # Separate the results for the two bases, collecting An, netmat, and spatial_map\n",
    "        An_1, netmats_1, spatial_maps_1 = zip(*[(res[0][0], res[0][1], res[0][2]) for res in results if res[0] is not None])\n",
    "        An_2, netmats_2, spatial_maps_2 = zip(*[(res[1][0], res[1][1], res[1][2]) for res in results if res[1] is not None])\n",
    "\n",
    "        return (np.array(An_1), np.array(netmats_1), np.array(spatial_maps_1)), (np.array(An_2), np.array(netmats_2), np.array(spatial_maps_2))\n",
    "\n",
    "# Save function for An, netmats, and spatial maps\n",
    "def save_numpy_arrays(output_prefix, An_1, netmats_1, spatial_maps_1, An_2, netmats_2, spatial_maps_2):\n",
    "    \"\"\"\n",
    "    Saves the An arrays, netmats, and spatial maps to disk using np.save.\n",
    "    \n",
    "    Parameters:\n",
    "    output_prefix (str): Prefix for the output files.\n",
    "    An_1 (np.array): Time x Components matrix for z_maps_1.\n",
    "    netmats_1 (np.array): Network matrices for z_maps_1.\n",
    "    spatial_maps_1 (np.array): Spatial maps for z_maps_1.\n",
    "    An_2 (np.array): Time x Components matrix for z_maps_2.\n",
    "    netmats_2 (np.array): Network matrices for z_maps_2.\n",
    "    spatial_maps_2 (np.array): Spatial maps for z_maps_2.\n",
    "    \"\"\"\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_An_1.npy\", An_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_netmats_1.npy\", netmats_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_spatial_maps_1.npy\", spatial_maps_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_An_2.npy\", An_2)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_netmats_2.npy\", netmats_2)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_spatial_maps_2.npy\", spatial_maps_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group A - Training Set\n",
    "(group_An_1_train, group_netmats_1_train, group_spatial_maps_1_train), (group_An_2_train, group_netmats_2_train, group_spatial_maps_2_train) = dual_regress(combined_paths_train, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(os.path.join(fold_outputfolder,\"group_train\"), group_An_1_train, group_netmats_1_train, group_spatial_maps_1_train, group_An_2_train, group_netmats_2_train, group_spatial_maps_2_train)\n",
    "# For Group A - Test Set\n",
    "(group_An_1_test, group_netmats_1_test, group_spatial_maps_1_test), (group_An_2_test, group_netmats_2_test, group_spatial_maps_2_test) = dual_regress(combined_paths_test, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(os.path.join(fold_outputfolder,\"group_test\"), group_An_1_test, group_netmats_1_test, group_spatial_maps_1_test, group_An_2_test, group_netmats_2_test, group_spatial_maps_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_netmats_1_train = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"group_train_netmats_1.npy\"))\n",
    "group_netmats_2_train = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"group_train_netmats_2.npy\"))\n",
    "group_netmats_1_test = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"group_test_netmats_1.npy\"))\n",
    "group_netmats_2_test = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"group_test_netmats_2.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_netmats_2_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tan_regression(train_netmats,test_netmats, train_values, test_values, train_age, test_age, metric=metric):\n",
    "    \n",
    "    # Plot the histogram of training data and calculated weights\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Plot histogram of training target values\n",
    "    plt.hist(train_values, bins=n_bins, alpha=0.7, color='blue', edgecolor='black', label='Train Data')\n",
    "    plt.title(f'Train Data Histogram and Weights')\n",
    "    plt.xlabel('Target Value')\n",
    "    plt.ylabel('Frequency / Weight')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    train_mean = mean_covariance(train_netmats, metric=metric)\n",
    "    tan_train = tangent_space(train_netmats, train_mean,metric=metric)\n",
    "    tan_test = tangent_space(test_netmats, train_mean,metric=metric)\n",
    "\n",
    "    tan_mean = np.mean(tan_train, axis=0)\n",
    "    tan_train_centered = tan_train - tan_mean\n",
    "    tan_test_centered = tan_test - tan_mean\n",
    "\n",
    "    age_reg = regress_out_age(tan_train_centered, train_age)\n",
    "    tan_train_centered = tan_train_centered - age_reg.predict(train_age.reshape(-1, 1))\n",
    "    tan_test_centered = tan_test - age_reg.predict(test_age.reshape(-1, 1))\n",
    "\n",
    "    # reg_model = Ridge(alpha=1)\n",
    "    reg_model = LinearSVR(C=1,fit_intercept=False)\n",
    "\n",
    "    reg_model.fit(tan_train_centered, train_values)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_score = reg_model.score(tan_test_centered, test_values)\n",
    "    predictions = reg_model.predict(tan_test_centered)\n",
    "    fold_corr, _ = pearsonr(test_values, predictions)\n",
    "\n",
    "    print(f\"Fold Test R² Score: {test_score}\")\n",
    "    print(f\"Fold Test R Score: {fold_corr}\")\n",
    "\n",
    "    # Plot predictions vs true values for test fold\n",
    "    plot_predictions(test_values, predictions,f'Test Predictions vs True Values - Fold R²: {test_score:.2f}', 'Test')\n",
    "    # Now plot for train fold\n",
    "    predictions_train = reg_model.predict(tan_train_centered)\n",
    "    plot_predictions(values_train, predictions_train, f'Train Predictions vs True Values - Fold R²: {test_score:.2f}', 'Train')\n",
    "     # Plot residuals vs true values for test fold\n",
    "    plot_residuals(test_values, predictions, f'Test Residuals vs True Values - Fold R²: {test_score:.2f}', 'Test')\n",
    "\n",
    "    # Plot residuals vs true values for train fold\n",
    "    plot_residuals(train_values, predictions_train, f'Train Residuals vs True Values - Fold R²: {test_score:.2f}', 'Train')\n",
    "\n",
    "\n",
    "tan_regression(group_netmats_1_train,group_netmats_1_test, values_train, values_test, age_train, age_test, metric=metric)\n",
    "tan_regression(group_netmats_2_train,group_netmats_2_test, values_train, values_test, age_train, age_test, metric=metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
