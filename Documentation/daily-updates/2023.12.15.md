# 2023.12.15

## General Notes

## Algorithm Design

### Developments

* For unsupervised analysis/individual analysis/no reference group, constructing a similarity matrix via summation of the eigenvalues variance from .5 or summation of the eigenvalues variance and then running MDS effectively discriminates the simulated data in a low dimension. I think it can be viewed as a manifold learning technique since it is unsupervised, the fisher discrimination can be viewed as metric learning, and then the MDS represents the data in a lower dimension. I am not sure if that interpretation is 100% correct, but I think it might be worthwhile to look into this problem from a manifold learning approach as well. For more on the relation of FDA/FKT to manifolds and metrics look into work from [Benyamin Ghojogh](https://arxiv.org/pdf/1906.09436.pdf). [This ](https://www-jstage-jst-go-jp.proxy.lib.umich.edu/article/jsaisigtwo/2007/DMSM-A603/2007\_04/\_pdf/-char/ja)seems to take a similar approach but is supervised I think, but look into it.

<figure><img src="../.gitbook/assets/image (5).png" alt=""><figcaption><p>Similarity matrix constructed using np.sum(abs(eigh(C1, C1+C2, eigvals_only=True)-.5))</p></figcaption></figure>

* With not as many shared components normal MDS with Euclidean similarity performs similarly. However, when there were more shared components, it appears the MDS with the Fisher discrimination similarity started to perform a little better. Maybe this can be better tested with complex data that has different noise to components ratio and shared components to discriminative components ratio

<div align="center">

<figure><img src="../.gitbook/assets/image (7).png" alt=""><figcaption><p>MDS with Similairty Matrix Constructed from Pairwise Fisher Discrimination</p></figcaption></figure>

</div>

<figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption><p>MDS with Similarity Matrix Constructed Using Euclidean Distance</p></figcaption></figure>

### Questions

## Code

### Developments

### ToDo
