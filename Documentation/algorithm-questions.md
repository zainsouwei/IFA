# Algorithm Questions

* [ ] Are the [spatial filters](https://github.com/zainsouwei/ICASPADE/blob/21adaa891aab69852804d4ae05bb6f2460be63d4/simulate\_time.py#L81C1-L83C6) always orthogonal with our data?
* [ ] Are SPADE Filters orthogonal
* [ ] How to best generalize to individual/no reference group case
* [ ] Bhattacharyya distance
* [ ] Computational cost of running ICA number of subject tiems vs once and then doing dual regression. Maybe doesnt matter cause can donit for subject of interest.
* [ ] Do the discrimination before ICA. Which means we are looking for low order discrimination. Which then means is the benefit of CC ICA that we look for higher order discrimination? How does CC-ICA performance compare/enforcing learning rule in the criterion
* [ ] How should we be denoising? This method seems prone to site effects
* [ ] What happens if all the discriminatory information is represented by higher-order information? I think it still might work in the case you view it as a linear approximation
* [ ] What is our extension of SPADE/what part of the pipeline? What are the works related to it? How does it differ/what are the benefits of this method?
* [ ] How to best generalize to individual/no reference group case?
* [ ] Is the best way to generalize to no reference group case one v all, pairwise comparisons, or add all separate covariance matrices?
* [ ] Look into group FKT paper
* [ ] What is the best distance similarity measure to form the within-subject FC? What is the best similarity measure to measure subject similarity? Is it right to say the parcellated covariance matrix is implicitly based on Euclidean distance and the SPADE algorithm is related to Bhattacharya distance since it uses two different covariance matrices? Are there other kernels we can use in formulating the parcellated covariance matrices and in the FKT/dissimilarity calculation? In forming the covariance matrices it is important to ask what space the vectors/observations live in. For doing the dissimilarity analysis it is important to figure out what space the covariance matrices live in. Should we relate the vectors/observations based on mean, polynomials, variance, and higher-order statistics? the same question applies to relating the separate covariance matrices
* [ ] Is it right that at the discrimination step we are taking more of a metric/manifold learning approach (including the covariances matrices should then be measuring Mahalanobis distance which is a metric) but when forming the [covariance matrix we are assuming euclidean space](https://www.astroml.org/book\_figures/chapter7/fig\_S\_manifold\_PCA.html)?
* [ ] Could you make it completely unsupervised by doing one vs. all and then comparing filters? Do pairwise and then group together observations (can do something like pairs, whose trace of eigenvalue matrix from filters is less than a threshold, are assumed to be close together ... or can make distance matrix by making entries between observations be the trace of eigenvalue matrix (of maybe first k filters or) or avg variance of difference from .5 of filters and then visualize via MDS) - maybe should not be trace since trace explains variance. If discriminative components do not make up much variance it might be poorly representative but since MDS would compare all traces maybe it's fine? [Could it potentially be viewed as a manifold learning approach](https://yao-lab.github.io/2019\_csic5011/slides/lecture13\_key.pdf)? Could be similar/build off [this](https://proceedings.neurips.cc/paper\_files/paper/2003/file/d69116f8b0140cdeb1f99a4d5096ffe4-Paper.pdf) or [this](https://www-jstage-jst-go-jp.proxy.lib.umich.edu/article/jsaisigtwo/2007/DMSM-A603/2007\_04/\_pdf/-char/ja)? Or [LPDP](https://link-springer-com.proxy.lib.umich.edu/chapter/10.1007/978-3-642-04020-7\_60)
* [ ] Which Fisher Discriminant based method to choose? Foley Sammon and FKT although similary make different uses/lack of uses of the mean. Additionally, in tradational FKT it was proposed that you use R which includes mean magnitude not mean direction so if there are positive and negative values then FKT might not be optimal. Is this something to be concerned about? HTC might be good for multiclass as well. I want to say no since we are not using the autocorrelation matrices and I think our data is demeaned but maybe something to look into
