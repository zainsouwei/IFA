{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('/project/3022057.01/IFA/utils')\n",
    "\n",
    "from preprocessing_ABIDE import parcellate, get_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcellate(\"/project/3022057.01/ABIDE\", target_shape=(195, 208),n_workers=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the directory containing the functional preprocessed files\n",
    "base_dir = \"/project/3022057.01/ABIDE/Outputs/ccs/filt_noglobal/func_preproc\"\n",
    "\n",
    "# Initialize a dictionary to store time series lengths by location\n",
    "time_series_lengths = {}\n",
    "\n",
    "# Get the list of files in the base directory\n",
    "files = [f for f in os.listdir(base_dir) if f.endswith(\"_func_preproc.nii.gz\")]\n",
    "\n",
    "# Dictionary to store one sample per location\n",
    "location_samples = defaultdict(list)\n",
    "\n",
    "# Parse the filenames to group by location\n",
    "for file_name in files:\n",
    "    # Extract location from the filename (assumes format \"Location_SubjectID_...\")\n",
    "    location = file_name.split(\"_\")[0]\n",
    "    location_samples[location].append(file_name)\n",
    "\n",
    "# Process one sample from each location\n",
    "for location, sample_files in location_samples.items():\n",
    "    sample_file = sample_files[0]  # Take the first file for this location\n",
    "    sample_path = os.path.join(base_dir, sample_file)\n",
    "    \n",
    "    try:\n",
    "        # Load the NIfTI file\n",
    "        img = nib.load(sample_path)\n",
    "        data = img.get_fdata()\n",
    "\n",
    "        # Extract time series length (number of time points)\n",
    "        time_series_length = data.shape[-1]  # Last dimension is time\n",
    "\n",
    "        # Store the result\n",
    "        time_series_lengths[location] = time_series_length\n",
    "\n",
    "        print(f\"Location: {location}, Sample File: {sample_file}, Time Series Length: {time_series_length}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {sample_file} in {location}: {e}\")\n",
    "\n",
    "# Summarize results\n",
    "print(\"\\nSummary of Time Series Lengths by Location:\")\n",
    "for location, length in time_series_lengths.items():\n",
    "    print(f\"{location}: {length} time points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/project/3022057.01/IFA/utils')\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pyriemann.estimation import Covariances\n",
    "import numpy as np\n",
    "from preprocessing_ABIDE import continuous_confounders, categorical_confounders\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import subprocess\n",
    "import pickle\n",
    "from analysis import evaluate_IFA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df = pd.read_pickle('/project/3022057.01/ABIDE/phenotype_parcellated_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"phenotype\": \"DX_GROUP\",\n",
    "    \"percentile\": 0.0,\n",
    "    \"outputfolder\": \"Run_11\",\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 2,\n",
    "    \"nPCA\": 30,\n",
    "    \"Tangent_Class\": True,\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": 1,\n",
    "    \"b_label\": 0,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": True\n",
    "}\n",
    "\n",
    "# Ensure the output folder exists\n",
    "outputfolder = settings[\"outputfolder\"]\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the settings file\n",
    "settings_filepath = os.path.join(outputfolder, \"settings.json\")\n",
    "\n",
    "# Save the settings to a JSON file\n",
    "with open(settings_filepath, \"w\") as f:\n",
    "    json.dump(settings, f, indent=4)\n",
    "\n",
    "phenotype = settings[\"phenotype\"]\n",
    "percentile = settings[\"percentile\"]\n",
    "n_folds = settings[\"n_folds\"]\n",
    "random_state = settings[\"random_state\"]\n",
    "n_filters_per_group = settings[\"n_filters_per_group\"]\n",
    "nPCA = settings[\"nPCA\"]\n",
    "Tangent_Class = settings[\"Tangent_Class\"]\n",
    "metric = settings[\"metric\"]\n",
    "a_label = int(settings[\"a_label\"])\n",
    "b_label = int(settings[\"b_label\"])\n",
    "self_whiten = settings[\"self_whiten\"]\n",
    "deconfound = settings[\"deconfound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              FILE_ID                                               Path  \\\n",
      "114      SDSU_0050182  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "115      SDSU_0050183  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "116      SDSU_0050184  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "117      SDSU_0050185  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "118      SDSU_0050186  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "..                ...                                                ...   \n",
      "772  Stanford_0051173  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "774  Stanford_0051175  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "775  Stanford_0051177  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "776  Stanford_0051178  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "777  Stanford_0051179  /project/3022057.01/ABIDE/Outputs/ccs/filt_nog...   \n",
      "\n",
      "     Unnamed: 0.1  Unnamed: 0  SUB_ID    X  subject   SITE_ID  DX_GROUP  \\\n",
      "114           121         122   50182  122    50182      SDSU         1   \n",
      "115           122         123   50183  123    50183      SDSU         1   \n",
      "116           123         124   50184  124    50184      SDSU         1   \n",
      "117           124         125   50185  125    50185      SDSU         1   \n",
      "118           125         126   50186  126    50186      SDSU         1   \n",
      "..            ...         ...     ...  ...      ...       ...       ...   \n",
      "772           851         852   51173  852    51173  STANFORD         1   \n",
      "774           853         854   51175  854    51175  STANFORD         1   \n",
      "775           855         856   51177  856    51177  STANFORD         1   \n",
      "776           856         857   51178  857    51178  STANFORD         1   \n",
      "777           857         858   51179  858    51179  STANFORD         1   \n",
      "\n",
      "     DSM_IV_TR  ...  qc_anat_rater_2        qc_anat_notes_rater_2  \\\n",
      "114          2  ...               OK                          NaN   \n",
      "115          1  ...               OK                          NaN   \n",
      "116          2  ...               OK                          NaN   \n",
      "117          1  ...            maybe  skull-striping fail; Motion   \n",
      "118          2  ...               OK                          NaN   \n",
      "..         ...  ...              ...                          ...   \n",
      "772      -9999  ...            maybe  skull-striping fail; Motion   \n",
      "774      -9999  ...            maybe  skull-striping fail; Motion   \n",
      "775      -9999  ...            maybe  skull-striping fail; Motion   \n",
      "776      -9999  ...            maybe  skull-striping fail; Motion   \n",
      "777      -9999  ...            maybe  skull-striping fail; Motion   \n",
      "\n",
      "    qc_func_rater_2  qc_func_notes_rater_2  qc_anat_rater_3  \\\n",
      "114              OK                    NaN               OK   \n",
      "115              OK                    NaN               OK   \n",
      "116              OK                    NaN               OK   \n",
      "117              OK                    NaN             fail   \n",
      "118              OK                    NaN               OK   \n",
      "..              ...                    ...              ...   \n",
      "772              OK                    NaN               OK   \n",
      "774              OK                    NaN             fail   \n",
      "775              OK                    NaN               OK   \n",
      "776              OK                    NaN               OK   \n",
      "777              OK                    NaN               OK   \n",
      "\n",
      "     qc_anat_notes_rater_3  qc_func_rater_3 qc_func_notes_rater_3 SUB_IN_SMP  \\\n",
      "114                    NaN               OK                   NaN          1   \n",
      "115                    NaN               OK                   NaN          1   \n",
      "116                    NaN               OK                   NaN          0   \n",
      "117             headmotion               OK                   NaN          0   \n",
      "118                    NaN               OK                   NaN          1   \n",
      "..                     ...              ...                   ...        ...   \n",
      "772                    NaN               OK                   NaN          1   \n",
      "774             headmotion               OK                   NaN          1   \n",
      "775                    NaN               OK                   NaN          1   \n",
      "776                    NaN               OK                   NaN          1   \n",
      "777                    NaN               OK                   NaN          1   \n",
      "\n",
      "                                      parcellated_data  \n",
      "114  [[-0.23801802, -1.2389371, 0.540491, -0.048545...  \n",
      "115  [[-1.3276856, 0.059082713, -1.2453284, -0.6207...  \n",
      "116  [[-0.99969697, -0.7751156, -0.33219466, -0.273...  \n",
      "117  [[-1.6158843, -0.039085865, 0.8567465, 1.24068...  \n",
      "118  [[-0.038629845, 0.5593748, -0.5994145, 0.70700...  \n",
      "..                                                 ...  \n",
      "772  [[0.73117524, 0.7232058, -0.61295086, 0.771411...  \n",
      "774  [[0.055658564, 0.22532713, -0.013450778, 1.515...  \n",
      "775  [[0.88250124, 0.80589795, -0.4235815, 0.234494...  \n",
      "776  [[-0.7134422, 0.14948826, 0.7395593, 0.6388272...  \n",
      "777  [[0.42206076, -0.61311054, -0.70542026, -1.376...  \n",
      "\n",
      "[226 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "vals = np.array([mat.shape[0] for mat in loaded_df[\"parcellated_data\"]])\n",
    "# Find the most frequent shape[0]\n",
    "most = stats.mode(vals)[0]\n",
    "\n",
    "# Filter the DataFrame to keep rows with the most frequent shape[0]\n",
    "loaded_df_filtered = loaded_df[[mat.shape[0] == most for mat in loaded_df[\"parcellated_data\"]]]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(loaded_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "from tangent import tangent_classification\n",
    "\n",
    "# Define the conditions for the split\n",
    "condition_low_iq = (loaded_df_filtered['DX_GROUP'] == 1)  # ASD and Low IQ\n",
    "condition_high_iq = (loaded_df_filtered['DX_GROUP'] == 2)  # ASD and High IQ\n",
    "\n",
    "# Split the dataset into two groups\n",
    "a = loaded_df_filtered[condition_low_iq]  # Group A: ASD + Low IQ\n",
    "b = loaded_df_filtered[condition_high_iq]  # Group B: ASD + High IQ\n",
    "\n",
    "# min_size = min(len(a), len(b))\n",
    "# # Sample rows randomly from each group\n",
    "# a = a.sample(n=min_size, random_state=random_state)\n",
    "# b = b.sample(n=min_size, random_state=random_state)\n",
    "target_length = min(len(series) for series in a[\"parcellated_data\"])  # Smallest time series length\n",
    "print(target_length)\n",
    "cov_est = Covariances(estimator='oas')\n",
    "\n",
    "A_data = np.array([series[:target_length, :] for series in a[\"parcellated_data\"]])\n",
    "A_covs = cov_est.transform(np.transpose(A_data, (0, 2, 1)))\n",
    "A_paths = a[\"Path\"].to_numpy()\n",
    "A_con_confounders = np.stack(a[continuous_confounders].to_numpy())\n",
    "A_cat_confounders = np.stack(a[categorical_confounders].to_numpy())\n",
    "\n",
    "B_data = np.array([series[:target_length, :] for series in b[\"parcellated_data\"]])\n",
    "B_covs = cov_est.transform(np.transpose(B_data, (0, 2, 1)))\n",
    "B_paths = b[\"Path\"].to_numpy()\n",
    "B_con_confounders = np.stack(b[continuous_confounders].to_numpy())\n",
    "B_cat_confounders = np.stack(b[categorical_confounders].to_numpy())\n",
    "\n",
    "labels = np.concatenate([a_label*np.ones(len(A_data), dtype=int), b_label*np.ones(len(B_data), dtype=int)])\n",
    "data = np.concatenate([A_data, B_data], axis=0)\n",
    "covs = np.concatenate([A_covs, B_covs], axis=0)\n",
    "paths = np.concatenate([A_paths, B_paths], axis=0)\n",
    "with open(os.path.join(outputfolder, \"paths.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(paths, f)\n",
    "con_confounders = np.concatenate([A_con_confounders, B_con_confounders], axis=0)\n",
    "cat_confounders = np.concatenate([A_cat_confounders, B_cat_confounders], axis=0)\n",
    "with open(os.path.join(outputfolder, \"cat_confounders.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cat_confounders, f)\n",
    "\n",
    "# sgkf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "# splits = list(sgkf.split(data, labels))\n",
    "# accuracy_per_fold = defaultdict(list)  # Stores accuracies for each classifier\n",
    "# accuracy_per_fold_deconf = defaultdict(list)  # Stores accuracies for deconfounded data\n",
    "\n",
    "# for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "#     train_labels = labels[train_idx]\n",
    "#     train_data = data[train_idx]\n",
    "#     train_covs = covs[train_idx]\n",
    "#     train_paths = paths[train_idx]\n",
    "#     train_con_confounders = con_confounders[train_idx]\n",
    "#     train_cat_confounders = cat_confounders[train_idx]\n",
    "\n",
    "#     test_labels = labels[test_idx]\n",
    "#     test_data = data[test_idx]\n",
    "#     test_covs = covs[test_idx]\n",
    "#     test_con_confounders = con_confounders[test_idx]\n",
    "#     test_cat_confounders = cat_confounders[test_idx]\n",
    "#     print(\"here\")\n",
    "\n",
    "#     tangent_class_metrics = tangent_classification(\n",
    "#         train_covs, train_labels, test_covs, test_labels, clf_str='all', z_score=0, metric=metric, deconf=False\n",
    "#     )\n",
    "#     print(\"here\")\n",
    "\n",
    "#     tangent_class_metrics_deconf = tangent_classification(\n",
    "#         train_covs, train_labels, test_covs, test_labels, clf_str='all', z_score=0, metric=metric, deconf=True,\n",
    "#         con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders,\n",
    "#         con_confounder_test=test_con_confounders, cat_confounder_test=test_cat_confounders\n",
    "#     )\n",
    "#     print(\"here\")\n",
    "\n",
    "# # Track accuracies for each classifier\n",
    "# for clf, metrics in tangent_class_metrics.items():\n",
    "#     print(\"here\")\n",
    "#     accuracy_per_fold[clf].append(metrics[\"accuracy\"])\n",
    "\n",
    "# for clf, metrics in tangent_class_metrics_deconf.items():\n",
    "#     print(\"here\")\n",
    "#     accuracy_per_fold_deconf[clf].append(metrics[\"accuracy\"])\n",
    "\n",
    "# # Calculate average accuracies across folds\n",
    "# average_accuracy = {clf: np.mean(accs) for clf, accs in accuracy_per_fold.items()}\n",
    "# average_accuracy_deconf = {clf: np.mean(accs) for clf, accs in accuracy_per_fold_deconf.items()}\n",
    "\n",
    "# # Print results\n",
    "# print(\"Average accuracies (non-deconfounded):\", average_accuracy)\n",
    "# print(\"Average accuracies (deconfounded):\", average_accuracy_deconf)\n",
    "\n",
    "np.save(os.path.join(outputfolder,\"labels.npy\"),labels)\n",
    "np.save(os.path.join(outputfolder,\"data.npy\"),data)\n",
    "np.save(os.path.join(outputfolder,\"covs.npy\"),covs)\n",
    "np.save(os.path.join(outputfolder,\"con_confounders.npy\"),con_confounders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 46625341\n",
      "Submitted batch job 46625342\n",
      "Submitted batch job 46625343\n",
      "Submitted batch job 46625344\n",
      "Submitted batch job 46625345\n"
     ]
    }
   ],
   "source": [
    "# Stratified k-fold setup\n",
    "# https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html\n",
    "sgkf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "splits = list(sgkf.split(data, labels))\n",
    "run_fold_script = \"/project/3022057.01/IFA/run_IFA/run_fold.sh\"\n",
    "\n",
    "# New job per fold\n",
    "for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    # Create Fold Outputfolder\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "    if not os.path.exists(fold_output_dir):\n",
    "        os.makedirs(fold_output_dir)\n",
    "    \n",
    "    indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "    if not os.path.exists(indices_dir):\n",
    "        os.makedirs(indices_dir)\n",
    "    np.save(os.path.join(indices_dir, \"train_idx.npy\"), train_idx)\n",
    "    np.save(os.path.join(indices_dir, \"test_idx.npy\"), test_idx)\n",
    "\n",
    "    # Prepare SLURM command to call `run_fold.sh` with arguments for outputfolder and fold\n",
    "    command = [\n",
    "        \"sbatch\",\n",
    "        \"--job-name\", f\"fold_{fold}\",\n",
    "        \"--output\", os.path.join(fold_output_dir, \"slurm-%j.out\"),\n",
    "        \"--error\", os.path.join(fold_output_dir, \"slurm-%j.err\"),\n",
    "        run_fold_script,  # Path to `run_fold.sh`\n",
    "        outputfolder,     # Pass outputfolder as first argument\n",
    "        str(fold)         # Pass fold as second argument\n",
    "    ]\n",
    "    \n",
    "    # Submit the job\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "from pathlib import Path\n",
    "\n",
    "def load_results(output_folder, prefix, subfold=\"Demeaned\"):\n",
    "    results = {\n",
    "        f\"{prefix}_var_results\": [],\n",
    "        f\"{prefix}_cov_results\": [],\n",
    "        f\"{prefix}_Class_Result\": ([], []),\n",
    "        f\"{prefix}_recon\": ([], [])\n",
    "    }\n",
    "    for fold in range(0,5):\n",
    "        fold_results_file = Path(output_folder) / f\"fold_{fold}\" / \"Results\" / f\"{subfold}\" /f\"{prefix}_results_{subfold.lower()}.pkl\"\n",
    "        with open(fold_results_file, \"rb\") as f:\n",
    "            fold_data = pickle.load(f)\n",
    "            \n",
    "        for key in results.keys():\n",
    "            if \"Class\" in key:\n",
    "                results[key][0].append(list(fold_data[key].keys()))\n",
    "                results[key][1].append([fold_data[key][class_key][\"accuracy\"] for class_key in fold_data[key].keys()])\n",
    "            elif \"recon\" in key:\n",
    "                results[key][0].extend(fold_data[key][0])\n",
    "                results[key][1].extend(fold_data[key][1])\n",
    "            else:\n",
    "                results[key].append(fold_data[key])\n",
    "    return results\n",
    "\n",
    "def summarize_results(results):\n",
    "    summary = {key: [] for key in results.keys()}\n",
    "    for key, values in results.items():\n",
    "        values_array = np.array(values if \"Class\" not in key and \"recon\" not in key else values[1])\n",
    "        if \"_var_results\" in key or \"_cov_results\" in key:\n",
    "            summary[key].extend([np.mean(values_array, axis=0), sem(values_array, axis=0)])\n",
    "        elif \"Class\" in key:\n",
    "            classifiers = results[key][0][0]\n",
    "            avg_accuracy = np.mean(values_array, axis=0)\n",
    "            std_error = sem(values_array, axis=0)\n",
    "            summary[key] = (classifiers, avg_accuracy, std_error)\n",
    "        elif \"recon\" in key:\n",
    "            summary[key] = values\n",
    "    return summary\n",
    "\n",
    "# Define output folders and load/save results\n",
    "output_folder = Path(outputfolder)\n",
    "\n",
    "# Process IFA and ICA results\n",
    "IFA_all_results_norm = load_results(output_folder, \"IFA\",subfold=\"Normalized\")\n",
    "IFA_results_summary_norm = summarize_results(IFA_all_results_norm)\n",
    "\n",
    "ICA_all_results_norm = load_results(output_folder, \"ICA\", subfold=\"Normalized\")\n",
    "ICA_results_summary_norm = summarize_results(ICA_all_results_norm)\n",
    "\n",
    "\n",
    "# Process IFA and ICA results\n",
    "IFA_all_results_demean = load_results(output_folder, \"IFA\",subfold=\"Demeaned\")\n",
    "IFA_results_summary_demean = summarize_results(IFA_all_results_demean)\n",
    "\n",
    "ICA_all_results_demean = load_results(output_folder, \"ICA\", subfold=\"Demeaned\")\n",
    "ICA_results_summary_demean = summarize_results(ICA_all_results_demean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ([],[])\n",
    "for fold in range(0,5):\n",
    "    if deconfound:\n",
    "        end = \"_deconf\"\n",
    "    else:\n",
    "        end = \"\"\n",
    "    baseline_file = Path(output_folder) / f\"fold_{fold}\" / \"Results\" /f\"tangent_class_metrics{end}.pkl\"\n",
    "    with open(baseline_file, \"rb\") as f:\n",
    "        baseline_data = pickle.load(f)\n",
    "    baseline[0].append(list(baseline_data.keys()))\n",
    "    baseline[1].append([baseline_data[class_key][\"accuracy\"] for class_key in baseline_data.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from analysis import reconstruction_plot\n",
    "\n",
    "def plot_model_comparison(x, mean_IFA, sterr_IFA, mean_ICA, sterr_ICA,xlabel=\"\",ylabel=\"\",title=\"\",output_dir=\"path\", baseline=None):   \n",
    "    # Increase the spacing between bins\n",
    "    spacing_factor = 10  # Increased spacing factor from 2 to 3\n",
    "    x_positions = np.arange(len(x)) * spacing_factor\n",
    "    \n",
    "    # Width for offsets (should be less than half of spacing_factor)\n",
    "    width = (spacing_factor / 5)*0\n",
    "    \n",
    "    # Offsets for each method\n",
    "    offsets = [-width, 0, width]\n",
    "    \n",
    "    # Adjust x-values for each method\n",
    "    x_IFA = x_positions + offsets[0]\n",
    "    x_ICA = x_positions + offsets[2]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  # Adjust the width (e.g., 12) to make the figure wider\n",
    "    # Plotting TSSF, ICA, and FKT with adjusted x-values\n",
    "    plt.errorbar(x_ICA, mean_ICA, yerr=sterr_ICA, fmt='o', label='ICA', color='orange', capsize=0)\n",
    "    plt.errorbar(x_IFA, mean_IFA, yerr=sterr_IFA, fmt='o', label='IFA', color='blue', capsize=0)\n",
    "    if baseline is not None:\n",
    "        mean_baseline = np.mean(baseline[1], axis=0)\n",
    "        sem_baseline = sem(baseline[1], axis=0)\n",
    "\n",
    "        # Plot the baseline line\n",
    "        plt.plot(x_positions, mean_baseline, label='Baseline', color='red')\n",
    "\n",
    "        # Add shaded error region\n",
    "        plt.fill_between(\n",
    "            x_positions,\n",
    "            mean_baseline - sem_baseline,\n",
    "            mean_baseline + sem_baseline,\n",
    "            color='red',\n",
    "            alpha=0.3,  # Transparency of the shading\n",
    "        )\n",
    "\n",
    "    # Set x-ticks to x_positions without offsets, labels to models\n",
    "    plt.xticks(x_positions, x, rotation=45, ha='right')\n",
    "\n",
    "    \n",
    "    # Formatting plot\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f'{title}.svg'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_output_dir_normalized = output_folder / \"Results_Normalized\"\n",
    "all_results_output_dir_normalized.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_var_results\"][0][:,0],IFA_results_summary_norm[\"IFA_var_results\"][0][:,2],IFA_results_summary_norm[\"IFA_var_results\"][1][:,2],ICA_results_summary_norm[\"ICA_var_results\"][0][:,2],ICA_results_summary_norm[\"ICA_var_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Var_Accuracy_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_var_results\"][0][:,0],IFA_results_summary_norm[\"IFA_var_results\"][0][:,1],IFA_results_summary_norm[\"IFA_var_results\"][1][:,1],ICA_results_summary_norm[\"ICA_var_results\"][0][:,1],ICA_results_summary_norm[\"ICA_var_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Var_Distance_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_cov_results\"][0][:,0],IFA_results_summary_norm[\"IFA_cov_results\"][0][:,2],IFA_results_summary_norm[\"IFA_cov_results\"][1][:,2],ICA_results_summary_norm[\"ICA_cov_results\"][0][:,2],ICA_results_summary_norm[\"ICA_cov_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Cov_Accuracy_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_cov_results\"][0][:,0],IFA_results_summary_norm[\"IFA_cov_results\"][0][:,1],IFA_results_summary_norm[\"IFA_cov_results\"][1][:,1],ICA_results_summary_norm[\"ICA_cov_results\"][0][:,1],ICA_results_summary_norm[\"ICA_cov_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Cov_Distance_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_Class_Result\"][0],IFA_results_summary_norm[\"IFA_Class_Result\"][1],IFA_results_summary_norm[\"IFA_Class_Result\"][2],ICA_results_summary_norm[\"ICA_Class_Result\"][1],ICA_results_summary_norm[\"ICA_Class_Result\"][2],xlabel=\"Model\", ylabel=\"Accuracy\", title=\"Tangent Netmat Accuracy Across Folds\",output_dir=all_results_output_dir_normalized, baseline=baseline)\n",
    "\n",
    "reconstruction_plot(IFA_results_summary_norm[\"IFA_recon\"][0], ICA_results_summary_norm[\"ICA_recon\"][0],label=\"Train\",output_dir=all_results_output_dir_normalized)\n",
    "reconstruction_plot(IFA_results_summary_norm[\"IFA_recon\"][1], ICA_results_summary_norm[\"ICA_recon\"][1],label=\"Test\",output_dir=all_results_output_dir_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_output_dir_demean = output_folder / \"Results_Demeaned\"\n",
    "all_results_output_dir_demean.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_var_results\"][0][:,0],IFA_results_summary_demean[\"IFA_var_results\"][0][:,2],IFA_results_summary_demean[\"IFA_var_results\"][1][:,2],ICA_results_summary_demean[\"ICA_var_results\"][0][:,2],ICA_results_summary_demean[\"ICA_var_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Var_Accuracy_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_var_results\"][0][:,0],IFA_results_summary_demean[\"IFA_var_results\"][0][:,1],IFA_results_summary_demean[\"IFA_var_results\"][1][:,1],ICA_results_summary_demean[\"ICA_var_results\"][0][:,1],ICA_results_summary_demean[\"ICA_var_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Var_Distance_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_cov_results\"][0][:,0],IFA_results_summary_demean[\"IFA_cov_results\"][0][:,2],IFA_results_summary_demean[\"IFA_cov_results\"][1][:,2],ICA_results_summary_demean[\"ICA_cov_results\"][0][:,2],ICA_results_summary_demean[\"ICA_cov_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Cov_Accuracy_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_cov_results\"][0][:,0],IFA_results_summary_demean[\"IFA_cov_results\"][0][:,1],IFA_results_summary_demean[\"IFA_cov_results\"][1][:,1],ICA_results_summary_demean[\"ICA_cov_results\"][0][:,1],ICA_results_summary_demean[\"ICA_cov_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Cov_Distance_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_Class_Result\"][0],IFA_results_summary_demean[\"IFA_Class_Result\"][1],IFA_results_summary_demean[\"IFA_Class_Result\"][2],ICA_results_summary_demean[\"ICA_Class_Result\"][1],ICA_results_summary_demean[\"ICA_Class_Result\"][2],xlabel=\"Model\", ylabel=\"Accuracy\", title=\"Tangent Netmat Accuracy Across Folds\",output_dir=all_results_output_dir_demean, baseline=baseline)\n",
    "\n",
    "reconstruction_plot(IFA_results_summary_demean[\"IFA_recon\"][0], ICA_results_summary_demean[\"ICA_recon\"][0],label=\"Train\",output_dir=all_results_output_dir_demean)\n",
    "reconstruction_plot(IFA_results_summary_demean[\"IFA_recon\"][1], ICA_results_summary_demean[\"ICA_recon\"][1],label=\"Test\",output_dir=all_results_output_dir_demean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot time series for parcellated data\n",
    "def plot_time_series(data, title=\"Time Series\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for parcel_idx, parcel_series in enumerate(data):  # Each row corresponds to a parcel\n",
    "        plt.plot(parcel_series, label=f\"Parcel {parcel_idx+1}\" if parcel_idx < 5 else \"\", alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time Points\")\n",
    "    plt.ylabel(\"Signal Intensity\")\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1), ncol=1, fontsize='small', frameon=False)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_time_series(loaded_df[\"parcellated_data\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_roi, plot_stat_map, view_img\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "# Display the parcellated image over the brain\n",
    "atlas_path = \"/project/3022057.01/resources/rois/ICPAtlas_v4_fine_208parcels.nii.gz\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_img, standardize=False)\n",
    "masker.fit()\n",
    "unmask = masker.inverse_transform(loaded_df[\"parcellated_data\"][0])\n",
    "plot_stat_map(image.index_img(unmask, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import view_img\n",
    "view_img(image.index_img(unmask, 0), title=\"Interactive Parcellated Brain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import center_of_mass\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# Load atlas image\n",
    "atlas_path = \"/project/3022057.01/resources/rois/ICPAtlas_v4_fine_208parcels.nii.gz\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()  # Extract the data array\n",
    "\n",
    "# Get unique region labels (excluding background)\n",
    "labels = np.unique(atlas_data)\n",
    "labels = labels[labels > 0]  # Exclude background (assumed to be 0)\n",
    "\n",
    "# Compute region centroids (coordinates in voxel space)\n",
    "coords = [\n",
    "    center_of_mass(atlas_data == label) for label in labels\n",
    "]\n",
    "\n",
    "# Convert voxel coordinates to world coordinates using atlas affine\n",
    "coords_world = nib.affines.apply_affine(atlas_img.affine, coords)\n",
    "\n",
    "# Print results\n",
    "print(\"Region Coordinates (World Space):\", coords_world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "correlation_measure = ConnectivityMeasure(\n",
    "    kind=\"precision\",\n",
    "    standardize=\"zscore_sample\",\n",
    ")\n",
    "correlation_matrix = correlation_measure.fit_transform([loaded_df[\"parcellated_data\"][0]])[0]\n",
    "\n",
    "# Display the correlation matrix\n",
    "import numpy as np\n",
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "# Mask out the major diagonal\n",
    "np.fill_diagonal(correlation_matrix, 0)\n",
    "plotting.plot_matrix(\n",
    "    correlation_matrix, labels=labels, colorbar=True, vmax=0.8, vmin=-0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We threshold to keep only the 20% of edges with the highest value\n",
    "# because the graph is very dense\n",
    "plotting.plot_connectome(\n",
    "    correlation_matrix, coords_world, edge_threshold=\"99.9%\", colorbar=True\n",
    ")\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = plotting.view_connectome(\n",
    "    correlation_matrix, coords_world, edge_threshold=\"99.9%\"\n",
    ")\n",
    "\n",
    "# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n",
    "# be displayed below the cell\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_roi\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# Load the atlas\n",
    "atlas_path = \"/project/3022057.01/resources/rois/ICPAtlas_v4_fine_208parcels.nii.gz\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "# Create a binary mask for the region where atlas == 0\n",
    "zero_mask = (atlas_data == 0).astype(int)\n",
    "zero_mask_img = nib.Nifti1Image(zero_mask, affine=atlas_img.affine)\n",
    "\n",
    "# Highlight the 0-labeled region on the brain\n",
    "plot_roi(zero_mask_img, title=\"Region Where Atlas = 0\", display_mode=\"ortho\", draw_cross=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = \"SUI009\"\n",
    "print(sum(df[check].dropna()))\n",
    "print(len(df[check].dropna()))\n",
    "print(sum(df[check].dropna())/len(df[check].dropna()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
