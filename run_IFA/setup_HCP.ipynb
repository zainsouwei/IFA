{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Spatial Basis & Joint Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import logm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh, subspace_angles\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "from pyriemann.utils.tangentspace import untangent_space, unupper\n",
    "from pyriemann.estimation import Covariances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.linalg import svd\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import ScipyMinimize\n",
    "from pymanopt.manifolds import Stiefel, Grassmann, Sphere\n",
    "from pymanopt.optimizers import ConjugateGradient, TrustRegions\n",
    "from pymanopt.optimizers.nelder_mead import compute_centroid\n",
    "from pymanopt import Problem\n",
    "import pymanopt\n",
    "from jax import debug\n",
    "\n",
    "import sys\n",
    "sys.path.append('/utils')\n",
    "\n",
    "from tangent import tangent_transform\n",
    "from classification import linear_classifier, clf_dict\n",
    "from haufe import haufe_transform\n",
    "from regression import deconfound\n",
    "\n",
    "def TSSF(covs, labels, clf_str=\"L2 SVM (C=1)\", metric=\"riemann\", deconf=True, con_confounder_train=None, cat_confounder_train=None, z_score=2, haufe=True, visualize=False,output_dir=\"plots\"):\n",
    "    clf = clf_dict[clf_str]\n",
    "    # https://ieeexplore.ieee.org/abstract/document/9630144/references#references\n",
    "    # https://arxiv.org/abs/1909.10567\n",
    "    data, Frechet_Mean = tangent_transform(covs,metric=metric)\n",
    "\n",
    "    if deconf:\n",
    "        data = deconfound(data, con_confounder_train, cat_confounder_train, X_test=None, con_confounder_test=None, cat_confounder_test=None)\n",
    "\n",
    "    if z_score == 1:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "        data = scaler.fit_transform(data)\n",
    "    elif z_score == 2:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        data = scaler.fit_transform(data)\n",
    "\n",
    "    clf.fit(data, labels)\n",
    "\n",
    "    coef = np.atleast_2d(clf.coef_)\n",
    "    if coef.shape[1] != data.shape[1]:\n",
    "        coef = coef.T\n",
    "\n",
    "    if haufe:\n",
    "        coef = haufe_transform(data, clf.coef_.T,method=\"basic\")\n",
    "    \n",
    "    # boundary_matrix = untangent_space(coef, Frechet_Mean, metric=metric)[0,:,:]\n",
    "    # eigs, filters  = eigh(boundary_matrix, Frechet_Mean)\n",
    "    # riem_eig = (np.log(eigs))**2\n",
    "    # TODO Dffferent transformation functions from each paper\n",
    "    # fkt_riem_eigs = np.maximum(eigs,1/eigs)\n",
    "    # TODO Specific for Log Euclidean case\n",
    "    # Equivalent to the 2 norm of the distance to reference eigenvalue on the tangent space\n",
    "    # sqrt((log(lamda) - log(1))^2) == abs(log(lamda) - log(1)) == abs(log(lambda))\n",
    "    # riem_eig = np.abs(np.log(eigs))\n",
    "        \n",
    "    boundary_matrix = unupper(coef)[0,:,:]\n",
    "    eigs, filters  = eigh(boundary_matrix)\n",
    "    riem_eig = ((eigs))**2\n",
    "\n",
    "    if visualize:\n",
    "        plt.scatter(range(0,riem_eig.shape[0]),riem_eig)\n",
    "        plt.title(\"Riemannian Distance Supported by Spatial Filter\")\n",
    "        plt.xlabel(\"Max Eigenvector for Group B to Max Eigenvector for Group A\")\n",
    "        # plt.ylabel(r\"$|log(\\lambda)|$\")\n",
    "        plt.ylabel(r\"$\\log(\\lambda)^2$\")\n",
    "        plt.savefig(os.path.join(output_dir, \"fkt_scree.svg\"))\n",
    "\n",
    "    return riem_eig, filters, boundary_matrix, Frechet_Mean\n",
    "\n",
    "\n",
    "def dual_projection(X, F):\n",
    "    \"\"\"\n",
    "    Perform dual regression (projection) using the matrix F.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input data matrix (subjects x components x features).\n",
    "        F: Projection matrix.\n",
    "\n",
    "    Returns:\n",
    "        Dual-projected data matrix.\n",
    "    \"\"\"\n",
    "    proj = X @ jnp.linalg.pinv(F.T)\n",
    "    proj_demeaned = proj - jnp.mean(proj,axis=1,keepdims=True)\n",
    "    return jnp.linalg.pinv(proj_demeaned) @ X\n",
    "\n",
    "def resid_calc(orig, filt):\n",
    "    recon = jnp.linalg.pinv(filt.T)@filt.T@orig@filt@jnp.linalg.pinv(filt)\n",
    "    residuals = (orig) - (recon)\n",
    "    # Compute variances\n",
    "    var_residuals = jnp.var(residuals, ddof=1)\n",
    "    var_original = jnp.var(orig, ddof=1)\n",
    "    # Calculate variance explained\n",
    "    variance_explained = (1 - (var_residuals / var_original))\n",
    "    return variance_explained\n",
    "\n",
    "# def var_calc(orig, filt):\n",
    "#     orig_stack = np.vstack(orig)\n",
    "#     recon = orig_stack@filt@jnp.linalg.pinv(filt)\n",
    "#     residuals = (orig_stack) - (recon)\n",
    "#     # Compute variances\n",
    "#     var_residuals = jnp.var(residuals, ddof=1)\n",
    "#     var_original = jnp.var(orig_stack, ddof=1)\n",
    "#     # Calculate variance explained\n",
    "#     variance_explained = (1 - (var_residuals / var_original))\n",
    "    # return variance_explained\n",
    "\n",
    "def connectivity_cost(F, W, mode=\"trace_ratio\"):\n",
    "    \"\"\"\n",
    "    Compute the connectivity term of the cost function.\n",
    "\n",
    "    Parameters:\n",
    "        F: Projection matrix.\n",
    "        C, W: Covariance matrices for connectivity differences.\n",
    "        mode: 'trace_ratio' or 'log_trace_difference'.\n",
    "\n",
    "    Returns:\n",
    "        Connectivity cost value.\n",
    "    \"\"\"\n",
    "    if mode == \"trace_ratio\":\n",
    "        return jnp.trace(F.T @ W @ F)\n",
    "    elif mode == \"var_explained\":\n",
    "        total_cost = resid_calc(W, F)\n",
    "        return total_cost\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'trace_ratio' or 'log_trace_difference'.\")\n",
    "\n",
    "def multitransp(A):\n",
    "    \"\"\"Vectorized matrix transpose for JAX.\"\"\"\n",
    "    if A.ndim == 2:\n",
    "        return A.T\n",
    "    return jnp.transpose(A, (0, 2, 1))\n",
    "\n",
    "def grassmann_dist_jax(A, B):\n",
    "    \"\"\"Compute the Grassmann distance between two points using JAX.\"\"\"\n",
    "    A_q, _ = jnp.linalg.qr(A.T)\n",
    "    B_q, _ = jnp.linalg.qr(B.T)\n",
    "    # print(A_q.shape, B_q.shape)\n",
    "    product = multitransp(A_q) @ B_q\n",
    "    s = jnp.linalg.svd(product, compute_uv=False)\n",
    "    angles = jnp.arccos(s)  # Convert to principal angles\n",
    "    # print(angles)\n",
    "    # print(s,angles)\n",
    "    # scipy_angles = subspace_angles(A.T, B.T)\n",
    "    # if not jnp.allclose(angles, scipy_angles):\n",
    "    #     raise ValueError(\"Grassmann distance mismatch with scipy's subspace angles!\")\n",
    "    return jnp.linalg.norm(angles)**2\n",
    "    return (jnp.linalg.norm(angles))\n",
    "    # return jnp.mean(angles)\n",
    "    # return jnp.linalg.norm(angles,ord=-jnp.inf)\n",
    "    return (jnp.linalg.norm(angles,ord=2)**2)\n",
    "\n",
    "\n",
    "\n",
    "# # Objective function for Nelder-Mead optimization\n",
    "# def karcher_mean_objective(y, points):\n",
    "#     \"\"\"\n",
    "#     Objective function to compute the Karcher mean.\n",
    "#     y: Candidate centroid on the Grassmann manifold.\n",
    "#     points: A list of points on the Grassmann manifold.\n",
    "#     \"\"\"\n",
    "#     distances = jnp.array([grassmann_dist_jax(y.T, point.T) ** 2 for point in points])\n",
    "#     return jnp.sum(distances) / 2\n",
    "\n",
    "\n",
    "# # Function to compute the Karcher mean\n",
    "# def compute_karcher_mean(points, init_guess):\n",
    "#     \"\"\"\n",
    "#     Compute the Karcher mean of points on the Grassmann manifold.\n",
    "\n",
    "#     Args:\n",
    "#         points: List of points on the Grassmann manifold (list of matrices).\n",
    "#         init_guess: Initial guess for the Karcher mean (matrix).\n",
    "#         max_iterations: Maximum number of iterations for optimization.\n",
    "\n",
    "#     Returns:\n",
    "#         The Karcher mean (matrix).\n",
    "#     \"\"\"\n",
    "#     def cost_function(y):\n",
    "#         return karcher_mean_objective(y, points)\n",
    "\n",
    "#     # Use ScipyMinimize with Nelder-Mead\n",
    "#     optimizer = ScipyMinimize(\n",
    "#         method=\"CG\",\n",
    "#         fun=cost_function\n",
    "#     )\n",
    "\n",
    "#     # Run optimization\n",
    "#     result = optimizer.run(init_guess)\n",
    "\n",
    "#     # Reshape the result back to the original matrix shape\n",
    "#     karcher_mean = result.params.reshape(init_guess.shape)\n",
    "#     return karcher_mean\n",
    "\n",
    "\n",
    "# def within_class_similarity(subject_matrices, centroid):\n",
    "#     \"\"\"\n",
    "#     Compute the average pairwise Grassmann distance for within-class similarity.\n",
    "\n",
    "#     Parameters:\n",
    "#         subject_matrices: 3D Array (subjectsx components x features) for each subject in a class.\n",
    "\n",
    "#     Returns:\n",
    "#         Average Grassmann distance (lower is better for similarity).\n",
    "#     \"\"\"\n",
    "#     distances = []\n",
    "#     n_subjects = len(subject_matrices)\n",
    "#     manifold_with = Grassmann(n=379, p=2)\n",
    "\n",
    "#     for i in range(n_subjects):\n",
    "#         dist = grassmann_dist_jax(subject_matrices[i].T, centroid)\n",
    "#         dist_2 = manifold_with.dist(subject_matrices[i], centroid.T)\n",
    "#         print(dist,dist_2)\n",
    "#         distances.append(dist)\n",
    "#     avg_distance = jnp.mean(jnp.array(distances))\n",
    "    return avg_distance\n",
    "\n",
    "\n",
    "def spatial_cost(F, X, labels, mode=\"mean_difference\"):\n",
    "    \"\"\"\n",
    "    Compute the spatial term of the cost function.\n",
    "\n",
    "    Parameters:\n",
    "        F: Projection matrix (components x features).\n",
    "        X: Input data matrix (subjects x components x features).\n",
    "        labels: Group labels for classifier-based optimization.\n",
    "        mode: 'mean_difference' or 'classifier'.\n",
    "\n",
    "    Returns:\n",
    "        Spatial cost value.\n",
    "    \"\"\"\n",
    "    # Dual-projected data\n",
    "    projected_data = dual_projection(X, F)\n",
    "    if mode == \"mean_difference\":\n",
    "        # Normalize each vector (row) within each subject\n",
    "        vector_norms = jnp.linalg.norm(projected_data, axis=2, keepdims=True)  # Shape: (subjects x components x 1)\n",
    "        normalized_data = projected_data / vector_norms  # Normalize each feature vector (subjects x components x features)\n",
    "    \n",
    "\n",
    "        # Compute mean vector for each group\n",
    "        group1_mean = jnp.mean(normalized_data[labels == 1], axis=0)  # Shape: (components x features)\n",
    "        group1_mean /= jnp.linalg.norm(group1_mean,axis=1,keepdims=True)\n",
    "        # within_group_1 = jnp.mean(jnp.array([(jnp.mean(jnp.diag(1 - sub@group1_mean.T))) for sub in normalized_data[labels==1]]))\n",
    "\n",
    "        group2_mean = jnp.mean(normalized_data[labels == 0], axis=0)  # Shape: (x components x features)\n",
    "        group2_mean /= jnp.linalg.norm(group2_mean,axis=1,keepdims=True)\n",
    "        # within_group_2 = jnp.mean(jnp.array([(jnp.mean(jnp.diag(1 - sub@group2_mean.T))) for sub in normalized_data[labels==0]]))\n",
    "\n",
    "        # within = (within_group_1 + within_group_2)/2\n",
    "        # distance = jnp.linalg.norm(group1_mean - group2_mean,axis=1)**2\n",
    "        between = (jnp.mean(jnp.diag(1 - group1_mean@group2_mean.T)))\n",
    "        # debug.print(\"Within {within}\", within=within)\n",
    "        # debug.print(\"Between {between}\", between=between)\n",
    "\n",
    "        return (between)/2\n",
    "    elif mode == \"subspace\":\n",
    "        # Define the Grassmann manifold based on the projected data\n",
    "        # n_features = F.shape[0]\n",
    "        # n_components = F.shape[1]\n",
    "        # manifold_mean = Grassmann(n=n_features, p=n_components)\n",
    "\n",
    "       # Compute Grassmann centroids for each class\n",
    "        # points_1 = [projected_data[i, :, :].T for i in range(len(labels)) if labels[i] == 1]\n",
    "        # mean_class_1 = jnp.array(compute_centroid(manifold_mean, points_1))\n",
    "        # karcher_mean = compute_karcher_mean(points_1, mean_class_1)\n",
    "        # print(karcher_mean.shape)\n",
    "        # points_0 = [projected_data[i, :, :].T for i in range(len(labels)) if labels[i] == 0]\n",
    "        # mean_class_0 = compute_centroid(manifold_mean, points_0)\n",
    "        # mean_class_1 = jnp.mean(projected_data[labels == 1], axis=0).T\n",
    "        # mean_class_0 = jnp.mean(projected_data[labels == 0], axis=0).T\n",
    "        # Normalize each vector (row) within each subject\n",
    "        vector_norms = jnp.linalg.norm(projected_data, axis=2, keepdims=True)  # Shape: (subjects x components x 1)\n",
    "        normalized_data = projected_data / vector_norms  # Normalize each feature vector (subjects x components x features)\n",
    "    \n",
    "        group1_mean = jnp.mean(normalized_data[labels == 1], axis=0)  # Shape: (components x features)\n",
    "        group1_mean /= jnp.linalg.norm(group1_mean,axis=1,keepdims=True)\n",
    "\n",
    "        group0_mean = jnp.mean(normalized_data[labels == 0], axis=0)  # Shape: (x components x features)\n",
    "        group0_mean /= jnp.linalg.norm(group0_mean,axis=1,keepdims=True)\n",
    "        return grassmann_dist_jax(group1_mean, group0_mean)\n",
    "        print(\"between mine\", grassmann_dist_jax(group1_mean.T, group0_mean.T))\n",
    "        # dist  = []\n",
    "        # for point in points_0:\n",
    "        #     dist.append(manifold_mean.dist(point, mean_class_1))\n",
    "        # plt.hist(dist,color=\"red\",alpha=0.6)\n",
    "        # dist2  = []\n",
    "        # for point in points_0:\n",
    "        #     dist2.append(manifold_mean.dist(point, mean_class_0))\n",
    "        # plt.hist(dist2,color=\"blue\",alpha=0.6)\n",
    "        # plt.show()\n",
    "        # Compute within-class similarity\n",
    "        # within_class = (\n",
    "        #     within_class_similarity(points_1, mean_class_1.T) +\n",
    "        #     within_class_similarity(points_0, mean_class_0.T)\n",
    "        # ) / 2\n",
    "        # print(\"within\", within_class)\n",
    "\n",
    "        # Compute between-class difference\n",
    "        # between_class = grassmann_dist_jax(mean_class_1, mean_class_0)\n",
    "        # between_class = manifold_mean.dist(mean_class_1,mean_class_0)\n",
    "        # print(\"between\", between_class)\n",
    "        # print(\"between mine\", grassmann_dist_jax(mean_class_1.T, mean_class_0.T))\n",
    "\n",
    "        # return between_class - within_class\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'mean_difference' or 'classifier'.\")\n",
    "\n",
    "\n",
    "def optimize_combined(W, X, labels, alpha=1.0, beta=1.0, c=1.0, conn_mode=\"var_explained\", spatial_mode=\"mean_difference\", optimizer=\"ConjugateGradient\", n_components=5,initial_point=None,variance_target=None):\n",
    "    \"\"\"\n",
    "    Optimize the combined cost function on the Stiefel manifold.\n",
    "\n",
    "    Parameters:\n",
    "        C, W: Covariance matrices for connectivity differences.\n",
    "        X: Input data matrix (subjects x features).\n",
    "        labels: Group labels for classifier-based optimization.\n",
    "        alpha, beta: Weights for connectivity and spatial terms.\n",
    "        conn_mode: 'trace_ratio' or 'log_trace_difference'.\n",
    "        spatial_mode: 'mean_difference' or 'classifier'.\n",
    "        n_components: Number of components to optimize.\n",
    "\n",
    "    Returns:\n",
    "        Optimized projection matrix F.\n",
    "    \"\"\"\n",
    "    n_features = W.shape[0]\n",
    "    manifold = Grassmann(n_features, n_components)\n",
    "    # manifold = Stiefel(n_features, n_components)\n",
    "    # manifold = Sphere(n_features, n_components)\n",
    "\n",
    "    @pymanopt.function.jax(manifold)\n",
    "    def combined_cost(F):\n",
    "        \"\"\"\n",
    "        Combined cost function for optimizing connectivity, spatial differences, and variance control.\n",
    "\n",
    "        Parameters:\n",
    "            F: Projection matrix.\n",
    "            W: Covariance matrices for connectivity differences.\n",
    "            X: Input data matrix (subjects x components x features).\n",
    "            labels: Group labels for classifier-based optimization.\n",
    "            variance_target: Desired variance reconstruction target.\n",
    "            alpha, beta, gamma: Weights for connectivity, spatial, and variance terms.\n",
    "\n",
    "        Returns:\n",
    "            Total cost value.\n",
    "        \"\"\"\n",
    "        # Compute connectivity cost\n",
    "        conn_term = connectivity_cost(F, W, conn_mode)\n",
    "        debug.print(\"Connectivity cost: {conn_term}\", conn_term=conn_term)\n",
    "        delta = 0.01\n",
    "        # conn_term_adj = -jnp.log(-(variance_target - delta - conn_term)/variance_target + 1)\n",
    "        conn_term_adj = 20*(.3-conn_term)\n",
    "        debug.print(\"Connectivity cost diff: {conn_term_diff}\", conn_term_diff=variance_target - conn_term)\n",
    "        debug.print(\"Connectivity cost (log): {conn_term_adj}\", conn_term_adj=conn_term_adj)\n",
    "\n",
    "        spatial_term = spatial_cost(F, X, labels, spatial_mode)\n",
    "        # spatial_term_adj = jnp.log(1 - spatial_term)\n",
    "        spatial_term_adj = -(spatial_term)\n",
    "        debug.print(\"Spatial cost: {spatial_term}\", spatial_term=spatial_term)\n",
    "        debug.print(\"Spatial cost (log): {spatial_term_adj}\", spatial_term_adj=spatial_term_adj)\n",
    "        \n",
    "        debug.print(\"Connecivity Adj: {conadj}\", conadj=alpha * conn_term_adj)\n",
    "        debug.print(\"Spatial Adj: {spadj}\", spadj=beta * spatial_term_adj)\n",
    "\n",
    "        total_cost = alpha * conn_term_adj + beta * spatial_term_adj\n",
    "        debug.print(\"Total: {total}\", total=total_cost)\n",
    "\n",
    "        return total_cost\n",
    "\n",
    "    # Set up the problem with the decorated cost function\n",
    "    problem = Problem(manifold=manifold, cost=combined_cost)\n",
    "\n",
    "    # Select optimizer\n",
    "    if optimizer == \"ConjugateGradient\":\n",
    "        opt = ConjugateGradient(max_iterations=300, min_gradient_norm=1e-05,  min_step_size=1e-10, max_time=3600)\n",
    "    elif optimizer == \"TrustRegions\":\n",
    "        opt = TrustRegions()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "\n",
    "    # Run optimization\n",
    "    if initial_point is not None:\n",
    "        print(\"Has starting point\")\n",
    "        result = opt.run(problem,initial_point=initial_point)\n",
    "    else:\n",
    "        result = opt.run(problem)\n",
    "    return result.point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "# Load variables\n",
    "labels = np.load(os.path.join(outputfolder,\"labels.npy\"))\n",
    "data = np.load(os.path.join(outputfolder,\"data.npy\"))\n",
    "covs = np.load(os.path.join(outputfolder,\"covs.npy\"))\n",
    "with open(os.path.join(outputfolder, \"paths.pkl\"), \"rb\") as f:\n",
    "    paths = pickle.load(f)    \n",
    "con_confounders = np.load(os.path.join(outputfolder,\"con_confounders.npy\"))\n",
    "with open(os.path.join(outputfolder, \"cat_confounders.pkl\"), \"rb\") as f:\n",
    "    cat_confounders = pickle.load(f)    \n",
    "with open(os.path.join(outputfolder, \"family_ID.pkl\"), \"rb\") as f: #TODO uncomment\n",
    "    family_ID = pickle.load(f)\n",
    "    \n",
    "# Load settings\n",
    "# settings_filepath = os.path.join(outputfolder, \"settings.json\") #TODO uncomment\n",
    "settings_filepath = os.path.join(outputfolder, f\"settings_{nPCA}.json\") #TODO Delete\n",
    "# Load the settings from the JSON file\n",
    "with open(settings_filepath, \"r\") as f:\n",
    "    settings = json.load(f)\n",
    "# Access each setting from the dictionary\n",
    "random_state = settings[\"random_state\"]\n",
    "n_filters_per_group = settings[\"n_filters_per_group\"]\n",
    "# nPCA = settings[\"nPCA\"] #TODO uncomment\n",
    "Tangent_Class = settings[\"Tangent_Class\"]\n",
    "Tan_Class_Str = settings[\"Tan_Class_Str\"]\n",
    "metric = settings[\"metric\"]\n",
    "a_label = int(settings[\"a_label\"])\n",
    "b_label = int(settings[\"b_label\"])\n",
    "self_whiten = settings[\"self_whiten\"]\n",
    "deconfound = settings[\"deconfound\"]\n",
    "\n",
    "\n",
    "# Load Fold Specific Vairables\n",
    "fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "summary_file_path = os.path.join(fold_output_dir, \"output_summary.txt\")\n",
    "indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "\n",
    "# Prepare data for train and test sets\n",
    "train_labels = labels[train_idx]\n",
    "train_data = data[train_idx]\n",
    "train_covs = covs[train_idx]\n",
    "train_paths = paths[train_idx]\n",
    "train_con_confounders = con_confounders[train_idx]\n",
    "train_cat_confounders = cat_confounders[train_idx]\n",
    "\n",
    "test_labels = labels[test_idx]\n",
    "test_data = data[test_idx]\n",
    "test_covs = covs[test_idx]\n",
    "test_con_confounders = con_confounders[test_idx]\n",
    "test_cat_confounders = cat_confounders[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riem_eig, initial_filters, boundary_matrix, Frechet_Mean = TSSF(covs[train_idx], labels[train_idx], clf_str=\"SVM (C=0.01)\", metric=metric, deconf=False, con_confounder_train=None, cat_confounder_train=None, z_score=0, haufe=False, visualize=True,output_dir=outputfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((np.sum(riem_eig[[0,-1]])/np.sum(riem_eig)))\n",
    "random_filters = np.random.randn(initial_filters.shape[0], 2)  # Generate two random filters\n",
    "extended_filters = np.hstack([initial_filters[:, [0, -1]], random_filters])  # Combine with initial basis\n",
    "orthonormal_filters, _ = np.linalg.qr(extended_filters)  # Orthonormalize using QR decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = optimize_combined(boundary_matrix, train_data, train_labels, alpha=1.0, beta=1.0, c=1.0, conn_mode=\"var_explained\", spatial_mode=\"subspace\", optimizer=\"ConjugateGradient\", n_components=2,initial_point=initial_filters[:, [0, -1]],variance_target=((np.sum(riem_eig[[0,-1]])/np.sum(riem_eig))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nilearn.plotting import view_surf\n",
    "\n",
    "# Extract the surface map and threshold it\n",
    "unparcellated_map = hcp.cortex_data(hcp.unparcellate(initial_filters[:, 1], hcp.mmp))\n",
    "\n",
    "# Apply a threshold (e.g., retain only values greater than a specific threshold)\n",
    "threshold = np.percentile(unparcellated_map, 90)  # Retain top 5% values\n",
    "thresholded_map = np.where(unparcellated_map >= threshold, unparcellated_map, 0)\n",
    "\n",
    "# Visualize the thresholded map\n",
    "view_combined = view_surf(\n",
    "    surf_mesh=hcp.mesh.inflated,\n",
    "    surf_map=thresholded_map,\n",
    "    black_bg=False,\n",
    "    colorbar=True,\n",
    ")\n",
    "\n",
    "# Display the visualization\n",
    "display(view_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from classification import linear_classifier\n",
    "\n",
    "# Function to generate a random 2x2 rotation matrix\n",
    "def generate_rotation_matrix():\n",
    "    theta = np.random.uniform(0, 2 * np.pi)  # Random angle in radians\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                  [np.sin(theta), np.cos(theta)]])\n",
    "    return R\n",
    "\n",
    "# Function to generate random orthogonal filters using QR decomposition\n",
    "def generate_random_orthogonal_filters():\n",
    "    manifold = Grassmann(379, 2)\n",
    "    return manifold.random_point()\n",
    "\n",
    "# Function to verify orthogonality\n",
    "def is_orthogonal(matrix, tolerance=1e-6):\n",
    "    identity = np.eye(matrix.shape[1])\n",
    "    orthogonal_check = np.allclose(matrix.T @ matrix, identity, atol=tolerance)\n",
    "    return orthogonal_check\n",
    "\n",
    "# List to store accuracies for rotated and random orthogonal filters\n",
    "accuracies_rotated = []\n",
    "accuracies_random = []\n",
    "\n",
    "# Apply rotations and orthogonal filters multiple times\n",
    "num_iterations = 20\n",
    "filter_dim = filters.shape[1]  # Assuming filters are available\n",
    "for _ in range(num_iterations):\n",
    "    # Generate a random rotation matrix\n",
    "    R = generate_rotation_matrix()\n",
    "    \n",
    "    # Verify orthogonality of random filters\n",
    "    if not is_orthogonal(R):\n",
    "        raise ValueError(\"Generated random filters are not orthogonal\")\n",
    "\n",
    "    # Generate random orthogonal filters\n",
    "    random_filters = generate_random_orthogonal_filters()\n",
    "    # Rotate filters using the rotation matrix\n",
    "    rotated_filters = initial_filters[:, [0, -1]] @ R\n",
    "\n",
    "    # Verify orthogonality of random filters\n",
    "    if not is_orthogonal(rotated_filters):\n",
    "        raise ValueError(\"Generated random filters are not orthogonal\")\n",
    "\n",
    "    # Perform dual projection for rotated filters\n",
    "    proj_train_rotated = dual_projection(train_data, rotated_filters)\n",
    "    proj_test_rotated = dual_projection(test_data, rotated_filters)\n",
    "\n",
    "    # Flatten the projections for classification\n",
    "    proj_train_rotated_flat = proj_train_rotated.reshape(proj_train_rotated.shape[0], -1)\n",
    "    proj_test_rotated_flat = proj_test_rotated.reshape(proj_test_rotated.shape[0], -1)\n",
    "\n",
    "    # Train and test the classifier on rotated filters\n",
    "    results_rotated = linear_classifier(proj_train_rotated_flat, labels[train_idx],\n",
    "                                        proj_test_rotated_flat, labels[test_idx], \n",
    "                                        clf_str='SVM (C=0.01)', z_score=2)\n",
    "    accuracies_rotated.append(results_rotated['SVM (C=0.01)']['accuracy'])\n",
    "\n",
    "    # Perform dual projection for random orthogonal filters\n",
    "    proj_train_random = dual_projection(train_data, random_filters)\n",
    "    proj_test_random = dual_projection(test_data, random_filters)\n",
    "\n",
    "    # Flatten the projections for classification\n",
    "    proj_train_random_flat = proj_train_random.reshape(proj_train_random.shape[0], -1)\n",
    "    proj_test_random_flat = proj_test_random.reshape(proj_test_random.shape[0], -1)\n",
    "\n",
    "    # Train and test the classifier on random orthogonal filters\n",
    "    results_random = linear_classifier(proj_train_random_flat, labels[train_idx],\n",
    "                                       proj_test_random_flat, labels[test_idx],\n",
    "                                       clf_str='SVM (C=0.01)', z_score=2)\n",
    "    accuracies_random.append(results_random['SVM (C=0.01)']['accuracy'])\n",
    "\n",
    "# Plot histograms of accuracies\n",
    "plt.hist(accuracies_rotated, bins=20, alpha=0.75, label=\"Rotated Filters\", color='blue', edgecolor='black')\n",
    "plt.hist(accuracies_random, bins=20, alpha=0.75, label=\"Random Orthogonal Filters\", color='orange', edgecolor='black')\n",
    "plt.title(\"Histogram of Classification Accuracies: Rotated vs Random Orthogonal Filters\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Mean Accuracy (Rotated): {np.mean(accuracies_rotated):.4f}\")\n",
    "print(f\"Standard Deviation (Rotated): {np.std(accuracies_rotated):.4f}\")\n",
    "print(f\"Mean Accuracy (Random Orthogonal): {np.mean(accuracies_random):.4f}\")\n",
    "print(f\"Standard Deviation (Random Orthogonal): {np.std(accuracies_random):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_train = dual_projection(train_data,filters[:,[0,-1]])\n",
    "proj_test = dual_projection(test_data, filters[:,[0,-1]])\n",
    "\n",
    "from classification import linear_classifier\n",
    "results_IFA = linear_classifier(proj_train.reshape(proj_train.shape[0],-1), labels[train_idx], proj_test.reshape(proj_test.shape[0],-1), labels[test_idx], clf_str='SVM (C=0.01)', z_score=2)\n",
    "print(results_IFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Perform t-tests on spatial maps\n",
    "t_values, p_values = ttest_ind(proj_test[test_labels == 1],proj_test[test_labels==0],axis=0,permutations=False)\n",
    "\n",
    "# Multiple comparison correction\n",
    "reject, corrected_p_values, _, _ = multipletests(p_values.flatten(), alpha=0.05, method=\"fdr_bh\")\n",
    "reject = reject.reshape(p_values.shape)\n",
    "corrected_p_values = corrected_p_values.reshape(p_values.shape)\n",
    "t_values = t_values.reshape(p_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.any(reject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the surface map and threshold it\n",
    "unparcellated_map = hcp.cortex_data(hcp.unparcellate(reject[1,:], hcp.mmp))\n",
    "\n",
    "# Apply a threshold (e.g., retain only values greater than a specific threshold)\n",
    "threshold = np.percentile(unparcellated_map, 0)  # Retain top 5% values\n",
    "thresholded_map = np.where(unparcellated_map >= threshold, unparcellated_map, 0)\n",
    "\n",
    "# Visualize the thresholded map\n",
    "view_combined = view_surf(\n",
    "    surf_mesh=hcp.mesh.inflated,\n",
    "    surf_map=thresholded_map,\n",
    "    black_bg=False,\n",
    "    colorbar=True,\n",
    "    symmetric_cmap=False,\n",
    ")\n",
    "\n",
    "# Display the visualization\n",
    "display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/project/3022057.01/IFA/utils')\n",
    "\n",
    "from filters import evaluate_filters\n",
    "# Function to generate a random 2x2 rotation matrix\n",
    "def generate_rotation_matrix():\n",
    "    theta = np.random.uniform(0, 2 * np.pi)  # Random angle in radians\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                  [np.sin(theta), np.cos(theta)]])\n",
    "    return R\n",
    "metrics_dict_logvar, metrics_dict_logcov = evaluate_filters(train_data, train_labels, test_data, test_labels, filters[:, [0, -1]], metric=metric, deconf=False, con_confounder_train=None, cat_confounder_train=None, con_confounder_test=None, cat_confounder_test=None,output_dir=outputfolder)\n",
    "print(metrics_dict_logvar)\n",
    "print(metrics_dict_logcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_dict_logvar)\n",
    "print(metrics_dict_logcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pymanopt.manifolds import Stiefel\n",
    "from sklearn.decomposition import FastICA\n",
    "import os\n",
    "import hcp_utils as hcp\n",
    "import json\n",
    "import pickle\n",
    "from nilearn.plotting import view_surf\n",
    "\n",
    "\n",
    "def run_and_plot_ica(mixed_data, true_time_signals, true_spatial_maps, time_points, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Run FastICA on mixed_data and then plot (a) the recovered time series versus the true ones,\n",
    "    and (b) the recovered spatial maps using Nilearn's view_surf.\n",
    "    \"\"\"\n",
    "    n_components = true_time_signals.shape[1]\n",
    "    ica = FastICA(n_components=n_components, random_state=0)\n",
    "    # FastICA expects data as (n_samples, n_features). Here, samples = time.\n",
    "    estimated_maps = ica.fit_transform(mixed_data.T)  # shape: (n_vertices, n_components)\n",
    "    estimated_time = ica.mixing_                        # shape: (n_time_points, n_components)\n",
    "    \n",
    "    # To align the estimated components with the true ones, compute a permutation matrix.\n",
    "    perm_matrix = (estimated_maps.T @ true_spatial_maps) / mixed_data.shape[0]\n",
    "    estimated_maps_aligned = estimated_maps @ perm_matrix\n",
    "    estimated_time_aligned = estimated_time @ perm_matrix\n",
    "\n",
    "    # Plot time series comparison.\n",
    "    for i in range(n_components):\n",
    "        plt.figure()\n",
    "        plt.title(f\"True Time\")\n",
    "        plt.plot(time_points, true_time_signals[:, i], label=\"True\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Display true spatial map.\n",
    "        true_view = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(true_spatial_maps[:, i]),\n",
    "            title=f\"{title_prefix} True Spatial Map Component {i+1}\"\n",
    "        )\n",
    "        display(true_view)        \n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(f\"Estimated Time\")\n",
    "        # plt.plot(np.concatenate((time_points,np.max(time_points)+time_points+1)), estimated_time_aligned[:, i], label=\"Estimated\")\n",
    "        plt.plot(time_points, estimated_time_aligned[:time_points.shape[0], i], label=\"True\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "\n",
    "        # Display estimated spatial map.\n",
    "        est_view = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(estimated_maps_aligned[:, i]),\n",
    "            title=f\"{title_prefix} Estimated Spatial Map Component {i+1}\"\n",
    "        )\n",
    "        display(est_view)\n",
    "    return estimated_time_aligned, estimated_maps_aligned\n",
    "\n",
    "def simulate_joint_group_data_ICA(n_time_points=200, n_shared=3, n_group=2, n_group_specific=1, parcellation=hcp.mmp, signal_eigval=1.1, noise_eigval=1.0, group_eig=0.5, sinusoid_freqs=None,n_noise=10):\n",
    "\n",
    "    # Total number of components: shared + group-specific (for all groups)\n",
    "    n_total_components = n_shared + n_group * n_group_specific\n",
    "\n",
    "    unique_parcels = np.unique(parcellation.map_all)\n",
    "    unique_parcels = unique_parcels[unique_parcels != 0]\n",
    "    n_parcels = len(unique_parcels)\n",
    "    if n_total_components > n_parcels:\n",
    "        raise ValueError(\"Total components cannot exceed number of available parcels.\")\n",
    "\n",
    "    # Create a random orthogonal basis over parcels.\n",
    "    basis_all = Stiefel(n_parcels, n_total_components).random_point()\n",
    "    # Unparcellate to full cortical space.\n",
    "    basis_all_unparc = hcp.unparcellate(basis_all.T, parcellation)\n",
    "    # Threshold to keep only the strongest values.\n",
    "    thresholds = np.percentile(basis_all_unparc, 99.9, axis=1, keepdims=True)\n",
    "    thresholded_data = np.where(basis_all_unparc >= thresholds, basis_all_unparc, 0.0)\n",
    "    \n",
    "    # Run ICA on the thresholded data to get independent spatial maps.\n",
    "    ica_spatial = FastICA(n_components=n_total_components, random_state=0)\n",
    "    maps_all = ica_spatial.fit_transform(thresholded_data.T)  # shape: (n_vertices, n_total_components)\n",
    "    spatial_maps_all = maps_all / np.linalg.norm(maps_all,axis=0)\n",
    "\n",
    "    if sinusoid_freqs is None:\n",
    "        sinusoid_freqs = np.random.uniform(low=0.5, high=3.0, size=n_total_components)\n",
    "    elif len(sinusoid_freqs) != n_total_components:\n",
    "        raise ValueError(\"Length of sinusoid_freqs must equal the total number of components.\")\n",
    "\n",
    "    time_points = np.linspace(0, 2 * np.pi, n_time_points)\n",
    "    # time_signals_all = np.zeros((n_time_points, n_total_components))\n",
    "    # for i in range(n_total_components):\n",
    "    #     time_signals_all[:, i] = np.sin(sinusoid_freqs[i] * time_points)\n",
    "    # time_signals_all = time_signals_all / np.linalg.norm(time_signals_all,axis=0)\n",
    "    time_signals_all = Stiefel(n_time_points, n_total_components).random_point()\n",
    "    u_time, _, _ = np.linalg.svd(time_signals_all, full_matrices=False)\n",
    "    \n",
    "    D_shared = np.diag([signal_eigval] * n_shared)\n",
    "    time_signals_shared = time_signals_all[:, :n_shared]\n",
    "    \n",
    "\n",
    "    # n_noise = n_time_points - (n_shared + 2*n_group_specific)\n",
    "    space_noise = Stiefel(spatial_maps_all.shape[0], n_noise).random_point()\n",
    "    space_noise = space_noise - spatial_maps_all @ (np.linalg.pinv(spatial_maps_all) @ space_noise)\n",
    "    \n",
    "    time_noise = Stiefel(n_time_points, n_noise).random_point()\n",
    "    time_noise = time_noise - u_time @ (np.linalg.pinv(u_time) @ time_noise)\n",
    "    D_noise = np.diag([noise_eigval] * n_noise)\n",
    "    common_noise = time_noise @ D_noise @ space_noise.T\n",
    "    print(np.linalg.matrix_rank(common_noise))\n",
    "\n",
    "    groups_data = {}\n",
    "    for g in range(n_group):\n",
    "        # Slice out group-specific time signals.\n",
    "        start = n_shared + g * n_group_specific\n",
    "        end = start + n_group_specific\n",
    "        # print(start,end,time_signals_all.shape)\n",
    "\n",
    "        # combined_time_signals = np.concatenate([time_signals_shared, time_signals_all[:, start:end]], axis=1)\n",
    "        # u_t_grp, e_t_grp, _ = np.linalg.svd(combined_time_signals.T, full_matrices=False)\n",
    "        # time_signals_grp = (u_t_grp  @ np.linalg.inv(np.diag(e_t_grp)) @ u_t_grp.T @ combined_time_signals.T).T\n",
    "        time_signals_grp = np.concatenate([time_signals_shared, time_signals_all[:, start:end]], axis=1)\n",
    "\n",
    "        time_signals_share_grp = time_signals_shared\n",
    "        time_signals_grp_spc = time_signals_all[:, start:end]\n",
    "        # print(time_signals_grp.shape)\n",
    "        D_group = np.diag([group_eig] * n_group_specific)\n",
    "        combined_spatial_maps = np.concatenate([spatial_maps_all[:, :n_shared],spatial_maps_all[:, start:end]], axis=1)\n",
    "        \n",
    "        shared = time_signals_share_grp @ D_shared @ spatial_maps_all[:, :n_shared].T\n",
    "        group = time_signals_grp_spc @ D_group @ spatial_maps_all[:, start:end].T\n",
    "        mixed_data = shared + group + common_noise \n",
    "        print(np.trace((mixed_data@ spatial_maps_all[:, :n_shared]).T@(mixed_data@ spatial_maps_all[:, :n_shared])))\n",
    "        print(np.trace((mixed_data@ spatial_maps_all[:, start:end]).T@(mixed_data@ spatial_maps_all[:, start:end])))\n",
    "        print(np.trace((mixed_data@ space_noise).T@(mixed_data@ space_noise)))\n",
    "\n",
    "        groups_data[f\"group_{g+1}\"] = {\n",
    "            \"mixed_data\": mixed_data,                   # (n_time_points, n_vertices)\n",
    "            \"time_signals\": time_signals_grp,      # (n_time_points, n_shared + n_group_specific)\n",
    "            \"spatial_maps\": combined_spatial_maps,      # (n_vertices, n_shared + n_group_specific)\n",
    "            \"noise_data\": common_noise,\n",
    "            \"noise_space\": space_noise,\n",
    "            \"time_points\": time_points\n",
    "        }\n",
    "    return groups_data\n",
    "\n",
    "\n",
    "def generate_subject_data(group_signal, n_subjects=10, noise_scale=0.02):\n",
    "\n",
    "    subjects_data = []\n",
    "    n_time_points, n_features = group_signal.shape\n",
    "    \n",
    "    for i in range(n_subjects):\n",
    "        # 1. Generate subject-specific noise and add to group signal.\n",
    "        noise = noise_scale * np.random.randn(n_time_points, n_features)\n",
    "        subject_data =  group_signal + noise\n",
    "        \n",
    "        subjects_data.append(subject_data)\n",
    "    \n",
    "    return subjects_data\n",
    "\n",
    "a_label = 1\n",
    "b_label = 0\n",
    "n_shared = 3\n",
    "groups_data = simulate_joint_group_data_ICA(\n",
    "    n_time_points=30,\n",
    "    n_shared=n_shared,\n",
    "    n_group=2,\n",
    "    n_group_specific=1,\n",
    "    parcellation=hcp.mmp,\n",
    "    signal_eigval = np.sqrt(15.0),\n",
    "    noise_eigval = np.sqrt(10.0),\n",
    "    group_eig = np.sqrt(5.0),\n",
    "    n_noise = 2\n",
    ")\n",
    "\n",
    "# Create a directory for simulated subjects.\n",
    "sim_data_dir = \"simulated_subjects\"\n",
    "os.makedirs(sim_data_dir, exist_ok=True)\n",
    "\n",
    "# Generate simulated subject data for each group.\n",
    "allsubs_1 = generate_subject_data(groups_data[\"group_1\"][\"mixed_data\"], n_subjects=100, noise_scale=0.01)\n",
    "allsubs_2 = generate_subject_data(groups_data[\"group_2\"][\"mixed_data\"], n_subjects=100, noise_scale=0.01)\n",
    "\n",
    "# Save group-level true information.\n",
    "for g in range(1, 3):\n",
    "    group_key = f\"group_{g}\"\n",
    "    group_info = groups_data[group_key]\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_true_time_signals.npy\"), group_info[\"time_signals\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_true_spatial_maps.npy\"), group_info[\"spatial_maps\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_mixed_data.npy\"), group_info[\"mixed_data\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_noise_data.npy\"), group_info[\"noise_data\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_noise_space.npy\"), group_info[\"noise_space\"])\n",
    "\n",
    "    # Visualize the thresholded map\n",
    "    for i in range(group_info[\"spatial_maps\"].shape[-1]):\n",
    "        view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(group_info[\"spatial_maps\"][:,i]),\n",
    "            colorbar=True,\n",
    "        )\n",
    "        display(view_combined)\n",
    "\n",
    "subject_paths = []\n",
    "# Save group 1 subjects with label 0.\n",
    "for i, sub in enumerate(allsubs_1):\n",
    "    file_path = os.path.join(sim_data_dir, f\"group1_sub_{i}.npy\")\n",
    "    np.save(file_path, sub)\n",
    "    subject_paths.append(file_path)\n",
    "# Save group 2 subjects with label 1.\n",
    "for i, sub in enumerate(allsubs_2):\n",
    "    file_path = os.path.join(sim_data_dir, f\"group2_sub_{i}.npy\")\n",
    "    np.save(file_path, sub)\n",
    "    subject_paths.append(file_path)\n",
    "subject_paths = np.array(subject_paths)\n",
    "# -----------------------------\n",
    "# 3. Create Metadata Files for run_fold\n",
    "# -----------------------------\n",
    "n_total = len(subject_paths)\n",
    "# Create binary labels: first half 0 (group1), second half 1 (group2).\n",
    "simulated_labels = np.array([a_label] * len(allsubs_1) + [b_label] * len(allsubs_2))\n",
    "# For Sub_ID, we simply use the file paths.\n",
    "simulated_Sub_ID = np.array(subject_paths)\n",
    "# For family_ID, assign a unique family ID (or use a dummy value if appropriate).\n",
    "simulated_family_ID = np.arange(n_total)\n",
    "\n",
    "\n",
    "# Create an output folder for simulated pipeline metadata.\n",
    "outputfolder = \"simulated_test\"\n",
    "os.makedirs(outputfolder, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(outputfolder, \"labels.npy\"), simulated_labels)\n",
    "np.save(os.path.join(outputfolder, \"Sub_ID.npy\"), simulated_Sub_ID)\n",
    "with open(os.path.join(outputfolder, \"paths.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(subject_paths, f)\n",
    "with open(os.path.join(outputfolder, \"family_ID.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(simulated_family_ID, f)\n",
    "\n",
    "settings = {\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"nPCA_levels\": [n_shared], \n",
    "    \"tangent_class\": True,\n",
    "    \"tan_class_model\": \"SVM (C=0.1)\",  # Replace with your actual model string.\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": a_label,\n",
    "    \"b_label\": b_label,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": False,\n",
    "    \"paired\": False,\n",
    "    \"outputfolder\": outputfolder\n",
    "}\n",
    "with open(os.path.join(outputfolder, \"settings.json\"), \"w\") as f:\n",
    "    json.dump(settings, f, indent=4)\n",
    "\n",
    "print(f\"Simulated subjects and metadata saved in {outputfolder}.\")\n",
    "\n",
    "random_state = settings[\"random_state\"]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html\n",
    "sgkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "splits = list(sgkf.split(subject_paths, simulated_labels))\n",
    "\n",
    "# New job per fold\n",
    "for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    # Create Fold Outputfolder\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "    if not os.path.exists(fold_output_dir):\n",
    "        os.makedirs(fold_output_dir)\n",
    "    indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "    if not os.path.exists(indices_dir):\n",
    "        os.makedirs(indices_dir)\n",
    "    np.save(os.path.join(indices_dir, \"train_idx.npy\"), train_idx)\n",
    "    np.save(os.path.join(indices_dir, \"test_idx.npy\"), test_idx)\n",
    "\n",
    "\n",
    "# u,e,v = np.linalg.svd(allsubs_1,full_matrices=False)\n",
    "# plt.scatter(range(e.shape[0]),e)\n",
    "# plt.show()\n",
    "# allsubs_1 = np.array(allsubs_1).reshape(-1,np.array(allsubs_1).shape[-1])\n",
    "# u,e,v = np.linalg.svd(allsubs_1,full_matrices=False)\n",
    "# plt.scatter(range(30),e[:30])\n",
    "# plt.show()\n",
    "\n",
    "# _ = run_and_plot_ica(\n",
    "#     mixed_data=allsubs_1,\n",
    "#     true_time_signals=groups_data[\"group_1\"][\"time_signals\"],\n",
    "#     true_spatial_maps=groups_data[\"group_1\"][\"spatial_maps\"],\n",
    "#     time_points=groups_data[\"group_1\"][\"time_points\"],\n",
    "#     title_prefix=\"a\"\n",
    "# )\n",
    "\n",
    "# Step 2. Run ICA separately on each group's data.\n",
    "# for group_name, data in groups_data.items():\n",
    "#     print(f\"\\nRunning ICA for {group_name} ...\")\n",
    "#     _ = run_and_plot_ica(\n",
    "#         mixed_data=data[\"mixed_data\"],\n",
    "#         true_time_signals=data[\"time_signals\"],\n",
    "#         true_spatial_maps=data[\"spatial_maps\"],\n",
    "#         time_points=data[\"time_points\"],\n",
    "#         title_prefix=f\"{group_name}\"\n",
    "#     )\n",
    "\n",
    "# mixed_data_joint = groups_data[\"group_1\"][\"mixed_data\"] + groups_data[\"group_2\"][\"mixed_data\"]\n",
    "# mixed_data_joint = np.concatenate([ groups_data[\"group_1\"][\"mixed_data\"], groups_data[\"group_2\"][\"mixed_data\"]], axis=0)\n",
    "# # u, e, vh = np.linalg.svd(mixed_data_joint,full_matrices=False)\n",
    "# # mixed_data_joint = u[:,:200].T@mixed_data_joint\n",
    "# # For the true signals, we form the joint signal by concatenating the shared and group-specific parts.\n",
    "# true_shared = groups_data[\"group_1\"][\"time_signals\"][:, :n_shared]\n",
    "# true_group1_spec = groups_data[\"group_1\"][\"time_signals\"][:, n_shared:]\n",
    "# true_group2_spec = groups_data[\"group_2\"][\"time_signals\"][:, n_shared:]\n",
    "# true_time_signals_joint = np.concatenate([true_shared, true_group1_spec, true_group2_spec], axis=1)\n",
    "\n",
    "# # For spatial maps, similarly concatenate the shared and group-specific portions.\n",
    "# true_spatial_shared = groups_data[\"group_1\"][\"spatial_maps\"][:, :n_shared]\n",
    "# true_spatial_group1 = groups_data[\"group_1\"][\"spatial_maps\"][:, n_shared:]\n",
    "# true_spatial_group2 = groups_data[\"group_2\"][\"spatial_maps\"][:, n_shared:]\n",
    "# true_spatial_maps_joint = np.concatenate([true_spatial_shared, true_spatial_group1, true_spatial_group2], axis=1)\n",
    "\n",
    "# print(np.trace((mixed_data_joint@ true_spatial_shared).T@(mixed_data_joint@ true_spatial_shared)))\n",
    "# print(np.trace((mixed_data_joint@ true_spatial_group1).T@(mixed_data_joint@ true_spatial_group1)))\n",
    "# print(np.trace((mixed_data_joint@ true_spatial_group2).T@(mixed_data_joint@ true_spatial_group2)))\n",
    "# print((np.trace((mixed_data_joint@  groups_data[\"group_1\"][\"noise_space\"]).T@(mixed_data_joint@  groups_data[\"group_1\"][\"noise_space\"]))))\n",
    "\n",
    "# print(\"\\nRunning ICA for the joint (summed) dataset ...\")\n",
    "# _ = run_and_plot_ica(\n",
    "#     mixed_data=mixed_data_joint,\n",
    "#     true_time_signals=true_time_signals_joint,\n",
    "#     true_spatial_maps=true_spatial_maps_joint,\n",
    "#     time_points=groups_data[\"group_1\"][\"time_points\"],\n",
    "#     title_prefix=\"Joint\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "import hcp_utils as hcp\n",
    "from pyriemann.estimation import Covariances\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor, TimeoutError\n",
    "import functools\n",
    "\n",
    "# Add the path to custom modules\n",
    "sys.path.append('/project/3022057.01/IFA/utils')\n",
    "\n",
    "# Import necessary modules\n",
    "from analysis import evaluate, compare\n",
    "from PCA import PPCA, migp\n",
    "from filters import whiten, orthonormalize_filters, save_brain\n",
    "from ICA import ICA, threshold_and_visualize\n",
    "from DualRegression import DualRegress\n",
    "from filters import TSSF, FKT, evaluate_filters\n",
    "from tangent import tangent_classification\n",
    "from haufe import partial_filter_dual_regression\n",
    "from preprocessing import load_subject\n",
    "\n",
    "def save_text_results(text, filepath):\n",
    "    \"\"\"Save text results to a file.\"\"\"\n",
    "    with open(filepath, \"a\") as f:  # Using 'a' to append results to the file\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "def check_job_completion(job_id):\n",
    "    \"\"\"Poll the status of a job and wait until it reaches a final state.\"\"\"\n",
    "    while True:\n",
    "        job_status = subprocess.run(\n",
    "            [\"sacct\", \"-j\", job_id, \"--format=State\", \"--noheader\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        # Split lines and take the first non-empty line as the status\n",
    "        state = job_status.stdout.splitlines()[0].strip()\n",
    "        \n",
    "        if \"COMPLETED\" in state:\n",
    "            return True\n",
    "        elif any(status in state for status in [\"FAILED\", \"CANCELLED\", \"TIMEOUT\"]):\n",
    "            return False\n",
    "        \n",
    "        # Sleep for a bit before checking again\n",
    "        time.sleep(120)  # Poll every 120 seconds\n",
    "\n",
    "def partiallate_subject(sub, vt):\n",
    "    try:\n",
    "        # Load subject data using our load_subject function.\n",
    "        data = load_subject(sub)\n",
    "        # Remove the projection onto vt.\n",
    "        partialled_subject = data - (data @ np.linalg.pinv(vt)) @ vt\n",
    "        del data  # free memory\n",
    "        # Parcellate the residual data.\n",
    "        Xp = hcp.parcellate(partialled_subject, hcp.mmp)\n",
    "        del partialled_subject\n",
    "\n",
    "        # If the subject is simulated (i.e. file path ends with '.npy'), return Xp without extra normalization.\n",
    "        if isinstance(sub, str) and sub.endswith('.npy'):\n",
    "            return Xp\n",
    "        else:\n",
    "            # Otherwise, apply the final demeaning and normalization.\n",
    "            return hcp.normalize(Xp - Xp.mean(axis=1, keepdims=True))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {sub}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def partiallate_subjects(paths, vt, output_dir, n_workers=20):\n",
    "    try:\n",
    "        # Create a function that always uses the same vt.\n",
    "        func = functools.partial(partiallate_subject, vt=vt)\n",
    "        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "            # Map over the list of subject paths. Results will be in the same order.\n",
    "            partiallated_data_list = list(executor.map(func, paths))\n",
    "        \n",
    "        # Convert the list to a NumPy array.\n",
    "        partial_data = np.array(partiallated_data_list)\n",
    "        data_save_path = os.path.join(output_dir, \"partiallated_data.npy\")\n",
    "        np.save(data_save_path, partial_data)\n",
    "        print(f\"Partiallated data saved to {data_save_path}\")\n",
    "        \n",
    "        # Compute covariances.\n",
    "        cov_est = Covariances(estimator='oas')\n",
    "        partial_covs = cov_est.transform(partial_data.transpose(0, 2, 1))\n",
    "        covs_save_path = os.path.join(output_dir, \"partiallated_covs.npy\")\n",
    "        np.save(covs_save_path, partial_covs)\n",
    "        print(f\"Covariances saved to {covs_save_path}\")\n",
    "        \n",
    "        return partial_data, partial_covs\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parcellation process: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise  # Re-raise the error so the process crashes.\n",
    "\n",
    "def major_recon_discrim(discrim_basis, major_space,output_folder):\n",
    "    try:\n",
    "        # Compute reconstruction\n",
    "        reconstructed = discrim_basis.T @ np.linalg.pinv(major_space) @ major_space\n",
    "        numerator = np.linalg.norm(discrim_basis.T - reconstructed, 'fro') ** 2\n",
    "        denominator = np.linalg.norm(discrim_basis.T, 'fro') ** 2\n",
    "        reconstruction_percentage = (1 - numerator / denominator)\n",
    "        print(\"Reconstruction Percentage:\", reconstruction_percentage)\n",
    "        \n",
    "        # Save the reconstruction percentage\n",
    "        recon_file = os.path.join(output_folder, \"discriminant_reconstruction_percentage_vt.txt\")\n",
    "        with open(recon_file, \"w\") as f:\n",
    "            f.write(str(reconstruction_percentage) + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to compute reconstruction percentage:\", e)\n",
    "        reconstruction_percentage = None\n",
    "\n",
    "def PPCA_ICA(reducedsubs,basis=None, n_components=None, IFA=True, self_whiten=False,random_state=42,whiten_method=\"InvCov\", output_folder=None):\n",
    "    if IFA:\n",
    "        if self_whiten:\n",
    "            ## Whiten Basis, will need to whiten because this is a combined basis; method chosen wil rotate it to change ICA unmixing starting position (ICA unmixing is nondeterministic)\n",
    "            basis, _ = whiten(basis, n_components=basis.shape[0], method=whiten_method)\n",
    "        \n",
    "        # Variance Normalize Data (PPCA is only being used for variance normalizing data since we already have the basis)\n",
    "        data_vn, _ = PPCA(reducedsubs.copy(), filters=basis.T, threshold=0.0, niters=1)\n",
    "    else:\n",
    "        # For group ICA need to use PPCA to get the major space to match the dimensionality from IFA\n",
    "        data_vn, basis = PPCA(reducedsubs.copy(), threshold=0.0, niters=1, n=n_components)\n",
    "\n",
    "        if self_whiten:\n",
    "            ## Although basis is orthogonal, this rewhitening accounts for number of samples for whitening\n",
    "            basis, _ = whiten(basis, n_components=basis.shape[0], method=whiten_method)\n",
    "\n",
    "    spatial_maps, A, W = ICA(data_vn, basis, whiten=(not self_whiten), output_dir=output_folder,random_state=random_state)\n",
    "    zmaps, zmaps_thresh = threshold_and_visualize(data_vn, W, spatial_maps.T, visualize=True,output_dir=output_folder)\n",
    "\n",
    "    for i in range(zmaps.shape[1]):\n",
    "        save_brain(zmaps[:,i], f\"z_map_{i}\", output_folder)\n",
    "\n",
    "    np.save(os.path.join(output_folder, \"basis.npy\"), basis)\n",
    "    np.save(os.path.join(output_folder, \"data_vn.npy\"), data_vn)\n",
    "    np.save(os.path.join(output_folder, \"spatial_maps.npy\"), spatial_maps)\n",
    "    np.save(os.path.join(output_folder, \"A.npy\"), A)\n",
    "    np.save(os.path.join(output_folder, \"W.npy\"), W)\n",
    "    np.save(os.path.join(output_folder, \"ICA_zmaps.npy\"), zmaps)\n",
    "    np.save(os.path.join(output_folder, \"ICA_zmaps_thresh.npy\"), zmaps_thresh)\n",
    "\n",
    "    return zmaps\n",
    "\n",
    "def run_comparisons(results_list, base_output_folder, pairs, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Run pairwise comparisons for a list of evaluation results.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_list: list of evaluation results (e.g., normalized or unnormalized).\n",
    "    - base_output_folder: base directory where comparison subfolders will be created.\n",
    "    - pairs: list of tuples (i, j, label_one, label_two) indicating the indices in results_list and their labels.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_output_folder):\n",
    "        os.makedirs(base_output_folder)\n",
    "        \n",
    "    for i, j, label_one, label_two in pairs:\n",
    "        pair_dir = os.path.join(base_output_folder, f\"{label_one}_vs_{label_two}\")\n",
    "        if not os.path.exists(pair_dir):\n",
    "            os.makedirs(pair_dir)\n",
    "        compare(\n",
    "            results_list[i], results_list[j],\n",
    "            label_one=label_one, label_two=label_two,\n",
    "             alpha=alpha, output_dir=pair_dir\n",
    "        )\n",
    "\n",
    "fold = 0 \n",
    "outputfolder = \"simulated_test\"\n",
    "\n",
    "with open(os.path.join(outputfolder, \"settings.json\"), \"r\") as f:\n",
    "    settings = json.load(f)\n",
    "random_state = settings[\"random_state\"]\n",
    "n_filters_per_group = settings[\"n_filters_per_group\"]\n",
    "nPCA_levels = settings[\"nPCA_levels\"]\n",
    "tangent_class = settings[\"tangent_class\"]\n",
    "tan_class_model = settings[\"tan_class_model\"]\n",
    "metric = settings[\"metric\"]\n",
    "a_label = settings[\"a_label\"]\n",
    "b_label = settings[\"b_label\"]\n",
    "self_whiten = settings[\"self_whiten\"]\n",
    "deconfound = settings[\"deconfound\"]\n",
    "paired = settings[\"paired\"]\n",
    "\n",
    "# Load pickle files\n",
    "with open(os.path.join(outputfolder, \"paths.pkl\"), \"rb\") as f:\n",
    "    paths = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(outputfolder, \"family_ID.pkl\"), \"rb\") as f:\n",
    "    family_ID = pickle.load(f)\n",
    "\n",
    "# Load numpy files\n",
    "sub_ID = np.load(os.path.join(outputfolder, \"Sub_ID.npy\"))\n",
    "labels = np.load(os.path.join(outputfolder, \"labels.npy\"))\n",
    "\n",
    "# Load Fold Specific Vairables\n",
    "fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "summary_file_path = os.path.join(fold_output_dir, \"output_summary.txt\")\n",
    "indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "fold_results = os.path.join(fold_output_dir, \"Results\")\n",
    "if not os.path.exists(fold_results):\n",
    "    os.makedirs(fold_results)\n",
    "\n",
    "# Prepare data for train and test sets\n",
    "train_labels = labels[train_idx]\n",
    "train_paths = paths[train_idx]\n",
    "\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "if deconfound:\n",
    "    with open(os.path.join(outputfolder, \"cat_confounders.pkl\"), \"rb\") as f:\n",
    "        cat_confounders = pickle.load(f)\n",
    "    con_confounders = np.load(os.path.join(outputfolder, \"con_confounders.npy\"))\n",
    "    train_con_confounders = con_confounders[train_idx]\n",
    "    train_cat_confounders = cat_confounders[train_idx]\n",
    "    test_con_confounders = con_confounders[test_idx]\n",
    "    test_cat_confounders = cat_confounders[test_idx]\n",
    "else:\n",
    "    train_con_confounders = None\n",
    "    train_cat_confounders = None\n",
    "    test_con_confounders = None\n",
    "    test_cat_confounders = None\n",
    "\n",
    "# Save summary of data split\n",
    "train_groups = set(np.unique(family_ID[train_idx]))\n",
    "test_groups = set(np.unique(family_ID[test_idx]))\n",
    "intersection = train_groups & test_groups\n",
    "save_text_results(f\"Fold {fold + 1}:\", summary_file_path)\n",
    "save_text_results(f\"  Train size: {len(train_idx)}\", summary_file_path)\n",
    "save_text_results(f\"  Test size: {len(test_idx)}\", summary_file_path)\n",
    "save_text_results(f\"  Train labels distribution: {np.bincount(labels[train_idx].astype(int))}\", summary_file_path)\n",
    "save_text_results(f\"  Test labels distribution: {np.bincount(labels[test_idx].astype(int))}\", summary_file_path)\n",
    "save_text_results(f\"  Intersection of groups: {len(intersection)} (Groups: {intersection})\", summary_file_path)\n",
    "if paired:\n",
    "    paired_train = np.array_equal(\n",
    "        sub_ID[train_idx][train_labels == a_label],\n",
    "        sub_ID[train_idx][train_labels == b_label]\n",
    "    )\n",
    "    paired_test = np.array_equal(\n",
    "        sub_ID[test_idx][test_labels == a_label],\n",
    "        sub_ID[test_idx][test_labels == b_label]\n",
    "    )\n",
    "    save_text_results(f\"  Paired Across Train: {paired_train}\", summary_file_path)\n",
    "    save_text_results(f\"  Paired Across Test: {paired_test}\", summary_file_path)\n",
    "\n",
    "# Run MIGP\n",
    "migp_dir = os.path.join(fold_output_dir, \"MIGP\")\n",
    "if not os.path.exists(migp_dir):\n",
    "    os.makedirs(migp_dir)\n",
    "\n",
    "# last element in path list is number of timepoints, see load_subject in preprocessing\n",
    "# m = train_paths[0][-1]\n",
    "m = np.load(train_paths[0]).shape[0]\n",
    "print(\"Keep this many pseudotime points\", m, flush=True)\n",
    "reducedsubsA = migp(train_paths[train_labels == a_label], m=m, n_jobs=15,batch_size=3)\n",
    "reducedsubsB = migp(train_paths[train_labels == b_label], m=m, n_jobs=15,batch_size=3)\n",
    "np.save(os.path.join(migp_dir, \"reducedsubsA.npy\"), reducedsubsA)\n",
    "np.save(os.path.join(migp_dir, \"reducedsubsB.npy\"), reducedsubsB)\n",
    "reducedsubs = np.concatenate((reducedsubsA, reducedsubsB), axis=0)\n",
    "np.save(os.path.join(migp_dir, \"reducedsubs.npy\"), reducedsubs)\n",
    "\n",
    "nPCA = nPCA_levels[0]\n",
    "\n",
    "# Directory for resulst for basis spanned by nPCA components + fixed number of filters\n",
    "nPCA_dir = os.path.join(fold_output_dir, f\"nPCA_{nPCA}\")\n",
    "if not os.path.exists(nPCA_dir):\n",
    "    os.makedirs(nPCA_dir)\n",
    "\n",
    "# Get Parcellated Filters\n",
    "filters_dir = os.path.join(nPCA_dir, \"Filters\")\n",
    "if not os.path.exists(filters_dir):\n",
    "    os.makedirs(filters_dir)\n",
    "\n",
    "_, vt = PPCA(reducedsubs.copy(), threshold=0.0, niters=1, n=nPCA)\n",
    "np.save(os.path.join(filters_dir, f\"vt.npy\"), vt)\n",
    "\n",
    "# # Run the PCA job, now just to get the voxel level filters using GPU\n",
    "# voxel_filters_dir = os.path.join(filters_dir, \"Voxel\")\n",
    "# if not os.path.exists(voxel_filters_dir):\n",
    "#     os.makedirs(voxel_filters_dir)\n",
    "\n",
    "# pca_script = \"/project/3022057.01/IFA/run_IFA/Partial/run_pca_partial.sh\"\n",
    "# pca_command = [\n",
    "#     \"sbatch\",\n",
    "#     \"--output\", os.path.join(fold_output_dir, \"pca-%j.out\"),\n",
    "#     \"--error\", os.path.join(fold_output_dir, \"pca-%j.err\"),\n",
    "#     pca_script,\n",
    "#     outputfolder, fold_output_dir, voxel_filters_dir\n",
    "# ]\n",
    "\n",
    "# pca_process = subprocess.run(pca_command, capture_output=True, text=True)\n",
    "# if pca_process.returncode != 0:\n",
    "#     print(f\"Error submitting PCA job: {pca_process.stderr}\")\n",
    "# job_id = pca_process.stdout.strip().split()[-1]\n",
    "# print(f\"PCA job submitted successfully with job ID: {job_id}\")\n",
    "\n",
    "# Need to partial the data before parcellating; partial then parcellate each subject\n",
    "partial_data, partial_covs = partiallate_subjects(paths, vt, output_dir=filters_dir, n_workers=15)\n",
    "\n",
    "# Then split into train and test using your indices:\n",
    "partial_train_data = partial_data[train_idx]\n",
    "partial_test_data  = partial_data[test_idx]\n",
    "\n",
    "partial_train_covs = partial_covs[train_idx]\n",
    "partial_test_covs  = partial_covs[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run tangent classification for measuring separability in parcellated space\n",
    "# tangent_class_metrics = tangent_classification(partial_train_covs, train_labels, partial_test_covs, test_labels, \n",
    "#                     clf_str='all', z_score=0, metric=metric, deconf=deconfound, \n",
    "#                     con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "#                     con_confounder_test=test_con_confounders, cat_confounder_test=test_cat_confounders)\n",
    "\n",
    "# # Save those tangent classification results to overall fold results directory\n",
    "# with open(os.path.join(filters_dir, \"tangent_class_metrics.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(tangent_class_metrics, f)   \n",
    "# save_text_results(\"Parcellated Tangent Classification \" + str(tangent_class_metrics), summary_file_path)\n",
    "\n",
    "\n",
    "# Directory for all things related to the parecllated filters\n",
    "parcellated_filters_dir = os.path.join(filters_dir, \"Parcellated\")\n",
    "if not os.path.exists(parcellated_filters_dir):\n",
    "    os.makedirs(parcellated_filters_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,e,v = np.linalg.svd(partial_train_covs.reshape(-1,partial_train_covs.shape[-1]),full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(e.shape[0]),e)\n",
    "plt.title(\"Scree plot of group after regressing out major eigenspace\")\n",
    "plt.ylabel(\"Singular Values\")\n",
    "plt.xlabel(\"First 20 Eigenvalues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can do haufe transform on the tangent space if permutation testing; set to true\n",
    "if tangent_class:\n",
    "    eigs, filters_all, W, C = TSSF(partial_train_covs, train_labels, \n",
    "                                clf_str=tan_class_model, metric=metric, deconf=deconfound, \n",
    "                                con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                                z_score=0, haufe=False, visualize=True, output_dir=parcellated_filters_dir)\n",
    "else:\n",
    "    eigs, filters_all = FKT(partial_train_covs, train_labels, \n",
    "                            metric=metric, deconf=deconfound, \n",
    "                            con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                            visualize=True, output_dir=parcellated_filters_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(np.arange(eigs.shape[0]),eigs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haufe import filter_dual_regression\n",
    "print(a_label,b_label)\n",
    "if a_label < b_label and Tangent_Class:\n",
    "    filtersB = filters_all[:, -n_filters_per_group:]\n",
    "    filtersA = filters_all[:, :n_filters_per_group]\n",
    "else: \n",
    "    filtersA = filters_all[:, -n_filters_per_group:]\n",
    "    filtersB = filters_all[:, :n_filters_per_group]\n",
    "    \n",
    "filters_parcellated = np.concatenate((filtersB, filtersA), axis=1)\n",
    "\n",
    "np.save(os.path.join(parcellated_filters_dir, \"filtersA.npy\"), filtersA)\n",
    "np.save(os.path.join(parcellated_filters_dir, \"filtersB.npy\"), filtersB)\n",
    "np.save(os.path.join(parcellated_filters_dir, \"filters_parcellated.npy\"), filters_parcellated)\n",
    "for i in range(filters_parcellated.shape[1]):\n",
    "    save_brain(hcp.unparcellate(filters_parcellated[:,i],hcp.mmp), f\"parcellated_filter_{i}\", parcellated_filters_dir)\n",
    "\n",
    "# Evaluate filters and save those results to overall fold results directory\n",
    "logvar_stats, logcov_stats = evaluate_filters(partial_train_data, train_labels, partial_test_data, test_labels, \n",
    "                                                filters_parcellated, metric=metric, deconf=deconfound, \n",
    "                                                con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                                                con_confounder_test=test_con_confounders, cat_confounder_test=test_cat_confounders,output_dir=parcellated_filters_dir)\n",
    "\n",
    "with open(os.path.join(filters_dir, \"logvar_stats.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(logvar_stats, f)     \n",
    "with open(os.path.join(filters_dir, \"logcov_stats.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(logcov_stats, f)      \n",
    "save_text_results(\"Log Var Filter Feature Classification \" + str(logvar_stats), summary_file_path)\n",
    "save_text_results(\"Log Cov Filter Feature Classification \" + str(logcov_stats), summary_file_path)\n",
    "\n",
    "# While PCA Job Runs, run dual regression on parcellated filters\n",
    "filtersA_transform = partial_filter_dual_regression(filtersA, partial_train_data[train_labels == a_label], train_paths[train_labels == a_label], vt, workers=15)\n",
    "filtersB_transform = partial_filter_dual_regression(filtersB, partial_train_data[train_labels == b_label], train_paths[train_labels == b_label], vt, workers=15)\n",
    "\n",
    "np.save(os.path.join(parcellated_filters_dir, \"A_filters_haufe.npy\"), filtersA_transform)\n",
    "np.save(os.path.join(parcellated_filters_dir, \"B_filters_haufe.npy\"), filtersB_transform)\n",
    "parcelvoxel_filters = orthonormalize_filters(filtersA_transform, filtersB_transform)\n",
    "np.save(os.path.join(parcellated_filters_dir, \"filters.npy\"), parcelvoxel_filters)\n",
    "for i in range(parcelvoxel_filters.shape[1]):\n",
    "    save_brain(parcelvoxel_filters[:,i], f\"parcelvoxel_filters{i}\", parcellated_filters_dir)\n",
    "\n",
    "# # Wait for PCA job completion so can read in voxel level filters\n",
    "# if not check_job_completion(job_id):\n",
    "#     print(f\"PCA job {job_id} did not complete successfully.\")\n",
    "# print(f\"PCA job {job_id} completed successfully.\")\n",
    "\n",
    "    \n",
    "# # after the job completes, load the relevant data\n",
    "# voxel_filters = np.load(os.path.join(voxel_filters_dir, \"filters.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_parcellated = np.load(\"/project/3022057.01/Relation_2Filt_Partial/fold_0/nPCA_8/Filters/Parcellated/filters_parcellated.npy\")\n",
    "train_idx = np.load(\"/project/3022057.01/Relation_2Filt_Partial/fold_0/Indices/train_idx.npy\")\n",
    "partial_train_data = np.load(\"/project/3022057.01/Relation_2Filt_Partial/fold_0/nPCA_8/Filters/partiallated_data.npy\")[train_idx]\n",
    "train_labels = np.load(\"/project/3022057.01/Relation_2Filt_Partial/labels.npy\")[train_idx]\n",
    "a_label = 1\n",
    "b_label = 0\n",
    "\n",
    "data_1_transform = (partial_train_data@filters_parcellated)[train_labels == a_label].reshape(-1,2)\n",
    "data_2_transform = (partial_train_data@filters_parcellated)[train_labels == b_label].reshape(-1,2)\n",
    "# .reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_label,b_label)\n",
    "filters_parcellated = np.load(\"/project/3022057.01/Relation_2Filt_Partial/fold_0/nPCA_8/Filters/Parcellated/filters_parcellated.npy\")\n",
    "train_idx = np.load(\"/project/3022057.01/Relation_2Filt_Partial/fold_0/Indices/train_idx.npy\")\n",
    "partial_train_data = np.load(\"/project/3022057.01/Relation_2Filt_Partial/fold_0/nPCA_8/Filters/partiallated_data.npy\")[train_idx]\n",
    "train_labels = np.load(\"/project/3022057.01/Relation_2Filt_Partial/labels.npy\")[train_idx]\n",
    "a_label = 1\n",
    "b_label = 0\n",
    "partial_train_covs = np.load(\"/project/3022057.01/Relation_2Filt_Partial/fold_0/nPCA_8/Filters/partiallated_covs.npy\")[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can do haufe transform on the tangent space if permutation testing; set to true\n",
    "def FKT(cov_matrices, labels, a_label, b_label, metric=\"riemann\", deconf=True, con_confounder_train=None, cat_confounder_train=None, visualize=True,output_dir=\"plots\"):\n",
    "    # Eigenvalues in ascending order from scipy eigh https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    if deconf:\n",
    "        data, Frechet_Mean = tangent_transform(cov_matrices,metric=metric)\n",
    "        data = deconfound(data, con_confounder_train, cat_confounder_train, X_test=None, con_confounder_test=None, cat_confounder_test=None)\n",
    "        groupA_cov_matrices_deconf = untangent_space(data[labels == a_label],Frechet_Mean,metric=metric)\n",
    "        groupB_cov_matrices_deconf = untangent_space(data[labels == b_label],Frechet_Mean,metric=metric)\n",
    "        groupA_mean_cov= mean_covariance(groupA_cov_matrices_deconf, metric=metric)\n",
    "        groupB_mean_cov = mean_covariance(groupB_cov_matrices_deconf, metric=metric)\n",
    "    else:\n",
    "        groupA_mean_cov= mean_covariance(cov_matrices[labels == a_label], metric=metric)\n",
    "        groupB_mean_cov = mean_covariance(cov_matrices[labels == b_label], metric=metric)\n",
    "\n",
    "    eigsA, filtersA  = eigh(groupA_mean_cov - groupB_mean_cov, groupA_mean_cov + groupB_mean_cov,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "    eigsB, filtersB = eigh(groupB_mean_cov - groupA_mean_cov, groupA_mean_cov + groupB_mean_cov,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "       \n",
    "    eigs = np.concatenate((eigsB[::-1], eigsA))\n",
    "    filters = np.concatenate((filtersB[:, ::-1], filtersA), axis=1)\n",
    "\n",
    "    # Transform Eigenvalues to Approximate Riemannian Distance https://ieeexplore.ieee.org/document/5662067\n",
    "    # Specific for AIR and is support of distance not distance\n",
    "    # As noted in the TSSF function and appendix of tssf paper, eigenvalues manifold representation can be calculated by \n",
    "    # doing the eigh in the tangent space. Classic FKT is linked to AIR so this transformation holds\n",
    "    fkt_riem_eigs = np.abs(np.log(eigs/(1-eigs)))**2\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(range(0,fkt_riem_eigs.shape[0]),fkt_riem_eigs)\n",
    "        plt.title(\"Riemannian Distance Supported by Spatial Filter\")\n",
    "        plt.xlabel(\"Max Eigenvector for Group B to Max Eigenvector for Group A\")\n",
    "        plt.ylabel(r\"$|\\log\\left(\\frac{\\lambda}{1 - \\lambda}\\right)|^2$\")\n",
    "        plt.savefig(os.path.join(output_dir, \"fkt_scree.svg\"))\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "    return fkt_riem_eigs, filters\n",
    "\n",
    "if False:\n",
    "    eigs, filters_all, W, C = TSSF(partial_train_covs, train_labels, \n",
    "                                clf_str=\"Logistic Regression (elasticnet C=1)\", metric=metric, deconf=deconfound, \n",
    "                                con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                                z_score=0, haufe=False, visualize=True, output_dir=parcellated_filters_dir)\n",
    "else:\n",
    "    eigs, filters_all = FKT(partial_train_covs, train_labels, a_label, b_label,\n",
    "                            metric=metric, deconf=deconfound, \n",
    "                            con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                            visualize=True, output_dir=parcellated_filters_dir)\n",
    "    \n",
    "if a_label < b_label and Tangent_Class:\n",
    "    filtersB = filters_all[:, -n_filters_per_group:]\n",
    "    filtersA = filters_all[:, :n_filters_per_group]\n",
    "else: \n",
    "    filtersA = filters_all[:, -n_filters_per_group:]\n",
    "    filtersB = filters_all[:, :n_filters_per_group]\n",
    "    \n",
    "filters_parcellated = np.concatenate((filtersB, filtersA), axis=1)\n",
    "\n",
    "data_1_transform = (partial_train_data@filters_parcellated)[train_labels == a_label].reshape(-1,2)\n",
    "data_2_transform = (partial_train_data@filters_parcellated)[train_labels == b_label].reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Create figure and gridspec layout\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "gs = gridspec.GridSpec(4, 4)\n",
    "\n",
    "# Define axes explicitly\n",
    "ax_scatter = fig.add_subplot(gs[1:4, 0:3])\n",
    "ax_hist_x = fig.add_subplot(gs[0, 0:3], sharex=ax_scatter)\n",
    "ax_hist_y = fig.add_subplot(gs[1:4, 3])\n",
    "\n",
    "# Scatter plot\n",
    "ax_scatter.scatter(data_1_transform[:, 0], data_1_transform[:, 1], \n",
    "                   label=f'Group {a_label}', color='blue', alpha=0.5)\n",
    "ax_scatter.scatter(data_2_transform[:, 0], data_2_transform[:, 1], \n",
    "                   label=f'Group {b_label}', color='red', alpha=0.5)\n",
    "ax_scatter.legend()\n",
    "ax_scatter.set_xlabel('Group 1 Filter')\n",
    "ax_scatter.set_ylabel('Group 2 Filter')\n",
    "ax_scatter.grid(True)\n",
    "\n",
    "# Histograms\n",
    "bins = 30\n",
    "\n",
    "# X-axis histogram (top)\n",
    "ax_hist_x.hist(data_1_transform[:, 0], bins=bins, color='blue', alpha=0.5, density=True)\n",
    "ax_hist_x.hist(data_2_transform[:, 0], bins=bins, color='red', alpha=0.5, density=True)\n",
    "ax_hist_x.grid(True)\n",
    "\n",
    "# Y-axis histogram (right side)\n",
    "ax_hist_y = fig.add_subplot(gs[1:4, 3], sharey=ax_scatter)\n",
    "ax_hist_y.hist(data_1_transform[:, 1], bins=bins, orientation='horizontal', \n",
    "               color='blue', alpha=0.5, density=True)\n",
    "ax_hist_y.hist(data_2_transform[:, 1], bins=bins, orientation='horizontal', color='red', alpha=0.5, density=True)\n",
    "ax_hist_y.grid(True)\n",
    "\n",
    "# Hide tick labels on histogram axes to reduce clutter\n",
    "ax_hist_x.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_hist_y.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "# Example of adding text annotations clearly and precisely:\n",
    "var_x1 = np.var(data_1_transform[:, 0])\n",
    "var_x2 = np.var(data_2_transform[:, 0])\n",
    "var_y1 = np.var(data_1_transform[:, 1])\n",
    "var_y2 = np.var(data_2_transform[:, 1])\n",
    "\n",
    "# Annotate variance clearly in the histogram plots:\n",
    "ax_hist_x.text(0.05, 0.9, f'Var X (Group {a_label}): {var_x1:.5f}\\nVar X (Group {b_label}): {np.var(data_2_transform[:,0]):.5f}', \n",
    "               transform=ax_hist_x.transAxes, fontsize=9, verticalalignment='top')\n",
    "\n",
    "ax_hist_y.text(0.9, 0.9, f'Var Y (Group {a_label}): {np.var(data_1_transform[:,1]):.5f}\\nVar Y (Group {b_label}): {np.var(data_2_transform[:,1]):.5f}',\n",
    "               transform=ax_hist_y.transAxes, fontsize=9, verticalalignment='top', rotation=-90)\n",
    "\n",
    "# Set legend\n",
    "ax_scatter.legend(loc='best')\n",
    "\n",
    "# Final layout adjustment\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Notes:\n",
    "# - You can adjust positioning via transform=ax.transAxes for normalized coordinates.\n",
    "# - Keep annotations concise to maintain readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_combined = view_surf(\n",
    "    surf_mesh=hcp.mesh.inflated,\n",
    "    surf_map=hcp.cortex_data(parcelvoxel_filters[:,1]),\n",
    "    colorbar=True,\n",
    ")\n",
    "display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the overlap between retained major eigenspace and discriminant subspace\n",
    "major_recon_discrim(parcelvoxel_filters, vt, parcellated_filters_dir)\n",
    "# major_recon_discrim(voxel_filters, vt, voxel_filters_dir)\n",
    "\n",
    "#Create Folders to store ICA outputs for this subspace dimension and run ICA for each basis\n",
    "ICA_dir = os.path.join(nPCA_dir, \"ICA\")\n",
    "if not os.path.exists(ICA_dir):\n",
    "    os.makedirs(ICA_dir)\n",
    "\n",
    "parcel_IFA_dir = os.path.join(ICA_dir, \"parcel_IFA\")\n",
    "if not os.path.exists(parcel_IFA_dir):\n",
    "    os.makedirs(parcel_IFA_dir)\n",
    "parcelvoxel_IFA_zmaps = PPCA_ICA(reducedsubs,basis=np.vstack((vt, parcelvoxel_filters.T)), n_components=None, IFA=True, self_whiten=self_whiten,random_state=random_state,whiten_method=\"InvCov\", output_folder=parcel_IFA_dir)\n",
    "\n",
    "# voxel_IFA_dir = os.path.join(ICA_dir, \"voxel_IFA\")\n",
    "# if not os.path.exists(voxel_IFA_dir):\n",
    "#     os.makedirs(voxel_IFA_dir)\n",
    "# voxel_IFA_zmaps = PPCA_ICA(reducedsubs,basis=np.vstack((vt, voxel_filters.T)), n_components=None, IFA=True, self_whiten=self_whiten,random_state=random_state,whiten_method=\"InvCov\", output_folder=voxel_IFA_dir)\n",
    "\n",
    "GICA_dir = os.path.join(ICA_dir, \"GICA\")\n",
    "if not os.path.exists(GICA_dir):\n",
    "    os.makedirs(GICA_dir)\n",
    "ICA_zmaps = PPCA_ICA(reducedsubs,basis=None, n_components=int(nPCA+2*n_filters_per_group), IFA=False, self_whiten=self_whiten,random_state=random_state,whiten_method=\"InvCov\", output_folder=GICA_dir)\n",
    "\n",
    "\n",
    "# spatial_maps = [ICA_zmaps, parcelvoxel_IFA_zmaps, voxel_IFA_zmaps]\n",
    "# outputfolders = [GICA_dir, parcel_IFA_dir, voxel_IFA_dir]\n",
    "spatial_maps = [ICA_zmaps, parcelvoxel_IFA_zmaps]\n",
    "outputfolders = [GICA_dir, parcel_IFA_dir]\n",
    "sample = np.min((5,train_idx.shape[0]))\n",
    "dual_regressor = DualRegress(\n",
    "    subs=paths,\n",
    "    spatial_maps=spatial_maps,\n",
    "    train_index=train_idx,\n",
    "    train_labels=train_labels,\n",
    "    outputfolders=outputfolders,\n",
    "    workers=15,\n",
    "    sample=sample,\n",
    "    method=\"bayesian\",\n",
    "    parallel_points=15,\n",
    "    parallel_subs=15,\n",
    "    n_calls=15,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "dual_regressor.dual_regress()\n",
    "\n",
    "# # Analyze each set of spatial maps\n",
    "# nPCA_results = os.path.join(nPCA_dir, \"Results\")\n",
    "# if not os.path.exists(nPCA_results):\n",
    "#     os.makedirs(nPCA_results)\n",
    "\n",
    "# # map_names = [\"GICA\",\"parcel_IFA\",\"voxel_IFA\"]\n",
    "# map_names = [\"GICA\",\"parcel_IFA\"]\n",
    "\n",
    "# normalized_result = []\n",
    "# unnormalized_result = []\n",
    "\n",
    "# for i, map_i in enumerate(map_names):\n",
    "#     result = dual_regressor.dual_regression_results[i]\n",
    "\n",
    "#     nPCA_results_maps = os.path.join(nPCA_results, map_i)\n",
    "#     if not os.path.exists(nPCA_results_maps):\n",
    "#         os.makedirs(nPCA_results_maps)\n",
    "    \n",
    "#     # For Normalized results\n",
    "#     nPCA_results_maps_norm = os.path.join(nPCA_results_maps, \"Normalized\")\n",
    "#     if not os.path.exists(nPCA_results_maps_norm):\n",
    "#         os.makedirs(nPCA_results_maps_norm)\n",
    "#     ##### BELOW HERE ############\n",
    "#     normalized_result_i = evaluate((result['normalized']['An'], result['normalized']['spatial_map'], result['normalized']['reconstruction_error']), \n",
    "#                         labels, train_idx, test_idx, \n",
    "#                         metric=metric, alpha=0.05, paired=paired, \n",
    "#                         permutations=10000, deconf=deconfound, \n",
    "#                         con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "#                         con_confounder_test=test_con_confounders, cat_confounder_test=test_cat_confounders,\n",
    "#                         output_dir=nPCA_results_maps_norm, random_seed=random_state, basis=f\"{map_i}_Normalized\")           \n",
    "    \n",
    "#     normalized_result.append(normalized_result_i)\n",
    "    \n",
    "#     # For Unnormalized (demeaned) results\n",
    "#     nPCA_results_maps_unnorm = os.path.join(nPCA_results_maps, \"Unnormalized\")\n",
    "#     if not os.path.exists(nPCA_results_maps_unnorm):\n",
    "#         os.makedirs(nPCA_results_maps_unnorm)\n",
    "\n",
    "#     unnormalized_result_i = evaluate((result['demean']['Adm'], result['demean']['spatial_mapdm'], result['demean']['reconstruction_error']), \n",
    "#             labels, train_idx, test_idx, \n",
    "#             metric=metric, alpha=0.05, paired=paired, \n",
    "#             permutations=10000, deconf=deconfound, \n",
    "#             con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "#             con_confounder_test=test_con_confounders, cat_confounder_test=test_cat_confounders,\n",
    "#             output_dir=nPCA_results_maps_unnorm, random_seed=random_state, basis=f\"{map_i}_Unnormalized\")\n",
    "    \n",
    "#     unnormalized_result.append(unnormalized_result_i)\n",
    "\n",
    "# # Define the pairwise comparisons (same for both normalized and unnormalized)\n",
    "# pairs = [\n",
    "#     (0, 1, \"GICA\", \"parcel_IFA\"),\n",
    "#     (0, 2, \"GICA\", \"voxel_IFA\"),\n",
    "#     (1, 2, \"parcel_IFA\", \"voxel_IFA\")\n",
    "# ]\n",
    "\n",
    "# # Run for normalized results\n",
    "# compare_dir_norm = os.path.join(nPCA_results, \"Compare\", \"Normalized\")\n",
    "# run_comparisons(normalized_result, compare_dir_norm, pairs, alpha=0.05)\n",
    "\n",
    "# # Run for unnormalized results\n",
    "# compare_dir_unnorm = os.path.join(nPCA_results, \"Compare\", \"Unnormalized\")\n",
    "# run_comparisons(unnormalized_result, compare_dir_unnorm, pairs, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nilearn.plotting import view_surf\n",
    "\n",
    "train_idx = np.load(\"/project/3022057.01/simulated_test/fold_0/Indices/train_idx.npy\")\n",
    "test_idx = np.load(\"/project/3022057.01/simulated_test/fold_0/Indices/test_idx.npy\")\n",
    "labels = np.load(\"/project/3022057.01/simulated_test/labels.npy\")\n",
    "spatial_maps = np.load(\"/project/3022057.01/simulated_test/fold_0/nPCA_3/ICA/GICA/spatial_map.npy\")\n",
    "test_labels = labels[test_idx]\n",
    "a_maps = spatial_maps[test_idx][test_labels == 1]\n",
    "b_maps = spatial_maps[test_idx][test_labels == 0] \n",
    "a_maps_mean = a_maps.mean(axis=0)\n",
    "b_maps_mean = b_maps.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyriemann.estimation import Covariances\n",
    "\n",
    "i = 1\n",
    "result = dual_regressor.dual_regression_results[i]\n",
    "spatial_maps = result['normalized']['spatial_map']\n",
    "A = result['demean']['Adm']\n",
    "A_train = A[train_idx]\n",
    "A_test = A[test_idx]\n",
    "cov_est = Covariances(estimator='oas')\n",
    "Netmats_train = cov_est.transform(np.transpose(A_train, (0, 2, 1)))\n",
    "Netmats_test = cov_est.transform(np.transpose(A_test, (0, 2, 1)))\n",
    "A_maps = spatial_maps[train_idx][train_labels==a_label]\n",
    "B_maps = spatial_maps[train_idx][train_labels==b_label]\n",
    "a_maps_mean = A_maps.mean(axis=0)\n",
    "b_maps_mean = B_maps.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):    \n",
    "    view_combined = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(a_maps_mean[i] - b_maps_mean[i]),\n",
    "        colorbar=True,\n",
    "    )\n",
    "    display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import tangent_classification, tangent_t_test\n",
    "Class_Result = tangent_classification(Netmats_train, train_labels, Netmats_test, test_labels, \n",
    "                                        clf_str='all', z_score=0, metric=metric, deconf=False, \n",
    "                                        con_confounder_train=None, cat_confounder_train=None, \n",
    "                                        con_confounder_test=None, cat_confounder_test=None)\n",
    "print(Class_Result)\n",
    "\n",
    "t_test = tangent_t_test(Netmats_train, Netmats_test, test_labels, \n",
    "                            alpha=0.05, paired=paired, permutations=1000, metric=metric,deconf=False, \n",
    "                            con_confounder_train=None, cat_confounder_train=None, \n",
    "                            con_confounder_test=None, cat_confounder_test=None,\n",
    "                            output_dir=fold_output_dir, basis=f\"{i}\",random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Extract classifier names and accuracy values\n",
    "classifiers = list(Class_Result.keys())\n",
    "accuracies = [Class_Result[clf]['accuracy'] for clf in classifiers]\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(classifiers, accuracies, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01, f\"{acc:.2f}\", \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label=\"Chance Level (50%)\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title('Classification Accuracies for Different Models')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example: spatial_maps is an array with shape (subjects, maps, vertices)\n",
    "# train_idx: indices of training subjects\n",
    "# train_labels: corresponding labels for each subject (with values a_label or b_label)\n",
    "# a_label, b_label: the two groups to classify\n",
    "\n",
    "n_maps = spatial_maps.shape[1]  # number of spatial maps\n",
    "results = {}\n",
    "\n",
    "# Loop over each spatial map (i.e. each column in the maps dimension)\n",
    "for map_idx in range(n_maps):\n",
    "    # Extract the spatial map data for training subjects:\n",
    "    # This yields an array of shape (n_train_subjects, vertices)\n",
    "    map_data = spatial_maps[train_idx, map_idx, :]\n",
    "\n",
    "    # Create binary labels: here, label 1 for subjects in group a, 0 for group b.\n",
    "    # (Adjust this if your labeling convention is different.)\n",
    "    labels = (train_labels == a_label).astype(int)\n",
    "\n",
    "    # Option 1: Use a simple train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        map_data, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    view_combined = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(clf.coef_[0,:]),\n",
    "        colorbar=True,\n",
    "    )\n",
    "    display(view_combined)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[map_idx] = acc\n",
    "    print(f\"Map {map_idx}: Accuracy = {acc:.2f}\")\n",
    "\n",
    "    # Option 2: Alternatively, perform cross-validation\n",
    "    scores = cross_val_score(clf, map_data, labels, cv=5)\n",
    "    print(f\"Map {map_idx}: Cross-validated Accuracy = {np.mean(scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save group-level true information.\n",
    "for g in range(1, 3):\n",
    "    group_key = f\"group_{g}\"\n",
    "    group_info = groups_data[group_key]\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_true_time_signals.npy\"), group_info[\"time_signals\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_true_spatial_maps.npy\"), group_info[\"spatial_maps\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_mixed_data.npy\"), group_info[\"mixed_data\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_noise_data.npy\"), group_info[\"noise_data\"])\n",
    "    np.save(os.path.join(sim_data_dir, f\"{group_key}_noise_space.npy\"), group_info[\"noise_space\"])\n",
    "\n",
    "    # Visualize the thresholded map\n",
    "    for i in range(group_info[\"spatial_maps\"].shape[-1]):\n",
    "        view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(group_info[\"spatial_maps\"][:,i]),\n",
    "            colorbar=True,\n",
    "        )\n",
    "        display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    view_combined = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(B_maps[i,:]),\n",
    "        colorbar=True,\n",
    "    )\n",
    "    display(view_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"phenotype\": [\"NEOFAC_O\"],\n",
    "    \"percentile\": 0.15,\n",
    "    \"outputfolder\": \"NEOFAC_O\",\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"nPCA_levels\": [8],\n",
    "    \"tangent_class\": True,\n",
    "    \"tan_class_model\": \"SVM (C=0.1)\",\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": 1,\n",
    "    \"b_label\": 0,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": True,\n",
    "    \"paired\": False,\n",
    "    \"data_path\": \"/project/3022057.01/HCP/REST/combined_data.pkl\",\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    \"phenotype\": [\n",
    "        \"tools\",\n",
    "        \"faces\"\n",
    "    ],\n",
    "    \"percentile\": 0.1,\n",
    "    \"outputfolder\": \"tools_v_faces_2_filt\",\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"nPCA_levels\": [8],\n",
    "    \"tangent_class\": True,\n",
    "    \"tan_class_model\": \"SVM (C=0.1)\",\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": 1,\n",
    "    \"b_label\": 0,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": False,\n",
    "    \"paired\": True,\n",
    "    \"data_path\": \"/project/3022057.01/HCP/WM/combined_data.pkl\"\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    \"phenotype\": [\"win\",\"loss\"],\n",
    "    \"percentile\": 0.15,\n",
    "    \"outputfolder\": \"Gamble\",\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 2,\n",
    "    \"nPCA_levels\": [6],\n",
    "    \"tangent_class\": True,\n",
    "    \"tan_class_model\": \"SVM (C=0.1)\",\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": 1,\n",
    "    \"b_label\": 0,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": False,\n",
    "    \"paired\": True,\n",
    "    \"data_path\": \"/project/3022057.01/HCP/GAMBLING/combined_data.pkl\",\n",
    "}\n",
    "\n",
    "\n",
    "settings = {\n",
    "    \"phenotype\": [\"relation\",\"match\"],\n",
    "    \"percentile\": 0.15,\n",
    "    \"outputfolder\": \"Relation\",\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"nPCA_levels\": [8],\n",
    "    \"tangent_class\": True,\n",
    "    \"tan_class_model\": \"SVM (C=0.1)\",\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": 1,\n",
    "    \"b_label\": 0,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": False,\n",
    "    \"paired\": True,\n",
    "    \"data_path\": \"/project/3022057.01/HCP/RELATIONAL/combined_data.pkl\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/project/3022057.01/IFA/utils')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from preprocessing import prepare_data\n",
    "# from regression import continuous_confounders, categorical_confounders, confounders\n",
    "from pyriemann.estimation import Covariances\n",
    "import hcp_utils as hcp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "\n",
    "# check_conf(full_data,phenotype[0],phen_confounders)\n",
    "# --- Load Settings & Run Preparation ---\n",
    "\n",
    "settings = {\n",
    "    \"phenotype\": [\"relation\", \"match\"],\n",
    "    \"percentile\": 0.15,\n",
    "    \"outputfolder\": \"relation_final_log\",\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"nPCA_levels\": [8],\n",
    "    \"tangent_class\": True,\n",
    "    \"tan_class_model\": \"SVM (C=0.1)\",\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": 1,\n",
    "    \"b_label\": 0,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": False,\n",
    "    \"paired\": True,\n",
    "    \"data_path\": \"/project/3022057.01/HCP/RELATIONAL/combined_data.pkl\",\n",
    "    \"cov_log\": False,         # Add a boolean flag for log-Euclidean metric\n",
    "    \"shrinkage\": 0.1,         # Set shrinkage value\n",
    "}\n",
    "\n",
    "\n",
    "prepare_data(settings,threshold_percentile=.001)\n",
    "\n",
    "# Ensure the output folder exists\n",
    "outputfolder = settings[\"outputfolder\"]\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)\n",
    "\n",
    "n_folds = settings[\"n_folds\"]\n",
    "with open(os.path.join(outputfolder, \"family_ID.pkl\"), \"rb\") as f:\n",
    "    family_ID = pickle.load(f)\n",
    "labels = np.load(os.path.join(outputfolder, \"labels.npy\"))\n",
    "data = np.load(os.path.join(outputfolder, \"data.npy\"))\n",
    "random_state = settings[\"random_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fold_script = \"/project/3022057.01/IFA/run_IFA/Partial/run_fold_partial.sh\"\n",
    "\n",
    "# New job per fold\n",
    "for fold in range(5):\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "\n",
    "    command = [\n",
    "        \"sbatch\",\n",
    "        \"--job-name\", f\"fold_{fold}\",\n",
    "        \"--output\", os.path.join(fold_output_dir, \"slurm-%j.out\"),\n",
    "        \"--error\", os.path.join(fold_output_dir, \"slurm-%j.err\"),\n",
    "        run_fold_script,  # Path to `run_fold.sh`\n",
    "        outputfolder,     # Pass outputfolder as first argument\n",
    "        str(fold),         # Pass fold as second argument\n",
    "    ]\n",
    "    \n",
    "    # Submit the job\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-fold setup\n",
    "# https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "splits = list(sgkf.split(data, labels, groups=family_ID))\n",
    "# run_fold_script = \"/project/3022057.01/IFA/run_IFA/run_fold.sh\"\n",
    "run_fold_script = \"/project/3022057.01/IFA/run_IFA/Partial/run_fold_partial.sh\"\n",
    "\n",
    "# New job per fold\n",
    "for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    # Create Fold Outputfolder\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "    if not os.path.exists(fold_output_dir):\n",
    "        os.makedirs(fold_output_dir)\n",
    "    indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "    if not os.path.exists(indices_dir):\n",
    "        os.makedirs(indices_dir)\n",
    "    np.save(os.path.join(indices_dir, \"train_idx.npy\"), train_idx)\n",
    "    np.save(os.path.join(indices_dir, \"test_idx.npy\"), test_idx)\n",
    "\n",
    "    # Prepare SLURM command to call `run_fold.sh` with arguments for outputfolder and fold\n",
    "    command = [\n",
    "        \"sbatch\",\n",
    "        \"--job-name\", f\"fold_{fold}\",\n",
    "        \"--output\", os.path.join(fold_output_dir, \"slurm-%j.out\"),\n",
    "        \"--error\", os.path.join(fold_output_dir, \"slurm-%j.err\"),\n",
    "        run_fold_script,  # Path to `run_fold.sh`\n",
    "        outputfolder,     # Pass outputfolder as first argument\n",
    "        str(fold),         # Pass fold as second argument\n",
    "    ]\n",
    "    \n",
    "    # Submit the job\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/project/3022057.01/IFA/utils')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from preprocessing import prepare_data\n",
    "# from regression import continuous_confounders, categorical_confounders, confounders\n",
    "from pyriemann.estimation import Covariances\n",
    "import hcp_utils as hcp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "\n",
    "# check_conf(full_data,phenotype[0],phen_confounders)\n",
    "# --- Load Settings & Run Preparation ---\n",
    "settings = {\n",
    "    \"phenotype\": [\"PicVocab_Unadj\",\"PMAT24_A_CR\"], #,\"LifeSatisf_Unadj\",SSAGA_Educ \"DDisc_AUC_200\",\"PMAT24_A_CR\"],\n",
    "    \"percentile\": 0.2,\n",
    "    \"outputfolder\": \"TEST\",\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"nPCA_levels\": [8],\n",
    "    \"tangent_class\": True,\n",
    "    \"tan_class_model\": \"SVM (C=0.1)\",\n",
    "    \"metric\": \"logeuclid\",\n",
    "    \"a_label\": 1,\n",
    "    \"b_label\": 0,\n",
    "    \"self_whiten\": False,\n",
    "    \"deconfound\": True,\n",
    "    \"paired\": False,\n",
    "    \"data_path\": \"/project/3022057.01/HCP/REST/combined_data.pkl\",\n",
    "}\n",
    "\n",
    "prepare_data(settings,threshold_percentile=.001)\n",
    "\n",
    "# Ensure the output folder exists\n",
    "outputfolder = settings[\"outputfolder\"]\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)\n",
    "\n",
    "n_folds = settings[\"n_folds\"]\n",
    "with open(os.path.join(outputfolder, \"family_ID.pkl\"), \"rb\") as f:\n",
    "    family_ID = pickle.load(f)\n",
    "labels = np.load(os.path.join(outputfolder, \"labels.npy\"))\n",
    "data = np.load(os.path.join(outputfolder, \"data.npy\"))\n",
    "random_state = settings[\"random_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-fold setup\n",
    "# https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "splits = list(sgkf.split(data, labels, groups=family_ID))\n",
    "run_fold_script = \"/project/3022057.01/IFA/run_IFA/run_fold.sh\"\n",
    "\n",
    "# New job per fold\n",
    "for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    # Create Fold Outputfolder\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "    if not os.path.exists(fold_output_dir):\n",
    "        os.makedirs(fold_output_dir)\n",
    "    indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "    if not os.path.exists(indices_dir):\n",
    "        os.makedirs(indices_dir)\n",
    "    np.save(os.path.join(indices_dir, \"train_idx.npy\"), train_idx)\n",
    "    np.save(os.path.join(indices_dir, \"test_idx.npy\"), test_idx)\n",
    "\n",
    "    # # Prepare SLURM command to call `run_fold.sh` with arguments for outputfolder and fold\n",
    "    # command = [\n",
    "    #     \"sbatch\",\n",
    "    #     \"--job-name\", f\"fold_{fold}\",\n",
    "    #     \"--output\", os.path.join(fold_output_dir, \"slurm-%j.out\"),\n",
    "    #     \"--error\", os.path.join(fold_output_dir, \"slurm-%j.err\"),\n",
    "    #     run_fold_script,  # Path to `run_fold.sh`\n",
    "    #     outputfolder,     # Pass outputfolder as first argument\n",
    "    #     str(fold),         # Pass fold as second argument\n",
    "    # ]\n",
    "    \n",
    "    # Submit the job\n",
    "    # subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "import hcp_utils as hcp\n",
    "\n",
    "# Add the path to custom modules\n",
    "sys.path.append('/project/3022057.01/IFA/utils')\n",
    "\n",
    "# Import necessary modules\n",
    "from analysis import evaluate, compare\n",
    "from PCA import PPCA, migp\n",
    "from filters import whiten, orthonormalize_filters, save_brain\n",
    "from ICA import ICA, threshold_and_visualize\n",
    "from DualRegression import DualRegress\n",
    "from filters import TSSF, FKT, evaluate_filters\n",
    "from tangent import tangent_classification\n",
    "from haufe import filter_dual_regression\n",
    "\n",
    "def save_text_results(text, filepath):\n",
    "    \"\"\"Save text results to a file.\"\"\"\n",
    "    with open(filepath, \"a\") as f:  # Using 'a' to append results to the file\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "def check_job_completion(job_id):\n",
    "    \"\"\"Poll the status of a job and wait until it reaches a final state.\"\"\"\n",
    "    while True:\n",
    "        job_status = subprocess.run(\n",
    "            [\"sacct\", \"-j\", job_id, \"--format=State\", \"--noheader\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        # Split lines and take the first non-empty line as the status\n",
    "        state = job_status.stdout.splitlines()[0].strip()\n",
    "        \n",
    "        if \"COMPLETED\" in state:\n",
    "            return True\n",
    "        elif any(status in state for status in [\"FAILED\", \"CANCELLED\", \"TIMEOUT\"]):\n",
    "            return False\n",
    "        \n",
    "        # Sleep for a bit before checking again\n",
    "        time.sleep(120)  # Poll every 120 seconds\n",
    "\n",
    "def major_recon_discrim(discrim_basis, major_space,output_folder):\n",
    "    try:\n",
    "        # Compute reconstruction\n",
    "        reconstructed = discrim_basis.T @ np.linalg.pinv(major_space) @ major_space\n",
    "        numerator = np.linalg.norm(discrim_basis.T - reconstructed, 'fro') ** 2\n",
    "        denominator = np.linalg.norm(discrim_basis.T, 'fro') ** 2\n",
    "        reconstruction_percentage = (1 - numerator / denominator)\n",
    "        print(\"Reconstruction Percentage:\", reconstruction_percentage)\n",
    "        \n",
    "        # Save the reconstruction percentage\n",
    "        recon_file = os.path.join(output_folder, \"discriminant_reconstruction_percentage_vt.txt\")\n",
    "        with open(recon_file, \"w\") as f:\n",
    "            f.write(str(reconstruction_percentage) + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to compute reconstruction percentage:\", e)\n",
    "        reconstruction_percentage = None\n",
    "\n",
    "def PPCA_ICA(reducedsubs,basis=None, n_components=None, IFA=True, self_whiten=False,random_state=42,whiten_method=\"InvCov\", output_folder=None):\n",
    "    if IFA:\n",
    "        if self_whiten:\n",
    "            ## Whiten Basis, will need to whiten because this is a combined basis; method chosen wil rotate it to change ICA unmixing starting position (ICA unmixing is nondeterministic)\n",
    "            basis, _ = whiten(basis, n_components=basis.shape[0], method=whiten_method)\n",
    "        \n",
    "        # Variance Normalize Data (PPCA is only being used for variance normalizing data since we already have the basis)\n",
    "        data_vn, _ = PPCA(reducedsubs.copy(), filters=basis.T, threshold=0.0, niters=1)\n",
    "    else:\n",
    "        # For group ICA need to use PPCA to get the major space to match the dimensionality from IFA\n",
    "        data_vn, basis = PPCA(reducedsubs.copy(), threshold=0.0, niters=1, n=n_components)\n",
    "\n",
    "        if self_whiten:\n",
    "            ## Although basis is orthogonal, this rewhitening accounts for number of samples for whitening\n",
    "            basis, _ = whiten(basis, n_components=basis.shape[0], method=whiten_method)\n",
    "\n",
    "    spatial_maps, A, W = ICA(data_vn, basis, whiten=(not self_whiten), output_dir=output_folder,random_state=random_state)\n",
    "    zmaps, zmaps_thresh = threshold_and_visualize(data_vn, W, spatial_maps.T, visualize=True,output_dir=output_folder)\n",
    "\n",
    "    for i in range(zmaps.shape[1]):\n",
    "        save_brain(zmaps[:,i], f\"z_map_{i}\", output_folder)\n",
    "\n",
    "    np.save(os.path.join(output_folder, \"basis.npy\"), basis)\n",
    "    np.save(os.path.join(output_folder, \"data_vn.npy\"), data_vn)\n",
    "    np.save(os.path.join(output_folder, \"spatial_maps.npy\"), spatial_maps)\n",
    "    np.save(os.path.join(output_folder, \"A.npy\"), A)\n",
    "    np.save(os.path.join(output_folder, \"W.npy\"), W)\n",
    "    np.save(os.path.join(output_folder, \"ICA_zmaps.npy\"), zmaps)\n",
    "    np.save(os.path.join(output_folder, \"ICA_zmaps_thresh.npy\"), zmaps_thresh)\n",
    "\n",
    "    return zmaps\n",
    "\n",
    "def run_comparisons(results_list, base_output_folder, pairs, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Run pairwise comparisons for a list of evaluation results.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_list: list of evaluation results (e.g., normalized or unnormalized).\n",
    "    - base_output_folder: base directory where comparison subfolders will be created.\n",
    "    - pairs: list of tuples (i, j, label_one, label_two) indicating the indices in results_list and their labels.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_output_folder):\n",
    "        os.makedirs(base_output_folder)\n",
    "        \n",
    "    for i, j, label_one, label_two in pairs:\n",
    "        pair_dir = os.path.join(base_output_folder, f\"{label_one}_vs_{label_two}\")\n",
    "        if not os.path.exists(pair_dir):\n",
    "            os.makedirs(pair_dir)\n",
    "        compare(\n",
    "            results_list[i], results_list[j],\n",
    "            label_one=label_one, label_two=label_two,\n",
    "             alpha=alpha, output_dir=pair_dir\n",
    "        )\n",
    "\n",
    "for fold in range(1):\n",
    "    # Read the settings from the JSON file\n",
    "    with open(os.path.join(outputfolder, \"settings.json\"), \"r\") as f:\n",
    "        settings = json.load(f)\n",
    "    random_state = settings[\"random_state\"]\n",
    "    n_filters_per_group = settings[\"n_filters_per_group\"]\n",
    "    nPCA_levels = settings[\"nPCA_levels\"]\n",
    "    tangent_class = settings[\"tangent_class\"]\n",
    "    tan_class_model = settings[\"tan_class_model\"]\n",
    "    metric = settings[\"metric\"]\n",
    "    a_label = settings[\"a_label\"]\n",
    "    b_label = settings[\"b_label\"]\n",
    "    self_whiten = settings[\"self_whiten\"]\n",
    "    deconfound = settings[\"deconfound\"]\n",
    "    paired = settings[\"paired\"]\n",
    "\n",
    "    # Load pickle files\n",
    "    with open(os.path.join(outputfolder, \"paths.pkl\"), \"rb\") as f:\n",
    "        paths = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(outputfolder, \"family_ID.pkl\"), \"rb\") as f:\n",
    "        family_ID = pickle.load(f)\n",
    "\n",
    "    # Load numpy files\n",
    "    sub_ID = np.load(os.path.join(outputfolder, \"Sub_ID.npy\"))\n",
    "    labels = np.load(os.path.join(outputfolder, \"labels.npy\"))\n",
    "    data = np.load(os.path.join(outputfolder, \"data.npy\"))\n",
    "    covs = np.load(os.path.join(outputfolder, \"covs.npy\"))\n",
    "\n",
    "    # Load Fold Specific Vairables\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "    summary_file_path = os.path.join(fold_output_dir, \"output_summary.txt\")\n",
    "    indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "    train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "    test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "    fold_results = os.path.join(fold_output_dir, \"Results\")\n",
    "    if not os.path.exists(fold_results):\n",
    "        os.makedirs(fold_results)\n",
    "\n",
    "    # Prepare data for train and test sets\n",
    "    train_labels = labels[train_idx]\n",
    "    train_data = data[train_idx]\n",
    "    train_covs = covs[train_idx]\n",
    "    train_paths = paths[train_idx]\n",
    "\n",
    "    test_labels = labels[test_idx]\n",
    "    test_data = data[test_idx]\n",
    "    test_covs = covs[test_idx]\n",
    "  \n",
    "    if deconfound:\n",
    "        with open(os.path.join(outputfolder, \"cat_confounders.pkl\"), \"rb\") as f:\n",
    "            cat_confounders = pickle.load(f)\n",
    "        con_confounders = np.load(os.path.join(outputfolder, \"con_confounders.npy\"))\n",
    "        train_con_confounders = con_confounders[train_idx]\n",
    "        train_cat_confounders = cat_confounders[train_idx]\n",
    "        test_con_confounders = con_confounders[test_idx]\n",
    "        test_cat_confounders = cat_confounders[test_idx]\n",
    "    else:\n",
    "        train_con_confounders = None\n",
    "        train_cat_confounders = None\n",
    "        test_con_confounders = None\n",
    "        test_cat_confounders = None\n",
    "\n",
    "    # Save summary of data split\n",
    "    train_groups = set(np.unique(family_ID[train_idx]))\n",
    "    test_groups = set(np.unique(family_ID[test_idx]))\n",
    "    intersection = train_groups & test_groups\n",
    "    save_text_results(f\"Fold {fold + 1}:\", summary_file_path)\n",
    "    save_text_results(f\"  Train size: {len(train_idx)}\", summary_file_path)\n",
    "    save_text_results(f\"  Test size: {len(test_idx)}\", summary_file_path)\n",
    "    save_text_results(f\"  Train labels distribution: {np.bincount(labels[train_idx].astype(int))}\", summary_file_path)\n",
    "    save_text_results(f\"  Test labels distribution: {np.bincount(labels[test_idx].astype(int))}\", summary_file_path)\n",
    "    save_text_results(f\"  Intersection of groups: {len(intersection)} (Groups: {intersection})\", summary_file_path)\n",
    "    if paired:\n",
    "        paired_train = np.array_equal(\n",
    "            sub_ID[train_idx][train_labels == a_label],\n",
    "            sub_ID[train_idx][train_labels == b_label]\n",
    "        )\n",
    "        paired_test = np.array_equal(\n",
    "            sub_ID[test_idx][test_labels == a_label],\n",
    "            sub_ID[test_idx][test_labels == b_label]\n",
    "        )\n",
    "        save_text_results(f\"  Paired Across Train: {paired_train}\", summary_file_path)\n",
    "        save_text_results(f\"  Paired Across Test: {paired_test}\", summary_file_path)\n",
    "        \n",
    "    # # Run MIGP\n",
    "    # migp_dir = os.path.join(fold_output_dir, \"MIGP\")\n",
    "    # if not os.path.exists(migp_dir):\n",
    "    #     os.makedirs(migp_dir)\n",
    "\n",
    "    # Get Parcellated Filters\n",
    "    filters_dir = os.path.join(fold_output_dir, \"Filters\")\n",
    "    if not os.path.exists(filters_dir):\n",
    "        os.makedirs(filters_dir)\n",
    "    \n",
    "    # # last element in path list is number of timepoints, see load_subject in preprocessing\n",
    "    # m = train_paths[0][-1]\n",
    "    # print(\"Keep this many pseudotime points\", m, flush=True)\n",
    "    # reducedsubsA = migp(train_paths[train_labels == a_label], m=m, n_jobs=15,batch_size=3)\n",
    "    # reducedsubsB = migp(train_paths[train_labels == b_label], m=m, n_jobs=15,batch_size=3)\n",
    "    # np.save(os.path.join(migp_dir, \"reducedsubsA.npy\"), reducedsubsA)\n",
    "    # np.save(os.path.join(migp_dir, \"reducedsubsB.npy\"), reducedsubsB)\n",
    "    # reducedsubs = np.concatenate((reducedsubsA, reducedsubsB), axis=0)\n",
    "    # np.save(os.path.join(migp_dir, \"reducedsubs.npy\"), reducedsubs)\n",
    "\n",
    "\n",
    "    # # Run the PCA job, now just to get the voxel level filters using GPU\n",
    "    # voxel_filters_dir = os.path.join(filters_dir, \"Voxel\")\n",
    "    # if not os.path.exists(voxel_filters_dir):\n",
    "    #     os.makedirs(voxel_filters_dir)\n",
    "\n",
    "    # pca_script = \"/project/3022057.01/IFA/run_IFA/run_pca.sh\"\n",
    "    # pca_command = [\n",
    "    #     \"sbatch\",\n",
    "    #     \"--output\", os.path.join(fold_output_dir, \"pca-%j.out\"),\n",
    "    #     \"--error\", os.path.join(fold_output_dir, \"pca-%j.err\"),\n",
    "    #     pca_script,\n",
    "    #     outputfolder, fold_output_dir, voxel_filters_dir\n",
    "    # ]\n",
    "\n",
    "    # pca_process = subprocess.run(pca_command, capture_output=True, text=True)\n",
    "    # if pca_process.returncode != 0:\n",
    "    #     print(f\"Error submitting PCA job: {pca_process.stderr}\")\n",
    "    #     return\n",
    "    # job_id = pca_process.stdout.strip().split()[-1]\n",
    "    # print(f\"PCA job submitted successfully with job ID: {job_id}\")\n",
    "\n",
    "\n",
    "    # Run tangent classification for measuring separability in parcellated space\n",
    "    tangent_class_metrics = tangent_classification(train_covs, train_labels, test_covs, test_labels, \n",
    "                           clf_str=tan_class_model, z_score=0, metric=metric, deconf=deconfound, \n",
    "                           con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                           con_confounder_test=test_con_confounders, cat_confounder_test=test_cat_confounders)\n",
    "    \n",
    "    # Save those tangent classification results to overall fold results directory\n",
    "    with open(os.path.join(fold_results, \"tangent_class_metrics.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(tangent_class_metrics, f)   \n",
    "    save_text_results(\"Parcellated Tangent Classification \" + str(tangent_class_metrics), summary_file_path)\n",
    "\n",
    "    \n",
    "    # Directory for all things related to the parecllated filters\n",
    "    parcellated_filters_dir = os.path.join(filters_dir, \"Parcellated\")\n",
    "    if not os.path.exists(parcellated_filters_dir):\n",
    "        os.makedirs(parcellated_filters_dir)\n",
    "    \n",
    "    tangent_class = True\n",
    "    n_filters_per_group = 2\n",
    "    if tangent_class:\n",
    "        eigs, filters_all, W, C = TSSF(train_covs, train_labels, \n",
    "                                       clf_str=tan_class_model, metric=metric, deconf=deconfound, \n",
    "                                       con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                                       z_score=0, haufe=False, visualize=True, output_dir=parcellated_filters_dir)\n",
    "    else:\n",
    "        eigs, filters_all = FKT(train_covs, train_labels, a_label, b_label,\n",
    "                                metric=metric, deconf=deconfound, \n",
    "                                con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                                visualize=True, output_dir=parcellated_filters_dir)\n",
    "\n",
    " # if TSSF was used then the lower label is the negative class and corresponds to eigenvalues < 1\n",
    "    if a_label < b_label and tangent_class:\n",
    "        filtersB = filters_all[:, -n_filters_per_group:]\n",
    "        filtersA = filters_all[:, :n_filters_per_group]\n",
    "    else: \n",
    "        filtersA = filters_all[:, -n_filters_per_group:]\n",
    "        filtersB = filters_all[:, :n_filters_per_group]\n",
    "\n",
    "    filters_parcellated = np.concatenate((filtersB, filtersA), axis=1)\n",
    "\n",
    "    np.save(os.path.join(parcellated_filters_dir, \"filtersA.npy\"), filtersA)\n",
    "    np.save(os.path.join(parcellated_filters_dir, \"filtersB.npy\"), filtersB)\n",
    "    np.save(os.path.join(parcellated_filters_dir, \"filters_parcellated.npy\"), filters_parcellated)\n",
    "    for i in range(filters_parcellated.shape[1]):\n",
    "        save_brain(hcp.unparcellate(filters_parcellated[:,i],hcp.mmp), f\"parcellated_filter_{i}\", parcellated_filters_dir)\n",
    "\n",
    "    # Evaluate filters and save those results to overall fold results directory\n",
    "    logvar_stats, logcov_stats = evaluate_filters(train_data, train_labels, test_data, test_labels, \n",
    "                                                    filters_parcellated, metric=metric, deconf=deconfound, \n",
    "                                                    con_confounder_train=train_con_confounders, cat_confounder_train=train_cat_confounders, \n",
    "                                                    con_confounder_test=test_con_confounders, cat_confounder_test=test_cat_confounders,output_dir=parcellated_filters_dir)\n",
    "\n",
    "    with open(os.path.join(fold_results, \"logvar_stats.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(logvar_stats, f)     \n",
    "    with open(os.path.join(fold_results, \"logcov_stats.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(logcov_stats, f)      \n",
    "    save_text_results(\"Log Var Filter Feature Classification \" + str(logvar_stats), summary_file_path)\n",
    "    save_text_results(\"Log Cov Filter Feature Classification \" + str(logcov_stats), summary_file_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fold_script = \"/project/3022057.01/IFA/run_IFA/run_fold.sh\"\n",
    "\n",
    "for fold in range(0,5):\n",
    "    # Create Fold Outputfolder\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "\n",
    "    # Prepare SLURM command to call `run_fold.sh` with arguments for outputfolder and fold\n",
    "    command = [\n",
    "        \"sbatch\",\n",
    "        \"--job-name\", f\"fold_{fold}\",\n",
    "        \"--output\", os.path.join(fold_output_dir, \"slurm-%j.out\"),\n",
    "        \"--error\", os.path.join(fold_output_dir, \"slurm-%j.err\"),\n",
    "        run_fold_script,  # Path to `run_fold.sh`\n",
    "        outputfolder,     # Pass outputfolder as first argument\n",
    "        str(fold),         # Pass fold as second argument\n",
    "    ]\n",
    "    \n",
    "    # Submit the job\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Across Folds and Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "baseline = ([],[])\n",
    "for fold in range(0,5):\n",
    "    if deconfound:\n",
    "        end = \"_deconf\"\n",
    "    else:\n",
    "        end = \"\"\n",
    "    baseline_file = Path(outputfolder) / f\"fold_{fold}\" / f\"Results\" /f\"tangent_class_metrics{end}.pkl\"\n",
    "    with open(baseline_file, \"rb\") as f:\n",
    "        baseline_data = pickle.load(f)\n",
    "    class_keys = baseline_data.keys()\n",
    "    baseline[0].append(class_keys)\n",
    "    baseline[1].append([baseline_data[class_key][\"accuracy\"] for class_key in class_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "from pathlib import Path\n",
    "\n",
    "def load_results(output_folder, prefix, subfold=\"Demeaned\"):\n",
    "    results = {\n",
    "        f\"{prefix}_var_results\": [],\n",
    "        f\"{prefix}_cov_results\": [],\n",
    "        f\"{prefix}_Class_Result\": ([], []),\n",
    "        f\"{prefix}_recon\": ([], [])\n",
    "    }\n",
    "    for fold in range(0,5):\n",
    "        if True:\n",
    "            fold_results_file = Path(output_folder) / f\"fold_{fold}\" /  f\"Results\" /f\"Results_{nPCA}\" / f\"{subfold}\" /f\"{prefix}_results_{subfold.lower()}.pkl\"\n",
    "            with open(fold_results_file, \"rb\") as f:\n",
    "                fold_data = pickle.load(f)\n",
    "            \n",
    "            for key in results.keys():\n",
    "                if \"Class\" in key:\n",
    "                    all_keys = fold_data[key].keys()\n",
    "                    results[key][0].append(all_keys)\n",
    "                    results[key][1].append([fold_data[key][class_key][\"accuracy\"] for class_key in all_keys])\n",
    "                elif \"recon\" in key:\n",
    "                    # (1 - np.linalg.norm(reconstructed - reconstructed.mean())/np.linalg.norm(Xn - Xn.mean()))*100\n",
    "                    # transform = lambda x: (1 - (x/100))**2\n",
    "                    results[key][0].extend((fold_data[key][0]))\n",
    "                    results[key][1].extend((fold_data[key][1]))\n",
    "                else:\n",
    "                    results[key].append(fold_data[key])\n",
    "    return results\n",
    "\n",
    "def summarize_results(results):\n",
    "    summary = {key: [] for key in results.keys()}\n",
    "    for key, values in results.items():\n",
    "        values_array = np.array(values if \"Class\" not in key and \"recon\" not in key else values[1])\n",
    "        if \"_var_results\" in key or \"_cov_results\" in key:\n",
    "            summary[key].extend([np.mean(values_array, axis=0), sem(values_array, axis=0)])\n",
    "        elif \"Class\" in key:\n",
    "            classifiers = results[key][0][0]\n",
    "            avg_accuracy = np.mean(values_array, axis=0)\n",
    "            std_error = sem(values_array, axis=0)\n",
    "            summary[key] = (classifiers, avg_accuracy, std_error)\n",
    "        elif \"recon\" in key:\n",
    "            summary[key] = values\n",
    "    return summary\n",
    "\n",
    "# Define output folders and load/save results\n",
    "output_folder = Path(outputfolder)\n",
    "\n",
    "# Process IFA and ICA results\n",
    "IFA_all_results_norm = load_results(outputfolder, \"IFA\",subfold=\"Normalized\")\n",
    "IFA_results_summary_norm = summarize_results(IFA_all_results_norm)\n",
    "\n",
    "ICA_all_results_norm = load_results(outputfolder, \"ICA\", subfold=\"Normalized\")\n",
    "ICA_results_summary_norm = summarize_results(ICA_all_results_norm)\n",
    "\n",
    "\n",
    "# Process IFA and ICA results\n",
    "IFA_all_results_demean = load_results(outputfolder, \"IFA\",subfold=\"Demeaned\")\n",
    "IFA_results_summary_demean = summarize_results(IFA_all_results_demean)\n",
    "\n",
    "ICA_all_results_demean = load_results(outputfolder, \"ICA\", subfold=\"Demeaned\")\n",
    "ICA_results_summary_demean = summarize_results(ICA_all_results_demean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "from pathlib import Path\n",
    "\n",
    "def across_dims(output_folder, prefix, subfold=\"Demeaned\"):\n",
    "    results = []\n",
    "    comp = [5, 30]\n",
    "    for n_comp in comp:\n",
    "        comp_list = [n_comp+n_filters_per_group*2]\n",
    "        class_acc = []\n",
    "        var_explain_train = []\n",
    "        var_explain_test = []\n",
    "        recon = []\n",
    "        for fold in range(0,5):\n",
    "            fold_results_file = Path(output_folder) / f\"fold_{fold}\" / f\"Results\" /f\"Results_{n_comp}\" / f\"{subfold}\" /f\"{prefix}_results_{subfold.lower()}.pkl\"\n",
    "            with open(fold_results_file, \"rb\") as f:\n",
    "                fold_data = pickle.load(f)\n",
    "            \n",
    "            for key in fold_data.keys():\n",
    "                if \"Class\" in key:\n",
    "                    all_keys = set(fold_data[key].keys())\n",
    "                    keys_to_exclude = {\"SVM (C=0.001)\", \"L2 SVM (C=0.001)\"}\n",
    "                    class_keys = list(all_keys - keys_to_exclude)\n",
    "                    class_acc.extend([fold_data[key][class_key][\"accuracy\"] for class_key in class_keys])\n",
    "                elif \"recon\" in key:\n",
    "                    var_explain_train.extend((fold_data[key][0]))\n",
    "                    var_explain_test.extend((fold_data[key][1]))\n",
    "            \n",
    "            discriminant = np.load(os.path.join(outputfolder,f\"fold_{fold}/Filters/haufe_filters_ortho.npy\"))\n",
    "            variance_explaining = np.load(os.path.join(outputfolder,f\"fold_{fold}/MIGP/vt_{n_comp}.npy\"))\n",
    "            reconstructed = discriminant.T@np.linalg.pinv(variance_explaining)@variance_explaining\n",
    "            numerator = np.linalg.norm(discriminant.T - reconstructed, 'fro') ** 2\n",
    "            denominator = np.linalg.norm(discriminant.T, 'fro') ** 2\n",
    "            reconstruction_percentage = (1 - numerator / denominator)\n",
    "            recon.append(reconstruction_percentage)\n",
    "\n",
    "        comp_list.extend([np.mean(class_acc),sem(class_acc),np.mean(var_explain_train),np.mean(var_explain_test),np.mean(recon),sem(recon)])\n",
    "        results.append(comp_list)\n",
    "    \n",
    "    return results\n",
    "\n",
    "IFA_across_dims = np.array(across_dims(outputfolder, \"IFA\", subfold=\"Normalized\"))\n",
    "ICA_across_dims = np.array(across_dims(outputfolder, \"ICA\", subfold=\"Normalized\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot IFA and ICA accuracies with error bars\n",
    "plt.errorbar(\n",
    "    IFA_across_dims[:, 0], IFA_across_dims[:, 1], yerr=IFA_across_dims[:, 2],\n",
    "    fmt='-o', label='IFA Accuracy', color='blue', capsize=3\n",
    ")\n",
    "plt.errorbar(\n",
    "    ICA_across_dims[:, 0], ICA_across_dims[:, 1], yerr=ICA_across_dims[:, 2],\n",
    "    fmt='-o', label='ICA Accuracy', color='orange', capsize=3\n",
    ")\n",
    "\n",
    "# # Plot the baseline line\n",
    "# baseline_mean = np.mean(np.array(baseline[1]))\n",
    "# baseline_sem = sem(np.array(baseline[1]).flatten())\n",
    "# plt.plot(\n",
    "#     IFA_across_dims[:, 0], np.tile(baseline_mean, len(IFA_across_dims[:, 0])),\n",
    "#     label='Baseline Accuracy', color='red', linestyle='--'\n",
    "# )\n",
    "# plt.fill_between(\n",
    "#     IFA_across_dims[:, 0],\n",
    "#     baseline_mean - baseline_sem,\n",
    "#     baseline_mean + baseline_sem,\n",
    "#     color='red', alpha=0.2\n",
    "# )\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"Number of Spatial Components\", fontsize=12)\n",
    "plt.ylabel(\"Classification Accuracy\", fontsize=12)\n",
    "plt.title(\"Classification Accuracy Averaged Across Models and Folds\", fontsize=14)\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the combined SEM for the difference https://www.statisticshowto.com/statistics-basics/error-propagation/\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Reconstruction Percentage on the left y-axis\n",
    "color = 'black'\n",
    "plt.xlabel(\"Number of Spatial Components\", fontsize=12)\n",
    "ax1.set_ylabel(\"Reconstruction Percentage (%) Averaged Across Folds\", color=color, fontsize=12)\n",
    "ax1.errorbar(\n",
    "    IFA_across_dims[:, 0], IFA_across_dims[:, 5], yerr=IFA_across_dims[:, 6],\n",
    "    fmt='-o', label='Discriminant information captured by PCA via linear reconstruction', color=color, capsize=3\n",
    ")\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Plot Accuracy Difference on the right y-axis\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "sem_diff = np.sqrt(IFA_across_dims[:, 2]**2 + ICA_across_dims[:, 2]**2)\n",
    "ax2.set_ylabel(\"Accuracy Difference (IFA - ICA)\", color=color, fontsize=12)\n",
    "ax2.errorbar(\n",
    "    IFA_across_dims[:, 0], IFA_across_dims[:, 1] - ICA_across_dims[:, 1],\n",
    "    yerr=sem_diff, fmt='-o', label='Accuracy Difference', color=color, capsize=3\n",
    ")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add title and legends\n",
    "fig.suptitle(\"Reconstruction of Discriminant Information and Accuracy Difference Between IFA & ICA\", fontsize=14)\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax2.legend(loc='upper right', fontsize=10)\n",
    "fig.tight_layout()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Variance Explained for Train and Test\n",
    "ax.plot(\n",
    "    IFA_across_dims[:, 0], IFA_across_dims[:, 3], 'o-', label='IFA Train Variance Explained', color='blue'\n",
    ")\n",
    "ax.plot(\n",
    "    IFA_across_dims[:, 0], IFA_across_dims[:, 4], 'o--', label='IFA Test Variance Explained', color='blue'\n",
    ")\n",
    "ax.plot(\n",
    "    IFA_across_dims[:, 0], ICA_across_dims[:, 3], 'o-', label='ICA Train Variance Explained', color='orange'\n",
    ")\n",
    "ax.plot(\n",
    "    IFA_across_dims[:, 0], ICA_across_dims[:, 4], 'o--', label='ICA Test Variance Explained', color='orange'\n",
    ")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel(\"Number of Spatial Components\", fontsize=12)\n",
    "ax.set_ylabel(\"Average Variance Explained Across Subjects\", fontsize=12)\n",
    "ax.set_title(\"Subject-Level Variance Explained by IFA or ICA Basis\", fontsize=14)\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from analysis import reconstruction_plot\n",
    "\n",
    "def plot_model_comparison(x, mean_IFA, sterr_IFA, mean_ICA, sterr_ICA,xlabel=\"\",ylabel=\"\",title=\"\",output_dir=\"path\", baseline=None):   \n",
    "    # Increase the spacing between bins\n",
    "    spacing_factor = 10  # Increased spacing factor from 2 to 3\n",
    "    x_positions = np.arange(len(x)) * spacing_factor\n",
    "    \n",
    "    # Width for offsets (should be less than half of spacing_factor)\n",
    "    width = (spacing_factor / 5)*1\n",
    "    \n",
    "    # Offsets for each method\n",
    "    offsets = [-width, 0, width]\n",
    "    \n",
    "    # Adjust x-values for each method\n",
    "    x_IFA = x_positions + offsets[0]\n",
    "    x_ICA = x_positions + offsets[2]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  # Adjust the width (e.g., 12) to make the figure wider\n",
    "    # Plotting TSSF, ICA, and FKT with adjusted x-values\n",
    "    plt.errorbar(x_ICA, mean_ICA, yerr=sterr_ICA, fmt='o', label='ICA', color='orange', capsize=0)\n",
    "    plt.errorbar(x_IFA, mean_IFA, yerr=sterr_IFA, fmt='o', label='IFA', color='blue', capsize=0)\n",
    "    if baseline is not None:\n",
    "        mean_baseline = np.mean(baseline[1], axis=0)\n",
    "        sem_baseline = sem(baseline[1], axis=0)\n",
    "\n",
    "        # Plot the baseline line\n",
    "        plt.plot(x_positions, mean_baseline, label='Baseline', color='red')\n",
    "\n",
    "        # Add shaded error region\n",
    "        plt.fill_between(\n",
    "            x_positions,\n",
    "            mean_baseline - sem_baseline,\n",
    "            mean_baseline + sem_baseline,\n",
    "            color='red',\n",
    "            alpha=0.3,  # Transparency of the shading\n",
    "        )\n",
    "\n",
    "    # Set x-ticks to x_positions without offsets, labels to models\n",
    "    plt.xticks(x_positions, x, rotation=45, ha='right')\n",
    "\n",
    "    \n",
    "    # Formatting plot\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f'{title}.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_output_dir_normalized = output_folder / \"Results_Normalized\"\n",
    "all_results_output_dir_normalized.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_var_results\"][0][:,0],IFA_results_summary_norm[\"IFA_var_results\"][0][:,2],IFA_results_summary_norm[\"IFA_var_results\"][1][:,2],ICA_results_summary_norm[\"ICA_var_results\"][0][:,2],ICA_results_summary_norm[\"ICA_var_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Var_Accuracy_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_var_results\"][0][:,0],IFA_results_summary_norm[\"IFA_var_results\"][0][:,1],IFA_results_summary_norm[\"IFA_var_results\"][1][:,1],ICA_results_summary_norm[\"ICA_var_results\"][0][:,1],ICA_results_summary_norm[\"ICA_var_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Var_Distance_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_cov_results\"][0][:,0],IFA_results_summary_norm[\"IFA_cov_results\"][0][:,2],IFA_results_summary_norm[\"IFA_cov_results\"][1][:,2],ICA_results_summary_norm[\"ICA_cov_results\"][0][:,2],ICA_results_summary_norm[\"ICA_cov_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Cov_Accuracy_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_cov_results\"][0][:,0],IFA_results_summary_norm[\"IFA_cov_results\"][0][:,1],IFA_results_summary_norm[\"IFA_cov_results\"][1][:,1],ICA_results_summary_norm[\"ICA_cov_results\"][0][:,1],ICA_results_summary_norm[\"ICA_cov_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Cov_Distance_Across_Folds\",output_dir=all_results_output_dir_normalized)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_norm[\"IFA_Class_Result\"][0],IFA_results_summary_norm[\"IFA_Class_Result\"][1],IFA_results_summary_norm[\"IFA_Class_Result\"][2],ICA_results_summary_norm[\"ICA_Class_Result\"][1],ICA_results_summary_norm[\"ICA_Class_Result\"][2],xlabel=\"Model\", ylabel=\"Accuracy\", title=\"Tangent Netmat Accuracy Across Folds\",output_dir=all_results_output_dir_normalized, baseline=baseline)\n",
    "\n",
    "reconstruction_plot(IFA_results_summary_norm[\"IFA_recon\"][0], ICA_results_summary_norm[\"ICA_recon\"][0],label=\"Train\",output_dir=all_results_output_dir_normalized)\n",
    "reconstruction_plot(IFA_results_summary_norm[\"IFA_recon\"][1], ICA_results_summary_norm[\"ICA_recon\"][1],label=\"Test\",output_dir=all_results_output_dir_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_output_dir_demean = output_folder / \"Results_Demeaned\"\n",
    "all_results_output_dir_demean.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_var_results\"][0][:,0],IFA_results_summary_demean[\"IFA_var_results\"][0][:,2],IFA_results_summary_demean[\"IFA_var_results\"][1][:,2],ICA_results_summary_demean[\"ICA_var_results\"][0][:,2],ICA_results_summary_demean[\"ICA_var_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Var_Accuracy_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_var_results\"][0][:,0],IFA_results_summary_demean[\"IFA_var_results\"][0][:,1],IFA_results_summary_demean[\"IFA_var_results\"][1][:,1],ICA_results_summary_demean[\"ICA_var_results\"][0][:,1],ICA_results_summary_demean[\"ICA_var_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Var_Distance_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_cov_results\"][0][:,0],IFA_results_summary_demean[\"IFA_cov_results\"][0][:,2],IFA_results_summary_demean[\"IFA_cov_results\"][1][:,2],ICA_results_summary_demean[\"ICA_cov_results\"][0][:,2],ICA_results_summary_demean[\"ICA_cov_results\"][1][:,2],xlabel=\"Number of FKT Filters\", ylabel=\"SVM Accuracy\", title=\"Log_Cov_Accuracy_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_cov_results\"][0][:,0],IFA_results_summary_demean[\"IFA_cov_results\"][0][:,1],IFA_results_summary_demean[\"IFA_cov_results\"][1][:,1],ICA_results_summary_demean[\"ICA_cov_results\"][0][:,1],ICA_results_summary_demean[\"ICA_cov_results\"][1][:,1],xlabel=\"Number of FKT Filters\", ylabel=\"Riemannian Distance\", title=\"Log_Cov_Distance_Across_Folds\",output_dir=all_results_output_dir_demean)\n",
    "\n",
    "plot_model_comparison(IFA_results_summary_demean[\"IFA_Class_Result\"][0],IFA_results_summary_demean[\"IFA_Class_Result\"][1],IFA_results_summary_demean[\"IFA_Class_Result\"][2],ICA_results_summary_demean[\"ICA_Class_Result\"][1],ICA_results_summary_demean[\"ICA_Class_Result\"][2],xlabel=\"Model\", ylabel=\"Accuracy\", title=\"Tangent Netmat Accuracy Across Folds\",output_dir=all_results_output_dir_demean, baseline=baseline)\n",
    "\n",
    "reconstruction_plot(IFA_results_summary_demean[\"IFA_recon\"][0], ICA_results_summary_demean[\"ICA_recon\"][0],label=\"Train\",output_dir=all_results_output_dir_demean)\n",
    "reconstruction_plot(IFA_results_summary_demean[\"IFA_recon\"][1], ICA_results_summary_demean[\"ICA_recon\"][1],label=\"Test\",output_dir=all_results_output_dir_demean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import load_subject, process_subject\n",
    "import nibabel as nib\n",
    "import nibabel as nib\n",
    "import nilearn.plotting as plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hcp_utils as hcp\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFA_groupmaps = np.load(\"/project/3022057.01/Run_19/fold_0/ICA/IFA/IFA_zmaps_thresh.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(IFA_groupmaps[:,2]), threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(IFA_groupmaps[:,4]), threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(IFA_groupmaps[:,5]), threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Spatial Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nilearn import datasets, surface, image\n",
    "from nilearn.plotting import view_surf\n",
    "from scipy.stats import zscore, pearsonr\n",
    "from scipy.ndimage import label\n",
    "import hcp_utils as hcp  # For HCP surface meshes\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Function: Convert volumetric maps to surface space\n",
    "def vol_to_surface_map(volume_map, mesh_L, mesh_R):\n",
    "    surf_left = surface.vol_to_surf(volume_map, mesh_L)\n",
    "    surf_right = surface.vol_to_surf(volume_map, mesh_R)\n",
    "    return np.hstack([surf_left, surf_right])\n",
    "\n",
    "# Fetch the Smith RSN templates (volumetric data)\n",
    "rsn_templates = datasets.fetch_atlas_smith_2009().rsn10  # 10-component RSN atlas\n",
    "\n",
    "# Load HCP surface meshes\n",
    "fsLR_mesh_L = hcp.mesh.inflated_left\n",
    "fsLR_mesh_R = hcp.mesh.inflated_right\n",
    "\n",
    "# Process all folds\n",
    "for fold in range(1):\n",
    "    print(f\"Processing fold {fold}...\")\n",
    "    \n",
    "    # Load spatial maps and test indices\n",
    "    fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "    fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "    fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "    dual_dir = os.path.join(fold_output_dir, f\"Dual_Regression_{nPCA}\") #TODO Delete\n",
    "    fold_results_normalized = os.path.join(fold_results_nPCA, f\"Normalized\")\n",
    "    fold_results_demeaned = os.path.join(fold_results_nPCA, f\"Demeaned\")\n",
    "    indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "    train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "    test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "    labels = np.load(os.path.join(outputfolder,\"labels.npy\"))\n",
    "    ica_data = np.load(os.path.join(dual_dir, \"IFA_spatial_mapdm.npy\"))\n",
    "    \n",
    "    # Average across subjects and Z-score\n",
    "    ica_data = np.mean(ica_data[test_idx, :, :], axis=0)\n",
    "    ica_data = zscore(ica_data, axis=1)  # Shape: n_components x grayordinates\n",
    "\n",
    "    # Initialize combined map and storage for component maps\n",
    "    combined_map = np.zeros(hcp.cortex_data(ica_data[0, :]).shape[0])\n",
    "    individual_maps = []\n",
    "\n",
    "    # Convert RSN templates to surface space\n",
    "    rsn_maps = [vol_to_surface_map(rsn, fsLR_mesh_L, fsLR_mesh_R) for rsn in image.iter_img(rsn_templates)]\n",
    "\n",
    "    # Process each spatial component\n",
    "    for i, spatial_map in enumerate(ica_data):\n",
    "        # Convert component map to surface space\n",
    "        spatial_map_surf = hcp.cortex_data(spatial_map)\n",
    "\n",
    "        # Threshold the map\n",
    "        threshold = 2.58  # p < 0.01 (two-tailed)\n",
    "\n",
    "        binary_map = (np.abs(spatial_map_surf) > threshold).astype(int)\n",
    "\n",
    "        # Identify clusters\n",
    "        labeled_clusters, num_clusters = label(binary_map)\n",
    "\n",
    "        # Keep significant clusters (e.g., size > 5)\n",
    "        component_map = np.zeros_like(combined_map)\n",
    "        for cluster_id in range(1, num_clusters + 1):\n",
    "            cluster_mask = (labeled_clusters == cluster_id).astype(int)\n",
    "            if cluster_mask.sum() >= 5:\n",
    "                combined_map += cluster_mask * (i + 1)\n",
    "                component_map += cluster_mask * (i + 1)\n",
    "\n",
    "        # Store individual component map\n",
    "        individual_maps.append(spatial_map_surf)\n",
    "\n",
    "        # Correlate with RSN templates\n",
    "        correlations = [pearsonr(spatial_map_surf, rsn_surf)[0] for rsn_surf in rsn_maps]\n",
    "        best_match = np.argmax(correlations)\n",
    "        print(f\"Component {i}: Best Match - RSN {best_match}, Correlation: {correlations[best_match]:.2f}\")\n",
    "\n",
    "    # Visualization: Combined Map\n",
    "    base_cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "    colors = [(1, 1, 1)] + list(base_cmap.colors)  # Add white for background\n",
    "    custom_cmap = ListedColormap(colors)\n",
    "\n",
    "    view_combined = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=combined_map,\n",
    "        cmap=custom_cmap,\n",
    "        symmetric_cmap=False,\n",
    "        black_bg=False,\n",
    "        colorbar=False,\n",
    "        title=f\"Fold {fold}: Combined Thresholded Components\"\n",
    "    )\n",
    "    display(view_combined)\n",
    "\n",
    "    # Visualization: Individual Components\n",
    "    for idx, component_map in enumerate(individual_maps, start=0):\n",
    "        view_component = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=component_map,\n",
    "            # cmap=custom_cmap,\n",
    "            # symmetric_cmap=False,\n",
    "            # black_bg=False,\n",
    "            colorbar=True,\n",
    "            title=f\"Fold {fold}: Component {idx}\"\n",
    "        )\n",
    "        display(view_component)\n",
    "\n",
    "    # Add color legend\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for idx, color in enumerate(colors[1:], start=1):\n",
    "        plt.bar(idx, 1, color=color, edgecolor=\"black\")\n",
    "        plt.text(idx, 0.5, f\"Comp {idx-1}\", ha=\"center\", va=\"center\", fontsize=10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Color Legend for Components\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Processing complete for all folds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import spatial_t_test\n",
    "\n",
    "def run_t(time_norm=False):\n",
    "    for fold in range(5):\n",
    "        print(f\"Processing fold {fold}...\")\n",
    "        # Load spatial maps and test indices\n",
    "        fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "        dual_dir = os.path.join(fold_output_dir, f\"Dual_Regression_{nPCA}\") #TODO Delete\n",
    "\n",
    "        indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "        test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "        labels = np.load(os.path.join(outputfolder,\"labels.npy\"))\n",
    "        test_labels = labels[test_idx]\n",
    "\n",
    "        if time_norm:\n",
    "            results = os.path.join(fold_results_nPCA, f\"Normalized\")\n",
    "            IFA_SpatialMaps_test = np.load(os.path.join(dual_dir, \"IFA_spatial_maps.npy\"))[test_idx]\n",
    "            ICA_SpatialMaps_test = np.load(os.path.join(dual_dir, \"ICA_spatial_maps.npy\"))[test_idx]\n",
    "        else:\n",
    "            results = os.path.join(fold_results_nPCA, f\"Demeaned\")\n",
    "            IFA_SpatialMaps_test = np.load(os.path.join(dual_dir, \"IFA_spatial_mapdm.npy\"))[test_idx]\n",
    "            ICA_SpatialMaps_test = np.load(os.path.join(dual_dir, \"ICA_spatial_mapdm.npy\"))[test_idx]\n",
    "\n",
    "\n",
    "        spatial_t_test(IFA_SpatialMaps_test, test_labels, alpha=0.05, permutations=False, correction=\"fdr_bh\", output_dir=os.path.join(results, \"IFA_SpatialMaps\"), basis=\"IFA\",corr_across_maps=False)\n",
    "        spatial_t_test(ICA_SpatialMaps_test, test_labels, alpha=0.05, permutations=False, correction=\"fdr_bh\", output_dir=os.path.join(results, \"ICA_SpatialMaps\"), basis=\"ICA\",corr_across_maps=False)\n",
    "run_t(time_norm=True)\n",
    "run_t(time_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "def visualize_spatial_variance_per_map(time_norm=False):\n",
    "    methods = [\"IFA\", \"ICA\"]\n",
    "    IFA_results = []\n",
    "    ICA_results = []\n",
    "    for fold in range(5):\n",
    "        fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        dual_dir = os.path.join(fold_output_dir, f\"Dual_Regression_{nPCA}\")\n",
    "        indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "        test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "        labels = np.load(os.path.join(outputfolder, \"labels.npy\"))\n",
    "        test_labels = labels[test_idx]\n",
    "\n",
    "        if time_norm:\n",
    "            IFA_SpatialMaps = np.load(os.path.join(dual_dir, \"IFA_spatial_maps.npy\"))[test_idx]\n",
    "            ICA_SpatialMaps = np.load(os.path.join(dual_dir, \"ICA_spatial_maps.npy\"))[test_idx]\n",
    "        else:\n",
    "            IFA_SpatialMaps = np.load(os.path.join(dual_dir, \"IFA_spatial_mapdm.npy\"))[test_idx]\n",
    "            ICA_SpatialMaps = np.load(os.path.join(dual_dir, \"ICA_spatial_mapdm.npy\"))[test_idx]\n",
    "\n",
    "        for method, spatial_maps in zip(methods, [IFA_SpatialMaps, ICA_SpatialMaps]):\n",
    "            spatial_maps = zscore(spatial_maps,axis=-1)\n",
    "            spatial_maps[spatial_maps <= 3.5] = 0 \n",
    "            print(spatial_maps.shape)\n",
    "            groupA_maps = spatial_maps[test_labels == 1]\n",
    "            groupB_maps = spatial_maps[test_labels == 0]\n",
    "            groupA_mean = np.mean(groupA_maps, axis=0)\n",
    "            groupB_mean = np.mean(groupB_maps, axis=0)\n",
    "\n",
    "            within_A = np.mean(np.linalg.norm(groupA_maps - groupA_mean, axis=-1),axis=0)\n",
    "            within_B = np.mean(np.linalg.norm(groupB_maps - groupB_mean, axis=-1),axis=0)\n",
    "            between = np.linalg.norm(groupA_mean - groupB_mean,axis=-1)\n",
    "            print(within_A,within_B,between)\n",
    "            groupA_map_norms = np.linalg.norm(groupA_maps, axis=-1)\n",
    "            plt.hist(groupA_map_norms.flatten(), bins=50, alpha=0.7, label=\"Group A Individual Maps\")\n",
    "            plt.hist(np.linalg.norm(groupA_mean,axis=-1).flatten(), bins=50, alpha=0.7, label=\"Group A Mean Norms\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            if method == \"IFA\":\n",
    "                IFA_results.append(np.array([between, within_A,within_B]))\n",
    "            else:\n",
    "                ICA_results.append(np.array([between, within_A,within_B]))\n",
    "            \n",
    "        return np.array(IFA_euc_map_spread), np.array(ICA_results)\n",
    "    \n",
    "IFA_euc_map_spread, ICA_euc_map_spread = visualize_spatial_variance_per_map(time_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_spatial_stat_maps(method,time_norm=False):\n",
    "    for fold in range(0,1):\n",
    "        print(f\"Processing fold {fold}...\")\n",
    "        # Load spatial maps and test indices\n",
    "        fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "\n",
    "        if time_norm:\n",
    "            results = os.path.join(fold_results_nPCA, f\"Normalized\")\n",
    "        else:\n",
    "            results = os.path.join(fold_results_nPCA, f\"Demeaned\")\n",
    "                \n",
    "        output = os.path.join(results, f\"{method}_SpatialMaps\")\n",
    "        reject = np.load(os.path.join(output, f\"{method}_reject.npy\"))\n",
    "        groupA_mean = np.load(os.path.join(output, f\"{method}_groupA_mean.npy\"))\n",
    "        groupB_mean = np.load(os.path.join(output, f\"{method}_groupB_mean.npy\"))\n",
    "        t_test = np.load(os.path.join(output, f\"{method}_t_values.npy\"))\n",
    "        if method==\"ICA\":\n",
    "            maps = np.load(os.path.join(fold_output_dir,f\"ICA_8/GICA/{method}_zmaps.npy\"))\n",
    "        else:\n",
    "            maps = np.load(os.path.join(fold_output_dir,f\"ICA_8/{method}/{method}_zmaps.npy\"))\n",
    "\n",
    "        for i in range(reject.shape[0]):\n",
    "            # if np.sum(reject[i,:]) < 30:\n",
    "            #     continue\n",
    "            # else:\n",
    "            #     print(f\"Component {i}\",np.sum(reject[i,:]))\n",
    "            #     print(f\"Component {i}\",np.max(np.abs((t_test*reject)[i,:])))\n",
    "            if i==3:\n",
    "                view_combined = view_surf(\n",
    "                    surf_mesh=hcp.mesh.inflated,\n",
    "                    surf_map=hcp.cortex_data((groupA_mean)[i,:]),\n",
    "                    # black_bg=False,\n",
    "                    title=f\"Fold {fold}: Group A {i} Component\"\n",
    "                )\n",
    "                display(view_combined)\n",
    "\n",
    "\n",
    "                view_combined = view_surf(\n",
    "                    surf_mesh=hcp.mesh.inflated,\n",
    "                    surf_map=hcp.cortex_data((groupB_mean)[i,:]),\n",
    "                    # black_bg=False,\n",
    "                    title=f\"Fold {fold}: Group B {i} Component\"\n",
    "                )\n",
    "                display(view_combined)\n",
    "\n",
    "                # print(np.var((groupA_mean)[i,:]),np.var((groupB_mean)[i,:]))\n",
    "\n",
    "\n",
    "                # view_combined = view_surf(\n",
    "                #     surf_mesh=hcp.mesh.inflated,\n",
    "                #     surf_map=hcp.cortex_data((((groupA_mean)[i,:] - (groupB_mean)[i,:]))**2),\n",
    "                #     # black_bg=False,\n",
    "                #     title=f\"Fold {fold}: T_Test (A-B) {i} Component\"\n",
    "                # )\n",
    "\n",
    "                # display(view_combined)\n",
    "\n",
    "                # view_combined = view_surf(\n",
    "                #     surf_mesh=hcp.mesh.inflated,\n",
    "                #     surf_map=hcp.cortex_data(maps[:,i]),\n",
    "                #     # black_bg=False,\n",
    "                #     title=f\"Fold {fold}: Map {i} Component\"\n",
    "                # )\n",
    "                # display(view_combined)\n",
    "\n",
    "vis_spatial_stat_maps(\"IFA\",time_norm=True)\n",
    "# vis_spatial_stat_maps(\"ICA\",time_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_components(components):\n",
    "    \"\"\"\n",
    "    Normalize each component to have zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    print(components.shape)\n",
    "    normalized = (components) / np.linalg.norm(components, axis=0)\n",
    "    return normalized\n",
    "\n",
    "def compute_and_visualize_correlations(components, filters, title):\n",
    "    \"\"\"\n",
    "    Compute the absolute value of the correlation matrix between components and filters,\n",
    "    and visualize it using a heatmap.\n",
    "    \"\"\"\n",
    "    # Normalize components and filters\n",
    "    norm_components = normalize_components(components)\n",
    "    norm_filters = normalize_components(filters)\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    correlation_matrix = np.abs(np.dot(norm_components.T, norm_filters))\n",
    "    \n",
    "    recon = components@(np.linalg.pinv(components)@filters)\n",
    "    residuals = (filters) - (recon)\n",
    "    # Compute variances\n",
    "    var_residuals = np.var(residuals, ddof=1)\n",
    "    var_original = np.var(filters, ddof=1)\n",
    "    # Calculate variance explained\n",
    "    variance_explained = (1 - (var_residuals / var_original))\n",
    "    # Visualize the correlation matrix\n",
    "    sns.heatmap(correlation_matrix, cmap='viridis',vmin=0,vmax=1)\n",
    "    plt.title(title + f\" | Recon % {variance_explained:.3f}\")\n",
    "    plt.xlabel('Filters')\n",
    "    plt.ylabel('Components')\n",
    "    plt.show()\n",
    "\n",
    "    view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(components[:,np.argmax(correlation_matrix[:,0])]),\n",
    "            # black_bg=False,\n",
    "            # threshold=np.percentile(np.abs(hcp.cortex_data(b[9,:])),90),\n",
    "            title=f\"{title} 0th components\"\n",
    "        )\n",
    "    display(view_combined)\n",
    "\n",
    "    view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(components[:,np.argmax(correlation_matrix[:,1])]),\n",
    "            # black_bg=False,\n",
    "            # threshold=np.percentile(np.abs(hcp.cortex_data(b[9,:])),90),\n",
    "            title=f\"{title} 1st components\"\n",
    "        )\n",
    "    display(view_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [(0, 'blue'), (0.5, 'white'), (1, 'red')]\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "from pyriemann.utils.tangentspace import untangent_space, mean_covariance, unupper\n",
    "from pyriemann.estimation import Covariances\n",
    "\n",
    "from tangent import tangent_transform\n",
    "\n",
    "metric =\"logeuclid\"\n",
    "condition = \"WM_2back_v_0back\"\n",
    "fold = 0\n",
    "cov_est = Covariances(estimator='oas')\n",
    "train = np.load(f\"{condition}/fold_{fold}/Indices/train_idx.npy\")\n",
    "test = np.load(f\"{condition}/fold_{fold}/Indices/test_idx.npy\")\n",
    "labels = np.load(f\"/project/3022057.01/{condition}/labels.npy\")\n",
    "filters = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/Filters/haufe_filters_ortho_voxel.npy\")\n",
    "\n",
    "ifa_z_maps = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/ICA_8/IFA/IFA_zmaps.npy\")\n",
    "A_ifa = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/Dual_Regression_8/IFA_Adm.npy\")\n",
    "An_ifa = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/Dual_Regression_8/IFA_An.npy\")\n",
    "A_ifa_train = A_ifa[train]\n",
    "A_ifa_test = A_ifa[test]\n",
    "An_ifa_train = An_ifa[train]\n",
    "An_ifa_test = An_ifa[test]\n",
    "ifa_maps = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/Dual_Regression_8/IFA_spatial_maps.npy\")\n",
    "ifa_maps_train = ifa_maps[train]\n",
    "ifa_maps_test = ifa_maps[test]\n",
    "ifa_train_covs =  cov_est.transform(np.transpose(A_ifa_train, (0, 2, 1)))\n",
    "ifa_test_covs =  cov_est.transform(np.transpose(A_ifa_test, (0, 2, 1)))\n",
    "ifa_train_vecs, ifa_test_vecs, ifa_mean = tangent_transform(ifa_train_covs, ifa_test_covs, metric=metric)\n",
    "\n",
    "ica_z_maps = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/ICA_8/GICA/ICA_zmaps.npy\")\n",
    "A_ica = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/Dual_Regression_8/ICA_Adm.npy\")\n",
    "An_ica = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/Dual_Regression_8/ICA_An.npy\")\n",
    "A_ica_train = A_ica[train]\n",
    "A_ica_test = A_ica[test]\n",
    "An_ica_train = An_ica[train]\n",
    "An_ica_test = An_ica[test]\n",
    "ica_maps = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/Dual_Regression_8/ICA_spatial_maps.npy\")\n",
    "ica_maps_train = ica_maps[train]\n",
    "ica_maps_test = ica_maps[test]\n",
    "ica_train_covs =  cov_est.transform(np.transpose(A_ica_train, (0, 2, 1)))\n",
    "ica_test_covs =  cov_est.transform(np.transpose(A_ica_test, (0, 2, 1)))\n",
    "ica_train_vecs, ica_test_vecs, ica_mean = tangent_transform(ica_train_covs, ica_test_covs, metric=metric)\n",
    "\n",
    "\n",
    "labels_train = labels[train]\n",
    "labels_test = labels[test]\n",
    "\n",
    "groupA = hcp.normalize(np.load(f\"/project/3022057.01/{condition}/fold_{fold}/MIGP/reducedsubsA.npy\"))\n",
    "groupB = hcp.normalize(np.load(f\"/project/3022057.01/{condition}/fold_{fold}/MIGP/reducedsubsB.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(f\"/project/3022057.01/{condition}\", \"paths.pkl\"), \"rb\") as f:\n",
    "    paths = pickle.load(f)    \n",
    "\n",
    "test_paths = paths[test]\n",
    "A_subtest_paths = test_paths[labels_test==1][:3]\n",
    "B_subtest_paths = test_paths[labels_test==0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO  \n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.termination.default import DefaultSingleObjectiveTermination\n",
    "from pymoo.core.problem import StarmapParallelization\n",
    "from pymoo.util.display.column import Column\n",
    "from pymoo.util.display.output import Output\n",
    "\n",
    "from preprocessing import load_subject\n",
    "\n",
    "def evaluate_subject_helper(args):\n",
    "    \"\"\"\n",
    "    A module-level helper function that unpacks arguments and calls\n",
    "    the instance's evaluate_subject method.\n",
    "    \"\"\"\n",
    "    instance, sub_path, hyperparams = args\n",
    "    return instance.evaluate_subject(sub_path, hyperparams)\n",
    "\n",
    "class DualRegressionOptimizer:\n",
    "    def __init__(self, subject_paths, spatial_map, mode=\"normalize\", search_space=None, \n",
    "                 parallel_points=10, parallel_subs=3):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          - subject_paths: list of file paths for subjects.\n",
    "          - spatial_map: spatial map array (e.g., from IFA/ICA), shape [V, C].\n",
    "          - mode: \"normalize\" or \"demean\". Determines how predictors are computed.\n",
    "          - search_space: Optional; either a dict with keys 'alpha' and 'l1_ratio' \n",
    "                          or a list of skopt dimensions.\n",
    "          - parallel_points: Number of parallel evaluations for Bayesian optimization.\n",
    "          - parallel_subs: Number of parallel subject evaluations.\n",
    "        \"\"\"\n",
    "        self.subject_paths = subject_paths\n",
    "        self.spatial_map = spatial_map\n",
    "        self.spatial_map_dm = self.spatial_map - self.spatial_map.mean(axis=0, keepdims=True)\n",
    "        self.spatial_map_dm_plus = np.linalg.pinv(self.spatial_map_dm.T)\n",
    "        self.parallel_points = parallel_points\n",
    "        self.parallel_subs = parallel_subs\n",
    "\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in [\"normalize\", \"demean\"]:\n",
    "            raise ValueError(\"mode must be either 'normalize' or 'demean'\")\n",
    "       \n",
    "        if search_space is None:\n",
    "            self.search_space_dict = {'alpha': (1e-3, 100), 'l1_ratio': (1e-3, 0.5)}\n",
    "        elif isinstance(search_space, dict):\n",
    "            self.search_space_dict = search_space\n",
    "        else:\n",
    "            raise ValueError(\"search_space must be either None or a dict\")\n",
    "\n",
    "        # Build a list of skopt dimensions for Bayesian optimization.\n",
    "        self.search_space_dims = [\n",
    "            Real(self.search_space_dict['alpha'][0], \n",
    "                 self.search_space_dict['alpha'][1], \n",
    "                 prior='log-uniform', \n",
    "                 name='alpha'),\n",
    "            Real(self.search_space_dict['l1_ratio'][0], \n",
    "                 self.search_space_dict['l1_ratio'][1], \n",
    "                 name='l1_ratio')\n",
    "        ]\n",
    "\n",
    "\n",
    "    def evaluate_subject(self, sub_path, hyperparams):\n",
    "        \"\"\"\n",
    "        For a given subject file path, load data, compute the network matrix A,\n",
    "        compute predictors (using the chosen mode), perform a train-test split,\n",
    "        train an ElasticNet model with given hyperparameters, and return the R² score.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            Xn = load_subject(sub_path)\n",
    "            # Demean each time point (row) of Xn.\n",
    "            Xn_demeaned = Xn - Xn.mean(axis=1, keepdims=True)\n",
    "            # Compute the network matrix A.\n",
    "            A = Xn_demeaned @ self.spatial_map_dm_plus\n",
    "            # Compute predictors.\n",
    "            if self.mode == \"normalize\":\n",
    "                predictors = hcp.normalize(A)\n",
    "            else:\n",
    "                predictors = A - A.mean(axis=0, keepdims=True)\n",
    "            # Split data into train and test sets.\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                predictors, Xn, test_size=0.3, random_state=42\n",
    "            )\n",
    "            # Train ElasticNet.\n",
    "            model = ElasticNet(alpha=hyperparams[0], l1_ratio=hyperparams[1],\n",
    "                               fit_intercept=False, max_iter=10000, selection='random')\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {sub_path}: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def objective(self, hyperparams):\n",
    "        \"\"\"\n",
    "        Shared objective function for both Bayesian and PSO optimization.\n",
    "        Receives hyperparams as a list [alpha, l1_ratio] and returns the negative\n",
    "        average R² score (to be minimized).\n",
    "        \"\"\"\n",
    "        # Prepare the arguments for each subject evaluation.\n",
    "        args = [(self, sub, hyperparams) for sub in self.subject_paths]\n",
    "        with ThreadPoolExecutor(max_workers=self.parallel_subs) as executor:\n",
    "            scores = list(executor.map(evaluate_subject_helper, args))\n",
    "        avg_score = np.mean(scores)\n",
    "        return -avg_score  # Negative because we minimize\n",
    "\n",
    "    def optimize_bayesian(self, n_calls=15, random_state=42):\n",
    "        res = gp_minimize(\n",
    "            func=self.objective,\n",
    "            dimensions=self.search_space_dims,\n",
    "            n_calls=n_calls,\n",
    "            random_state=random_state,\n",
    "            n_jobs=self.parallel_points  # Remove if unsupported by your skopt version\n",
    "        )\n",
    "        best_params = res.x\n",
    "        best_cv_score = -res.fun\n",
    "        return best_params, best_cv_score\n",
    "    \n",
    "    class MyOutput(Output):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.best_f = Column(\"best_f\", width=12)\n",
    "            self.best_alpha = Column(\"best_alpha\", width=12)\n",
    "            self.best_l1 = Column(\"best_l1\", width=12)\n",
    "            # Add the new columns to the output columns.\n",
    "            self.columns += [self.best_f, self.best_alpha, self.best_l1]\n",
    "\n",
    "        def update(self, algorithm):\n",
    "            super().update(algorithm)\n",
    "            F = algorithm.pop.get(\"F\")\n",
    "            best_index = np.argmin(F)\n",
    "            best_value = np.min(F)\n",
    "            X = algorithm.pop.get(\"X\")\n",
    "            best_candidate = X[best_index]\n",
    "            self.best_f.set(best_value)\n",
    "            self.best_alpha.set(best_candidate[0])\n",
    "            self.best_l1.set(best_candidate[1])\n",
    "\n",
    "    class AggregatedElasticNetCVProblem(ElementwiseProblem):\n",
    "        def __init__(self, outer, **kwargs):\n",
    "            \"\"\"\n",
    "            Parameters:\n",
    "              - outer: a reference to the outer DualRegressionOptimizer instance.\n",
    "            \"\"\"\n",
    "            self.outer = outer\n",
    "            # Set bounds: convert alpha to log10 space.\n",
    "            xl = [np.log10(self.outer.search_space_dict['alpha'][0]),\n",
    "                  self.outer.search_space_dict['l1_ratio'][0]]\n",
    "            xu = [np.log10(self.outer.search_space_dict['alpha'][1]),\n",
    "                  self.outer.search_space_dict['l1_ratio'][1]]\n",
    "            super().__init__(n_var=2, n_obj=1, n_constr=0, xl=xl, xu=xu, **kwargs)\n",
    "\n",
    "        def _evaluate(self, x, out):\n",
    "            alpha_original = 10 ** x[0]\n",
    "            out[\"F\"] = self.outer.objective([alpha_original, x[1]])\n",
    "\n",
    "    def optimize_pso(self, random_state=42, particles=15,ftol=1e-2, period=1, n_max_gen=5):\n",
    "        # Use a multiprocessing pool for parallel evaluations.\n",
    "        pool = multiprocessing.Pool(processes=particles)\n",
    "        runner = StarmapParallelization(pool.starmap)\n",
    "        problem_instance = self.AggregatedElasticNetCVProblem(outer=self,elementwise_runner=runner)\n",
    "        algorithm = PSO(pop_size=particles)\n",
    "        termination = DefaultSingleObjectiveTermination(ftol=ftol, period=period, n_max_gen=n_max_gen)\n",
    "        res = minimize(problem_instance, algorithm, termination=termination,\n",
    "                       seed=random_state, verbose=True, output=self.MyOutput())\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        best_params = res.X\n",
    "        best_cv_score = -res.F[0]\n",
    "        print(\"PSO optimization:\")\n",
    "        print(\"  Best hyperparameters found:\", best_params)\n",
    "        print(\"  Best CV R² score:\", best_cv_score)\n",
    "        # Note: best_params[0] is in log-space; convert it.\n",
    "        best_params[0] = 10 ** best_params[0]\n",
    "        return best_params, best_cv_score\n",
    "\n",
    "    def optimize(self, optimizer=\"bayesian\", **kwargs):\n",
    "        \"\"\"\n",
    "        Optimize the ElasticNet hyperparameters over all subjects using the selected method.\n",
    "        \n",
    "        Parameters:\n",
    "          - optimizer: \"bayesian\" or \"pso\"\n",
    "          - kwargs: additional parameters passed to the specific optimizer method.\n",
    "        \n",
    "        Returns:\n",
    "          A tuple: (best_params, best_cv_score)\n",
    "        \"\"\"\n",
    "        if optimizer.lower() == \"bayesian\":\n",
    "            return self.optimize_bayesian(**kwargs)\n",
    "        elif optimizer.lower() == \"pso\":\n",
    "            return self.optimize_pso(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"optimizer must be either 'bayesian' or 'pso'\")\n",
    "        \n",
    "\n",
    "# Create an instance of DualRegressionOptimizer.\n",
    "indices = np.random.choice(test_paths.shape[0], size=2, replace=False)\n",
    "sampled_subjects = test_paths[indices]\n",
    "optimizer_instance = DualRegressionOptimizer(sampled_subjects, ifa_z_maps, mode=\"normalize\",parallel_points=10, parallel_subs=2)\n",
    "\n",
    "# Choose an optimization method: \"bayesian\" or \"pso\"\n",
    "# method = \"pso\"  # or \"bayesian\"\n",
    "# best_params, best_cv_score = optimizer_instance.optimize(\n",
    "#     optimizer=method, random_state=42, particles=100,ftol=1e-2, period=1, n_max_gen=5\n",
    "# )\n",
    "\n",
    "method = \"bayesian\"  # or \"bayesian\"\n",
    "best_params, best_cv_score = optimizer_instance.optimize(\n",
    "    optimizer=method, n_calls=15, random_state=42)\n",
    "\n",
    "print(\"\\nFinal Aggregated Optimization Results:\")\n",
    "print(f\"  Optimal parameters: alpha = {best_params[0]:.4g}, l1_ratio = {best_params[1]:.4g}\")\n",
    "print(f\"  Best CV R² score: {best_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ICA import threshold_and_visualize\n",
    "# reducedsubs = hcp.normalize(np.concatenate((groupA, groupB), axis=0))\n",
    "# A = reducedsubs@np.linalg.pinv(filters.T)\n",
    "# W = np.linalg.pinv(A)\n",
    "# if not os.path.exists(condition+\"TEST\"):\n",
    "#     os.makedirs(condition+\"TEST\")\n",
    "\n",
    "# filter_zmaps, _ = threshold_and_visualize(reducedsubs.copy(), W, filters, visualize=True,output_dir=condition+\"TEST\")\n",
    "# print(filter_zmaps.shape)\n",
    "# ((train_IFA_An_subs, train_IFA_spatial_maps_subs, train_IFA_reconstruction_errors),\n",
    "# (train_IFA_Adm_subs, train_IFA_spatial_mapdm_subs)), \\\n",
    "# ((train_ICA_An_subs, train_ICA_spatial_maps_subs, train_ICA_reconstruction_errors),\n",
    "# (train_ICA_Adm_subs, train_ICA_spatial_mapdm_subs)) = dual_regress(np.vstack((A_subtrain_paths,B_subtrain_paths)), ifa_z_maps, ica_z_maps)\n",
    "\n",
    "# ((IFA_An_subs, IFA_spatial_maps_subs, IFA_reconstruction_errors),\n",
    "# (IFA_Adm_subs, IFA_spatial_mapdm_subs)), \\\n",
    "# ((ICA_An_subs, ICA_spatial_maps_subs, ICA_reconstruction_errors),\n",
    "# (ICA_Adm_subs, ICA_spatial_mapdm_subs)) =  dual_regress_test(np.vstack((A_subtest_paths,B_subtest_paths)), ifa_z_maps, ica_z_maps)\n",
    "((IFA_An_subs, IFA_spatial_maps_subs, IFA_reconstruction_errors),\n",
    "(IFA_Adm_subs, IFA_spatial_mapdm_subs)), \\\n",
    "((ICA_An_subs, ICA_spatial_maps_subs, ICA_reconstruction_errors),\n",
    "(ICA_Adm_subs, ICA_spatial_mapdm_subs)) =  dual_regress_test(test_paths[:3], ifa_z_maps, ica_z_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFA_spatial_maps_subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(view_surf(\n",
    "    surf_mesh=hcp.mesh.inflated,\n",
    "    surf_map=hcp.cortex_data(IFA_spatial_maps_subs[0,0,:]),\n",
    "    bg_map=hcp.mesh.sulc,\n",
    "    # title=f\"~IFA & ICA: {np.sum(~(np.sum((IFA_p_vals < alpha), axis=0) > 0) & (np.sum((ICA_p_vals < alpha), axis=0) > 0))}\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import hcp_utils as hcp \n",
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "import pickle\n",
    "\n",
    "with open(\"/project/3022057.01/WM_2back_v_0back/fold_0/Results/Results_8/Demeaned/IFA_results_demeaned.pkl\", \"rb\") as f:\n",
    "    IFA_results = pickle.load(f)  \n",
    "\n",
    "labels = np.load(os.path.join(\"/project/3022057.01/WM_2back_v_0back/\",\"labels.npy\"))\n",
    "test_idx = np.load(os.path.join(\"/project/3022057.01/WM_2back_v_0back/fold_0/Indices\", \"test_idx.npy\"))\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "IFA_acc = []\n",
    "for i in range(10):\n",
    "    IFA_acc.append(IFA_results['IFA_Spatial'][0][i][\"Logistic Regression\"][\"accuracy\"])\n",
    "IFA_acc = np.array(IFA_acc)\n",
    "\n",
    "# IFA_acc = IFA_acc[np.argsort(IFA_acc)[::-1]]\n",
    "IFA_ind = np.argsort(IFA_acc)[::-1]\n",
    "IFA_A_2 = np.load(\"/project/3022057.01/WM_2back_v_0back/fold_0/Dual_Regression_8/IFA_spatial_mapdm.npy\")[test_idx][test_labels==1][:50]\n",
    "IFA_B_2 = np.load(\"/project/3022057.01/WM_2back_v_0back/fold_0/Dual_Regression_8/IFA_spatial_mapdm.npy\")[test_idx][test_labels==0][:50]\n",
    "IFA_A = IFA_spatial_mapdm_subs[:50]\n",
    "IFA_B = IFA_spatial_mapdm_subs[50:]\n",
    "\n",
    "for ind in IFA_ind[0:1]:\n",
    "    print(IFA_acc[ind])\n",
    "    # For the i-th row in both datasets\n",
    "    data_A = IFA_A[:,ind, :]\n",
    "    data_B = IFA_B[:,ind, :]\n",
    "\n",
    "    data_A_2 = IFA_A_2[:,ind, :]\n",
    "    data_B_2 = IFA_B_2[:,ind, :]\n",
    "\n",
    "    t_values, p_values = ttest_rel(data_A, data_B, axis=0)\n",
    "    abs_max = np.max((np.max(np.abs(data_A)),np.max(np.abs(data_A))))\n",
    "    view_filter = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(t_values),\n",
    "        bg_map=hcp.mesh.sulc,\n",
    "        title=\"T-Test\",\n",
    "        # vmax= abs_max,\n",
    "    )\n",
    "    display(view_filter)\n",
    "    # os.mkdir(\"TEST\")\n",
    "    # view_filter.save_as_html(os.path.join(\"TEST\", f\"t_test.html\"))\n",
    "    A_mean = np.mean(data_A,axis=0)\n",
    "    B_mean = np.mean(data_B,axis=0)\n",
    "    A_mean_2 = np.mean(data_A_2,axis=0)\n",
    "    B_mean_2 = np.mean(data_B_2,axis=0)\n",
    "\n",
    "    abs_max = np.max((np.abs(A_mean),np.abs(B_mean)))\n",
    "    view_filter = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(A_mean),\n",
    "        bg_map=hcp.mesh.sulc,\n",
    "        title=\"Group A Mean\",\n",
    "    )\n",
    "    display(view_filter)\n",
    "\n",
    "    view_filter = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(A_mean_2),\n",
    "        bg_map=hcp.mesh.sulc,\n",
    "        title=\"Group A Mean\",\n",
    "    )\n",
    "    display(view_filter)\n",
    "    # view_filter.save_as_html(os.path.join(\"TEST\", f\"A.html\"))\n",
    "\n",
    "    view_filter = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(B_mean),\n",
    "        bg_map=hcp.mesh.sulc,\n",
    "        title=\"Group B Mean\",\n",
    "    )\n",
    "    display(view_filter)\n",
    "\n",
    "    view_filter = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(B_mean_2),\n",
    "        bg_map=hcp.mesh.sulc,\n",
    "        title=\"Group B Mean\",\n",
    "    )\n",
    "    display(view_filter)\n",
    "    # view_filter.save_as_html(os.path.join(\"TEST\", f\"B.html\"))\n",
    "\n",
    "    # # Visualization for IFA_B\n",
    "    # view_filter = view_surf(\n",
    "    #     surf_mesh=hcp.mesh.inflated,\n",
    "    #     surf_map=hcp.cortex_data(data_B),\n",
    "    #     bg_map=hcp.mesh.sulc,\n",
    "    #     vmax= abs_max,\n",
    "    # )\n",
    "    # display(view_filter)\n",
    "    # time.sleep(30)\n",
    "    # clear_output(wait=True)  # Clear the previous output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(filters[:,0]),\n",
    "            # black_bg=False,\n",
    "            # threshold=np.percentile(np.abs(hcp.cortex_data(b[9,:])),90),\n",
    "            title=f\"Filter 0\"\n",
    "        )\n",
    "display(view_combined)\n",
    "\n",
    "view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(filters[:,1]),\n",
    "            # black_bg=False,\n",
    "            # threshold=np.percentile(np.abs(hcp.cortex_data(b[9,:])),90),\n",
    "            title=f\"Filter 1\"\n",
    "        )\n",
    "display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_diff(A,B,F):\n",
    "    dual_r = lambda x: np.linalg.pinv(hcp.normalize(x@np.linalg.pinv(F.T)))@x\n",
    "    A_dual = dual_r(A)\n",
    "    B_dual = dual_r(B)\n",
    "\n",
    "    for i in range(F.shape[1]):\n",
    "        view_combined = view_surf(\n",
    "                    surf_mesh=hcp.mesh.inflated,\n",
    "                    surf_map=hcp.cortex_data((A_dual[i,:] - B_dual[i,:])**2),\n",
    "                    # black_bg=False,\n",
    "                    # threshold=np.percentile(np.abs(hcp.cortex_data(b[9,:])),90),\n",
    "                    title=f\"Filter {i}\"\n",
    "                )\n",
    "        display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_diff(groupA,groupB,filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "a_ICA = FastICA(n_components=10,\n",
    "        random_state=0,\n",
    "        whiten='unit-variance')\n",
    "a_ICA.fit(groupA)\n",
    "a_ICA.components_.shape\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "b_ICA = FastICA(n_components=10,\n",
    "        random_state=0,\n",
    "        whiten='unit-variance')\n",
    "b_ICA.fit(groupB)\n",
    "b_ICA.components_.shape\n",
    "\n",
    "compute_and_visualize_correlations(a_ICA.components_.T, filters, \"A Maps Abs. Corr. w/ Filters\")\n",
    "compute_and_visualize_correlations(b_ICA.components_.T, filters, \"B Maps Abs. Corr. w/ Filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_visualize_correlations(ifa_z_maps, filters, \"IFA Maps Abs. Corr. w/ Filters\")\n",
    "compute_and_visualize_correlations(ica_z_maps, filters, \"ICA Maps Abs. Corr. w/ Filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_diff(groupA,groupB,ifa_z_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ids = np.load(f\"/project/3022057.01/{condition}/Sub_ID.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub_ids[test][labels_test==1][:100])\n",
    "print(sub_ids[test][labels_test==0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(f\"/project/3022057.01/{condition}\", \"paths.pkl\"), \"rb\") as f:\n",
    "    paths = pickle.load(f)    \n",
    "\n",
    "train_paths = paths[train]\n",
    "A_subtrain_paths = train_paths[labels_train==1][:100]\n",
    "B_subtrain_paths = train_paths[labels_train==0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A_subtest_paths[0])\n",
    "print(B_subtest_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFA_basis = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/ICA_8/IFA/IFA_basis.npy\")\n",
    "from filters import whiten\n",
    "whitened_IFA_basis, _ = whiten(IFA_basis, n_components=IFA_basis.shape[0], method=\"InvCov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitened_IFA_basis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((IFA_basis@IFA_basis.T)/91282)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(whitened_IFA_basis[-2:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import permutation_cluster_test, ttest_1samp_no_p, ttest_ind_no_p, permutation_cluster_1samp_test\n",
    "\n",
    "def spatial_t_test_with_tfce(time_norm=True, n_permutations=1, random_seed=42):\n",
    "    for fold in range(1):\n",
    "        print(f\"Processing fold {fold}...\")\n",
    "        # Load spatial maps and test indices\n",
    "        fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "        dual_dir = os.path.join(fold_output_dir, f\"Dual_Regression_{nPCA}\") #TODO Delete\n",
    "\n",
    "        indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "        train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "        test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "        labels = np.load(os.path.join(outputfolder,\"labels.npy\"))\n",
    "        test_labels = labels[test_idx]\n",
    "\n",
    "        if time_norm:\n",
    "            IFA_SpatialMaps = np.load(os.path.join(dual_dir, \"IFA_spatial_maps.npy\"))[test_idx]\n",
    "            # ICA_SpatialMaps = np.load(os.path.join(dual_dir, \"ICA_spatial_maps.npy\"))[test_idx]\n",
    "        else:\n",
    "            IFA_SpatialMaps = np.load(os.path.join(dual_dir, \"IFA_spatial_mapdm.npy\"))[test_idx]\n",
    "            # ICA_SpatialMaps = np.load(os.path.join(dual_dir, \"ICA_spatial_mapdm.npy\"))[test_idx]\n",
    "\n",
    "        groupA = IFA_SpatialMaps[test_labels==1]\n",
    "        groupB = IFA_SpatialMaps[test_labels==0]\n",
    "\n",
    "        groupA_cortex = np.array([maps[:,hcp.struct.cortex] for maps in groupA])\n",
    "        groupB_cortex = np.array([maps[:,hcp.struct.cortex] for maps in groupB])\n",
    "\n",
    "        # # Perform cluster-based permutation test with TFCE\n",
    "        T_obs, clusters, cluster_p_values, H0 = permutation_cluster_1samp_test(\n",
    "            groupA_cortex - groupB_cortex,\n",
    "            threshold={\"start\": 0, \"step\": .4},  # TFCE parameters\n",
    "            n_permutations=n_permutations,\n",
    "            tail=0,                              # Two-tailed test\n",
    "            adjacency=hcp.cortical_adjacency,    # Adjacency for HCP grayordinates\n",
    "            # n_jobs=int(os.cpu_count() * 0.8),\n",
    "            n_jobs=3,\n",
    "            out_type=\"mask\",\n",
    "            seed=random_seed  \n",
    "        )\n",
    "\n",
    "        # X = [groupA_cortex, groupB_cortex]  # Combine groups for cluster-based permutation\n",
    "        # Perform cluster-based permutation test with TFCE\n",
    "        # T_obs, clusters, cluster_p_values, H0 = permutation_cluster_test(\n",
    "        #     X,\n",
    "        #     threshold={\"start\": 0, \"step\": .5},  # TFCE parameters\n",
    "        #     n_permutations=n_permutations,\n",
    "        #     tail=0,                              # Two-tailed test\n",
    "        #     stat_fun=ttest_ind_no_p,\n",
    "        #     adjacency=hcp.cortical_adjacency,    # Adjacency for HCP grayordinates\n",
    "        #     # n_jobs=int(os.cpu_count() * 0.2),\n",
    "        #     out_type=\"mask\",\n",
    "        #     seed=random_seed  # Reproducibility\n",
    "        # )\n",
    "\n",
    "        return T_obs, clusters, cluster_p_values, H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_obs, clusters, cluster_p_values, H0 = spatial_t_test_with_tfce(time_norm=True, n_permutations=1000, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(os.cpu_count() * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(cluster_p_values <= .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cluster_p_values.reshape(10,-1).shape[0]):\n",
    "    view_combined = view_surf(\n",
    "                surf_mesh=hcp.mesh.inflated,\n",
    "                surf_map=hcp.cortex_data((cluster_p_values.reshape(10,-1)[i,:])) <= 0.2,\n",
    "                # black_bg=False,\n",
    "                # threshold=np.percentile(np.abs(hcp.cortex_data((T_obs[i,:]))),80),\n",
    "                title=f\"Filter {i}\"\n",
    "            )\n",
    "    display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean_IFA = np.mean(IFA_spatial_maps_subs[:100],axis=0)\n",
    "B_mean_IFA = np.mean(IFA_spatial_maps_subs[100:],axis=0)\n",
    "# A_mean_IFA_train = np.mean(train_IFA_spatial_maps_subs[:100],axis=0)\n",
    "# B_mean_IFA_train = np.mean(train_IFA_spatial_maps_subs[100:],axis=0)\n",
    "\n",
    "A_mean_ICA = np.mean(ICA_spatial_maps_subs[:100],axis=0)\n",
    "B_mean_ICA = np.mean(ICA_spatial_maps_subs[100:],axis=0)\n",
    "# A_mean_ICA_train = np.mean(train_ICA_spatial_maps_subs[:100],axis=0)\n",
    "# B_mean_ICA_train = np.mean(train_ICA_spatial_maps_subs[100:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(A_mean_IFA - B_mean_IFA,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(A_mean_ICA - B_mean_ICA,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(B_mean_IFA.shape[0]):\n",
    "    view_combined = view_surf(\n",
    "                surf_mesh=hcp.mesh.inflated,\n",
    "                surf_map=hcp.cortex_data((A_mean_IFA[i,:] - B_mean_IFA[i,:])**2),\n",
    "                # black_bg=False,\n",
    "                # threshold=np.percentile(np.abs(hcp.cortex_data(b[9,:])),90),\n",
    "                title=f\"Filter {i}\"\n",
    "            )\n",
    "    display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(B_mean_ICA.shape[0]):\n",
    "    view_combined = view_surf(\n",
    "                surf_mesh=hcp.mesh.inflated,\n",
    "                surf_map=hcp.cortex_data((A_mean_ICA[i,:] - B_mean_ICA[i,:])**2),\n",
    "                # black_bg=False,\n",
    "                # threshold=np.percentile(np.abs(hcp.cortex_data(b[9,:])),90),\n",
    "                title=f\"Filter {i}\"\n",
    "            )\n",
    "    display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Example input\n",
    "# recon_filter_maps: subjects x components x voxels\n",
    "# labels_test: labels for group membership (binary: 0/1)\n",
    "\n",
    "def voxel_wise_paired_t_test(recon_filter_maps, labels, alpha=0.05, correction='fdr_bh'):\n",
    "    \"\"\"\n",
    "    Perform voxel-wise paired two-sample t-tests with multiple comparison correction.\n",
    "\n",
    "    Parameters:\n",
    "    recon_filter_maps (array): Data array of shape (subjects, components, voxels).\n",
    "    labels (array): Binary labels indicating group membership (aligned).\n",
    "    alpha (float): Significance level for multiple comparison correction.\n",
    "    correction (str): Method for multiple comparison correction ('fdr_bh' by default).\n",
    "\n",
    "    Returns:\n",
    "    t_values (array): T-values of shape (components, voxels).\n",
    "    p_values (array): Uncorrected p-values of shape (components, voxels).\n",
    "    corrected_p_values (array): Corrected p-values of shape (components, voxels).\n",
    "    significant (array): Boolean mask of significant results after correction.\n",
    "    \"\"\"\n",
    "    # Separate groups\n",
    "    group1 = recon_filter_maps[labels == 1]  # e.g., post-treatment\n",
    "    group0 = recon_filter_maps[labels == 0]  # e.g., pre-treatment\n",
    "\n",
    "    # Ensure shapes match\n",
    "    if group1.shape[0] != group0.shape[0]:\n",
    "        raise ValueError(\"Mismatch in the number of subjects between groups.\")\n",
    "    \n",
    "    # Initialize arrays\n",
    "    n_components, n_voxels = recon_filter_maps.shape[1:]\n",
    "    t_values = np.zeros((n_components, n_voxels))\n",
    "    p_values = np.zeros((n_components, n_voxels))\n",
    "    corrected_p_values = np.zeros((n_components, n_voxels))\n",
    "    significant = np.zeros((n_components, n_voxels), dtype=bool)\n",
    "\n",
    "    # Perform voxel-wise paired t-tests\n",
    "    for comp in range(n_components):\n",
    "        t_values[comp, :], p_values[comp, :] = ttest_rel(group1[:, comp, :], group0[:, comp, :], axis=0)\n",
    "\n",
    "        # Correct p-values for multiple comparisons\n",
    "        reject, corrected_p_values[comp, :], _, _ = multipletests(p_values[comp, :], alpha=alpha, method=correction)\n",
    "        significant[comp, :] = reject  # Boolean mask of significant results\n",
    "\n",
    "    return t_values, p_values, corrected_p_values, significant\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming recon_filter_maps has shape (subjects, components, voxels)\n",
    "# And labels_test is a binary array of size (subjects,)\n",
    "t_values, p_values, corrected_p_values, significant = voxel_wise_paired_t_test(\n",
    "    recon_filter_maps=IFA_spatial_maps_subs,\n",
    "    labels=np.hstack((labels_test[labels_test == 1][:100], labels_test[labels_test == 0][:100])),\n",
    "    alpha=0.05,\n",
    "    correction='fdr_bh'  # FDR correction\n",
    ")\n",
    "\n",
    "# Output results\n",
    "print(f\"Significant voxels (per component): {np.sum(significant, axis=1)}\")\n",
    "print(f\"Significant voxels (per component): {np.sum(significant)}\")\n",
    "\n",
    "ICA_t_values, ICA_p_values, ICA_corrected_p_values, ICA_significant = voxel_wise_paired_t_test(\n",
    "    recon_filter_maps=ICA_spatial_maps_subs,\n",
    "    labels=np.hstack((labels_test[labels_test == 1][:100], labels_test[labels_test == 0][:100])),\n",
    "    alpha=0.05,\n",
    "    correction='fdr_bh'  # FDR correction\n",
    ")\n",
    "\n",
    "# Output results\n",
    "print(f\"Significant voxels (per component): {np.sum(ICA_significant, axis=1)}\")\n",
    "print(f\"Significant voxels (per component): {np.sum(ICA_significant)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import spatial_t_test\n",
    "\n",
    "spatial_t_test(IFA_spatial_maps_subs, np.hstack((labels_test[labels_test==1][:100],labels_test[labels_test==0][:100])), alpha=0.05, permutations=False, correction=\"fdr_bh\", output_dir=condition+\"TEST\", basis=\"IFA\",corr_across_maps=False)\n",
    "t_values = np.load(\"/project/3022057.01/WM_2back_v_0backTEST/IFA_t_values.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFA_basis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(A_mean_IFA_train[i]),\n",
    "            # black_bg=False,\n",
    "            threshold=np.percentile(np.abs(hcp.cortex_data(A_mean_IFA_train[i])),0),\n",
    "            title=f\"Filter {i}\"\n",
    "        )\n",
    "display(view_combined)\n",
    "\n",
    "view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(B_mean_IFA_train[i]),\n",
    "            # black_bg=False,\n",
    "            threshold=np.percentile(np.abs(hcp.cortex_data(B_mean_IFA_train[i])),0),\n",
    "            title=f\"Filter {i}\"\n",
    "        )\n",
    "display(view_combined)\n",
    "\n",
    "view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(ifa_z_maps.T[i]),\n",
    "            # black_bg=False,\n",
    "            threshold=np.percentile(np.abs(hcp.cortex_data(ifa_z_maps.T[i])),0),\n",
    "            title=f\"Filter {i}\"\n",
    "        )\n",
    "display(view_combined)\n",
    "\n",
    "view_combined = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(t_values[i]),\n",
    "            # black_bg=False,\n",
    "            threshold=np.percentile(np.abs(hcp.cortex_data(t_values[i])),0),\n",
    "            title=f\"Filter {i}\"\n",
    "        )\n",
    "display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(corrected_p_values.shape[0]):\n",
    "    if np.sum(corrected_p_values[i] < 0.05) > 100:\n",
    "        view_combined = view_surf(\n",
    "                    surf_mesh=hcp.mesh.inflated,\n",
    "                    surf_map=hcp.cortex_data(t_values[i]),\n",
    "                    # black_bg=False,\n",
    "                    threshold=np.percentile(np.abs(hcp.cortex_data(t_values[i])),0),\n",
    "                    title=f\"Filter {i}\"\n",
    "                )\n",
    "        display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ICA_corrected_p_values.shape[0]):\n",
    "    if np.sum(ICA_corrected_p_values[i] < 0.05) > 10:\n",
    "        view_combined = view_surf(\n",
    "                    surf_mesh=hcp.mesh.inflated,\n",
    "                    surf_map=hcp.cortex_data(ICA_t_values[i]),\n",
    "                    # black_bg=False,\n",
    "                    threshold=np.percentile(np.abs(hcp.cortex_data(ICA_t_values[i])),0),\n",
    "                    title=f\"Filter {i}\"\n",
    "                )\n",
    "        display(view_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_comp(comp, maps_test, A_test, An_test,per=90):\n",
    "    \"\"\"\n",
    "    Visualize and analyze spatial maps, time series, and reconstructions for each component.\n",
    "\n",
    "    Parameters:\n",
    "    - comp: Component index to analyze.\n",
    "    - maps_test: Spatial maps from dual regression.\n",
    "    - A_test: Unmixed time-series matrices.\n",
    "    - An_test: Normalized unmixed time-series matrices.\n",
    "    - labels_test: Binary labels for the groups (e.g., 0 or 1).\n",
    "    - mesh: HCP inflated mesh for visualization.\n",
    "    \"\"\"\n",
    "    # Compute means\n",
    "    A_mean_a = np.mean(A_test[labels_test == 1], axis=0)[:, comp]\n",
    "    spatial_mean_a = np.mean(maps_test[labels_test == 1], axis=0)[comp, :]\n",
    "    recon_a = np.mean(An_test[labels_test == 1] @ maps_test[labels_test == 1], axis=0)\n",
    "\n",
    "    A_mean_b = np.mean(A_test[labels_test == 0], axis=0)[:, comp]\n",
    "    spatial_mean_b = np.mean(maps_test[labels_test == 0], axis=0)[comp, :]\n",
    "    recon_b = np.mean(An_test[labels_test == 0] @ maps_test[labels_test == 0], axis=0)\n",
    "\n",
    "    # Visualize spatial maps for A and B\n",
    "    view_combined_a = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(spatial_mean_a),\n",
    "        title=f\"Group A: Spatial Map for Component {comp}\"\n",
    "    )\n",
    "    display(view_combined_a)\n",
    "\n",
    "    view_combined_b = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(spatial_mean_b),\n",
    "        title=f\"Group B: Spatial Map for Component {comp}\"\n",
    "    )\n",
    "    display(view_combined_b)\n",
    "\n",
    "    view_combined_b = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data((spatial_mean_a - spatial_mean_b)**2),\n",
    "        title=f\"Diff {comp}\"\n",
    "    )\n",
    "    display(view_combined_b)\n",
    "\n",
    "    # Plot time-series for A and B\n",
    "    plt.plot(A_mean_a, label=f\"A Time Series (Comp {comp}) | var {np.var(A_mean_a):.4f}\", color=\"blue\")\n",
    "    plt.plot(A_mean_b, label=f\"B Time Series (Comp {comp})| var {np.var(A_mean_b):.4f}\", color=\"orange\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Time Series for Component {comp}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Identify significant regions (positive/negative)\n",
    "    mean_map = np.mean(maps_test, axis=0)[comp, :]\n",
    "    threshold = np.percentile(np.abs(mean_map), per)  # Top 10% regions\n",
    "    positive_indices = np.where(mean_map > threshold)[0]\n",
    "    negative_indices = np.where(mean_map < -threshold)[0]\n",
    "\n",
    "\n",
    "    indices = np.zeros_like(mean_map)\n",
    "    indices[negative_indices] = -1\n",
    "    indices[positive_indices] = 1\n",
    "\n",
    "    view_regions = view_surf(\n",
    "        surf_mesh=hcp.mesh.inflated,\n",
    "        surf_map=hcp.cortex_data(indices),\n",
    "        cmap='coolwarm',\n",
    "        title=f\"Significant Regions for Component {comp}\"\n",
    "    )\n",
    "    display(view_regions)\n",
    "\n",
    "    # Plot reconstructions for positive and negative regions\n",
    "    course_recon_a = -np.mean(recon_a[:, negative_indices], axis=-1) + np.mean(recon_a[:, positive_indices], axis=-1)\n",
    "    course_recon_b = -np.mean(recon_b[:, negative_indices], axis=-1) + np.mean(recon_b[:, positive_indices], axis=-1)\n",
    "\n",
    "    plt.plot(\n",
    "        course_recon_a,\n",
    "        label=f\"Group A | var {np.var(course_recon_a):.4f}\",\n",
    "        color=\"blue\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        course_recon_b,\n",
    "        label=f\"Group B | var {np.var(course_recon_b):.4f}\",\n",
    "        color=\"orange\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.title(f\"Reconstruction Differences: Negative vs Positive Regions (Comp {comp})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot positive and negative reconstructions separately\n",
    "    plt.plot(\n",
    "        np.mean(recon_a[:, positive_indices], axis=-1),\n",
    "        label=f\"Group A: Positive Regions | var {np.var(np.mean(recon_a[:, positive_indices], axis=-1)):.4f}\",\n",
    "        color=\"red\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.mean(recon_a[:, negative_indices], axis=-1),\n",
    "        label=f\"Group A: Negative Regions | var {np.var(np.mean(recon_a[:, negative_indices], axis=-1)):.4f}\",\n",
    "        color=\"blue\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.title(f\"Group A: Positive vs Negative Regions (Comp {comp})\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(\n",
    "        np.mean(recon_b[:, positive_indices], axis=-1),\n",
    "        label=f\"Group B: Positive Regions | var {np.var(np.mean(recon_b[:, positive_indices], axis=-1)):.4f}\",\n",
    "        color=\"red\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.mean(recon_b[:, negative_indices], axis=-1),\n",
    "        label=f\"Group B: Negative Regions | var {np.var(np.mean(recon_b[:, negative_indices], axis=-1)):.4f}\",\n",
    "        color=\"blue\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.title(f\"Group B: Positive vs Negative Regions (Comp {comp})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_comp(0,ifa_maps_test,A_ifa_test,An_ifa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "from scipy.linalg import subspace_angles, eigh\n",
    "from sklearn.covariance import OAS, LedoitWolf, EmpiricalCovariance\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "from pyriemann.estimation import Covariances\n",
    "from filters import TSSF\n",
    "import torch\n",
    "\n",
    "############################## Under Development ####################################\n",
    "# Sparse Bilinear Logistic Regression https://ww3.math.ucla.edu/camreport/cam14-12.pdf\n",
    "class SparseBilinearLogisticRegression:\n",
    "    def __init__(self, s, t, r, mu1=0.1, mu2=1.0, nu1=0.1, nu2=1.0, max_iter=100, tol=1e-4):\n",
    "        self.s = s  # Feature matrix rows\n",
    "        self.t = t  # Feature matrix columns\n",
    "        self.r = r  # Low-rank factorization dimension\n",
    "        self.mu1 = mu1  # L1 regularization on U\n",
    "        self.mu2 = mu2  # L2 regularization on U\n",
    "        self.nu1 = nu1  # L1 regularization on V\n",
    "        self.nu2 = nu2  # L2 regularization on V\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol  # Convergence tolerance\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.U = np.random.randn(s, r)\n",
    "        self.V = np.random.randn(t, r)\n",
    "        self.b = 0.0  # Bias term\n",
    "    \n",
    "    # Expit as defined by 12(a-c)\n",
    "    def expit(self, X, y):\n",
    "        trace = np.array([np.trace(self.U.T @ X_i @ self.V) for X_i in X])\n",
    "        return  np.power((1 + np.exp(y*(trace + self.b))),-1) \n",
    "\n",
    "    # 15a\n",
    "    def gradient_U(self, X, y):\n",
    "        all_subs_grad_U = (self.expit(X, y) * y)[:, np.newaxis, np.newaxis] * (X@self.V)\n",
    "        grad_U = -np.mean(all_subs_grad_U,axis=0)\n",
    "        return grad_U\n",
    "    \n",
    "    # 15b\n",
    "    def gradient_V(self, X, y):\n",
    "        all_subs_grad_V = (self.expit(X, y) * y)[:, np.newaxis, np.newaxis] * (np.transpose(X,(0,2,1))@self.U)\n",
    "        grad_V = -np.mean(all_subs_grad_V,axis=0)\n",
    "        return grad_V\n",
    "    \n",
    "    # 15c\n",
    "    def gradient_b(self, X, y):\n",
    "        all_subs_grad_b = (self.expit(X, y) * y)\n",
    "        grad_b = -np.mean(all_subs_grad_b,axis=0)\n",
    "        return grad_b\n",
    "\n",
    "    def soft_thresholding(self, Z, tau):\n",
    "        # https://eeweb.poly.edu/iselesni/lecture_notes/sparse_penalties/sparse_penalties.pdf Page 5\n",
    "        return np.sign(Z) * np.maximum(np.abs(Z) - tau, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n = y.shape[0]\n",
    "        U,E,Vh = np.linalg.svd(np.mean(X,axis=0),full_matrices=False)\n",
    "        self.U = U[:,:self.r]\n",
    "        self.V = Vh[:self.r,:].T\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            U_old = self.U\n",
    "            V_old = self.V\n",
    "            b_old = self.b\n",
    "            # Lipshitz Constant for U \n",
    "            L_u = (np.sqrt(2)/n)*(np.sum((np.linalg.norm(X@self.V,ord='fro',axis=(1,2)) + 1)**2,axis=0))\n",
    "            # Update bhat\n",
    "            self.b = self.b - (1/L_u)*self.gradient_b(X, y)\n",
    "\n",
    "            # Update U\n",
    "            grad_U = self.gradient_U(X, y)\n",
    "            self.U = self.soft_thresholding((L_u*self.U - grad_U)/(L_u+ self.mu2), self.mu1/(L_u+ self.mu2))\n",
    "            \n",
    "            # Lipshitz Constant for v \n",
    "            L_v = (np.sqrt(2)/n)*(np.sum((np.linalg.norm(np.transpose(X,(0,2,1))@self.U,ord='fro',axis=(1,2)) + 1)**2,axis=0))\n",
    "\n",
    "            # Update b\n",
    "            self.b = self.b - (1/L_v)*self.gradient_b(X, y)\n",
    "            \n",
    "            # Update V\n",
    "            grad_V = self.gradient_V(X, y)\n",
    "            self.V = self.soft_thresholding((L_v*self.V - grad_V)/(L_v+ self.nu2), self.nu1 /(L_v+ self.nu2))\n",
    "            \n",
    "            print(np.linalg.norm(self.U - U_old, 'fro') + np.linalg.norm(self.V - V_old, 'fro') + abs(self.b - b_old))\n",
    "\n",
    "            # Convergence check\n",
    "            if np.linalg.norm(self.U - U_old, 'fro') + np.linalg.norm(self.V - V_old, 'fro') + abs(self.b - b_old) < self.tol:\n",
    "                break\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logits = np.array([np.trace(self.U.T @ X_i @ self.V) for X_i in X]) + self.b\n",
    "        return np.sign(logits)\n",
    "\n",
    "def bilinear_class(method, time_norm=False):\n",
    "    for fold in range(1):\n",
    "        # Load indices and labels\n",
    "        indices_dir = os.path.join(outputfolder, f\"fold_{fold}\", \"Indices\")\n",
    "        train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "        test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "        labels = np.load(os.path.join(outputfolder, \"labels.npy\"))\n",
    "        labels[labels == 0] = -1\n",
    "        train_labels = labels[train_idx]\n",
    "        test_labels = labels[test_idx]\n",
    "\n",
    "        # Load spatial maps\n",
    "        dual_dir = os.path.join(outputfolder, f\"fold_{fold}\", f\"Dual_Regression_{nPCA}\")\n",
    "        if time_norm:\n",
    "            SpatialMaps = np.load(os.path.join(dual_dir, f\"{method}_spatial_maps.npy\"))\n",
    "        else:\n",
    "            SpatialMaps = np.load(os.path.join(dual_dir, f\"{method}_spatial_mapdm.npy\"))\n",
    "\n",
    "        train_maps = SpatialMaps[train_idx]\n",
    "        test_maps = SpatialMaps[test_idx]\n",
    "        print(f\"Training Shape: {train_maps.shape}, Testing Shape: {test_maps.shape}\")\n",
    "\n",
    "        # Train Sparse Bilinear Logistic Regression\n",
    "        sblr = SparseBilinearLogisticRegression(\n",
    "            s=train_maps.shape[1], \n",
    "            t=train_maps.shape[2], \n",
    "            r=1,  # Assuming a reduced rank of 10\n",
    "            mu1=.1, mu2=.1, nu1=.1, nu2=.1\n",
    "        )\n",
    "\n",
    "        sblr.fit(train_maps, train_labels)\n",
    "\n",
    "        # Predict on test set\n",
    "        predictions = sblr.predict(test_maps)\n",
    "\n",
    "        # Evaluate performance\n",
    "        accuracy = np.mean(predictions == test_labels)\n",
    "        print(f\"Fold {fold}: Accuracy = {accuracy:.4f}\")\n",
    "        return sblr.U, sblr.V\n",
    "####################################################################################################\n",
    "\n",
    "   \n",
    "# https://ieeexplore.ieee.org/abstract/document/1467563\n",
    "def spatial_fda(train_maps,train_labels,within=True):\n",
    "\n",
    "    # Separate Train Maps into Groups\n",
    "    groupA_train = train_maps[train_labels==1]\n",
    "    groupB_train = train_maps[train_labels==0]\n",
    "\n",
    "    # Calculate Average Set of Train Spatial Maps per Group\n",
    "    groupA_mean_train = np.mean(groupA_train,axis=0)\n",
    "    groupB_mean_train = np.mean(groupB_train,axis=0)\n",
    "    # cov_est = EmpiricalCovariance(assume_centered=True)\n",
    "    # cov_est = LedoitWolf(assume_centered=True)\n",
    "    cov_est = OAS(assume_centered=True)\n",
    "\n",
    "    all_mean = np.mean(train_maps,axis=0)\n",
    "    Sb = groupA_train.shape[0]*cov_est.fit((groupA_mean_train - all_mean).T).covariance_\n",
    "    Sb += groupB_train.shape[0]*cov_est.fit((groupB_mean_train - all_mean).T).covariance_\n",
    "    # Sb = (groupA_mean_train - groupB_mean_train)@(groupA_mean_train - groupB_mean_train).T\n",
    "\n",
    "    if within:\n",
    "        groupA_within = np.sum(np.array([cov_est.fit((A-groupA_mean_train).T).covariance_ for A in groupA_train]),axis=0)\n",
    "        groupB_within = np.sum(np.array([cov_est.fit((B-groupB_mean_train).T).covariance_ for B in groupB_train]),axis=0)\n",
    "        Sw = (groupA_within + groupB_within) \n",
    "        # # An Alternative Using Weighted & Riemannian Means Instead of Sums\n",
    "        # Sb /= groupA_train.shape[0] + groupB_train.shape[0]\n",
    "        # groupA_within = np.array([cov_est.fit((A-groupA_mean_train).T).covariance_ for A in groupA_train])\n",
    "        # groupB_within = np.array([cov_est.fit((B-groupB_mean_train).T).covariance_ for B in groupB_train])\n",
    "        # Sw = mean_covariance(np.vstack((groupA_within,groupB_within)),metric=metric)\n",
    "        # For class A (shape: (N_A, C, V)) with mean groupA_mean_train (C, V)\n",
    "\n",
    "        # # An Alternative Using Pooling Prior to Covariance Formation\n",
    "        # deviations_A = np.concatenate([subject - groupA_mean_train for subject in groupA_train], axis=1)\n",
    "        # deviations_A = deviations_A.T  # shape: (N_A * V, C)\n",
    "        # # For class B (shape: (N_B, C, V)) with mean groupB_mean_train (C, V)\n",
    "        # deviations_B = np.concatenate([subject - groupB_mean_train for subject in groupB_train], axis=1)\n",
    "        # deviations_B = deviations_B.T  # shape: (N_B * V, C)\n",
    "        # # Pool all deviations together:\n",
    "        # all_deviations = np.vstack((deviations_A, deviations_B))\n",
    "        # # Compute the pooled covariance:\n",
    "        # Sw = cov_est.fit(all_deviations).covariance_\n",
    "        # print(\"Pooled within-class covariance (both classes) shape:\", Sw.shape)\n",
    "        _, U = eigh(Sb,Sw)\n",
    "    else:\n",
    "        _, U = eigh(Sb)\n",
    "    \n",
    "    return U, U.T\n",
    "\n",
    "def sparse_spatial_dist(train_maps,train_labels,n_components=None,alpha=.01,batch_size=100):\n",
    "    # Separate Train Maps into Groups\n",
    "    groupA_train = train_maps[train_labels==1]\n",
    "    groupB_train = train_maps[train_labels==0]\n",
    "\n",
    "    # Calculate Average Set of Train Spatial Maps per Group\n",
    "    A = np.mean(groupA_train,axis=0)\n",
    "    B = np.mean(groupB_train,axis=0)\n",
    "    # Parameters for MiniBatch Sparse PCA\n",
    "    if n_components is None:\n",
    "        n_components = A.shape[0]  # Number of sparse components to extract\n",
    "\n",
    "    # Apply MiniBatch Sparse PCA\n",
    "    mini_batch_sparse_pca = MiniBatchSparsePCA(\n",
    "        n_components=n_components,\n",
    "        alpha=alpha,\n",
    "        batch_size=batch_size,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    diff_matrix = A - B\n",
    "\n",
    "    # Fit the model to the difference matrix\n",
    "    # Enforce sparsity in the Space Direction not the combination of component directions\n",
    "    mini_batch_sparse_pca.fit(diff_matrix)\n",
    "\n",
    "    # Explained variance approximation (for interpretation)\n",
    "    approximation = mini_batch_sparse_pca.transform(diff_matrix)\n",
    "    # Reverse the order of columns to be in ascending order like other two methods\n",
    "    approximation = approximation[:, ::-1]\n",
    "\n",
    "    print(np.linalg.norm(approximation,axis=0))\n",
    "    norms = np.linalg.norm(approximation,axis=0)\n",
    "    norms[norms==0] = 1\n",
    "    U = approximation/norms\n",
    "    \n",
    "    ## Get the sparse components (principal directions)\n",
    "    # sparse_components = mini_batch_sparse_pca.components_\n",
    "\n",
    "    ## Rough equivalent of eigenvalues (only measures variance explained for that component)\n",
    "    ##   U's in this case are not forced to be orthogonal, so this norm does not account for covariance\n",
    "    # e = np.linalg.norm(approximation,axis=0)**2\n",
    "\n",
    "    return U, U.T\n",
    "\n",
    "## This Learns Seperate Subspaces to Project Each Group onto so will result in higher \n",
    "### accuracies than the other methods if want to purely maximize cosine similarity can do eigh since \n",
    "#### this amounts to vAB.Tv where A and B are orthonormal basis, however no longer represents subspace angles\n",
    "def grassmann_dist(train_maps,train_labels):\n",
    "    # Separate Train Maps into Groups\n",
    "    groupA_train = train_maps[train_labels==1]\n",
    "    groupB_train = train_maps[train_labels==0]\n",
    "\n",
    "    # Calculate Average Set of Train Spatial Maps per Group\n",
    "    A = np.mean(groupA_train,axis=0)\n",
    "    B = np.mean(groupB_train,axis=0)\n",
    "    # The commented out code is a potential way for regularization by reducing the \"sample/ambient\" space via PCA\"\n",
    "    # A = hcp.normalize(A.T).T\n",
    "    # B = hcp.normalize(B.T).T\n",
    "    # U, S, Red = np.linalg.svd(np.vstack((A,B)),full_matrices=False)\n",
    "    # A_q, A_r = np.linalg.qr((A@Red[:20,:].T).T)\n",
    "    # B_q, B_r = np.linalg.qr((B@Red[:20,:].T).T)\n",
    "    A_q, _ = np.linalg.qr((A).T)\n",
    "    B_q, _ = np.linalg.qr((B).T)\n",
    "    product = A_q.T @ B_q\n",
    "    U, S, Vt = np.linalg.svd(product, full_matrices=False)\n",
    "    # print(product.shape)\n",
    "    # S, U = np.linalg.eigh(product)\n",
    "    # Vt = U.T\n",
    "    angles = np.arccos(S)  # Principal angles\n",
    "    print(S)\n",
    "    # Confirm S matches subspace_angles result\n",
    "    assert np.all(np.abs(angles - subspace_angles(A.T, B.T)[::-1]) < 1e-3), \"Mismatch in singular values and principal angles\"\n",
    "    return U, Vt  # Principal angles and directions\n",
    "\n",
    "\n",
    "def spatial_vis(map_accs, discrim_dir_acc,outputfolder=None,analysis=\"IFA\"):\n",
    "\n",
    "    # === 1. Gather the accuracies per classifier ===\n",
    "    # Assume map_accs is a list of length n_maps; each element is a dict with keys = classifier names.\n",
    "    n_maps = len(map_accs)\n",
    "    clf_names = list(map_accs[0].keys())  # e.g., [\"SVM (C=0.1)\", \"LDA\", ...]\n",
    "    # Initialize a dictionary for each classifier to hold distributions by method.\n",
    "    acc_by_clf = {clf: {} for clf in clf_names}\n",
    "    # Collect baseline accuracies (from each map) for each classifier.\n",
    "    for clf in clf_names:\n",
    "        # Each map_accs[i][clf] is assumed to be a dict with an 'accuracy' key.\n",
    "        baseline_vals = [map_accs[i][clf]['accuracy'] for i in range(n_maps)]\n",
    "        acc_by_clf[clf][\"Baseline\"] = baseline_vals\n",
    "\n",
    "    # Collect accuracies from each discriminative method.\n",
    "    # Here, discrim_dir_acc is assumed to be a dictionary:\n",
    "    #   key: method (e.g., 1, 2, 4, etc.)\n",
    "    #   value: a list (length = n_maps) of result dictionaries.\n",
    "    for method, res_list in discrim_dir_acc.items():\n",
    "        label = f\"Method {method}\"\n",
    "        for clf in clf_names:\n",
    "            # For each projection direction (assumed one per map),\n",
    "            # extract the accuracy value from each result dictionary.\n",
    "            vals = [res[clf]['accuracy'] for res in res_list]\n",
    "            acc_by_clf[clf][label] = vals\n",
    "\n",
    "    # === 2. Plot histograms (one per classifier) with distributions overlaid ===\n",
    "    n_clf = len(clf_names)\n",
    "    fig, axes = plt.subplots(n_clf, 1, figsize=(8, 4 * n_clf))\n",
    "    if n_clf == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # For each classifier, plot a histogram for each method distribution.\n",
    "    for i, clf in enumerate(clf_names):\n",
    "        ax = axes[i]\n",
    "        method_labels = list(acc_by_clf[clf].keys())  # e.g., [\"Baseline\", \"Method 1\", \"Method 2\", ...]\n",
    "        n_methods = len(method_labels)\n",
    "        colors = sns.color_palette(\"husl\", n_methods)  # Generate distinct colors for each method.\n",
    "        for j, m_label in enumerate(method_labels):\n",
    "            data = np.array(acc_by_clf[clf][m_label])\n",
    "            # Plot histogram with a step style and overlay a KDE.\n",
    "            sns.histplot(data, bins=data.shape[0]*2, stat=\"frequency\", element=\"step\", fill=True,\n",
    "                         color=colors[j], alpha=0.4, ax=ax)\n",
    "            # sns.kdeplot(data, color=colors[j], ax=ax)\n",
    "            # Mark the mean of this distribution with a vertical dashed line.\n",
    "            mean_val = np.mean(data)\n",
    "            ax.axvline(mean_val, color=colors[j], linestyle=\"--\", linewidth=2,\n",
    "                       label=f\"{m_label} mean, max: {mean_val:.2f},{np.max(data):.2f}\")\n",
    "        ax.set_title(f\"Classifier: {clf}\")\n",
    "        ax.set_xlabel(\"Accuracy\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "     # === 3. Save the plot as an SVG file if an output folder is provided ===\n",
    "    if outputfolder:\n",
    "        if not os.path.exists(outputfolder):\n",
    "            os.makedirs(outputfolder)\n",
    "        svg_path = os.path.join(outputfolder, f\"{analysis}_spatial_discrim.svg\")\n",
    "        plt.savefig(svg_path, format=\"svg\")\n",
    "        print(f\"Plot saved to {svg_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # Scatter plot visualization\n",
    "    fig, axes = plt.subplots(n_clf, 1, figsize=(8, 4 * n_clf))\n",
    "    if n_clf == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, clf in enumerate(clf_names):\n",
    "        ax = axes[i]\n",
    "        method_labels = list(acc_by_clf[clf].keys())\n",
    "        n_methods = len(method_labels)\n",
    "        colors = sns.color_palette(\"husl\", n_methods)\n",
    "\n",
    "        for j, m_label in enumerate(method_labels):\n",
    "            data = np.array(acc_by_clf[clf][m_label])\n",
    "            ax.plot(range(data.shape[0]), np.sort(data), '--o',alpha=0.6, label=m_label, color=colors[j])\n",
    "\n",
    "        ax.set_xticks(range(data.shape[0]))\n",
    "        ax.set_xlabel(\"Map/Basis Vector Sorted By Classification Accuracy\")\n",
    "        ax.set_title(f\"Classifier: {clf}\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save scatter plot\n",
    "    if outputfolder:\n",
    "        scatter_path = os.path.join(outputfolder, f\"{analysis}_spatial_discrim_scatter.svg\")\n",
    "        plt.savefig(scatter_path, format=\"svg\")\n",
    "        print(f\"Scatter plot saved to {scatter_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def reduce_dimensionality_torch(train_spatial_maps, test_spatial_maps, device=None, n=100, svd=True, demean=True):\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    elif device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    # Convert inputs to torch tensors (if they aren’t already) and move them to the specified device.\n",
    "    if not isinstance(train_spatial_maps, torch.Tensor):\n",
    "        train_spatial_maps = torch.tensor(train_spatial_maps, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        train_spatial_maps = train_spatial_maps.to(device)\n",
    "    \n",
    "    if not isinstance(test_spatial_maps, torch.Tensor):\n",
    "        test_spatial_maps = torch.tensor(test_spatial_maps, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        test_spatial_maps = test_spatial_maps.to(device)\n",
    "    \n",
    "    n_comp = int(train_spatial_maps.shape[-1] / n)\n",
    "    \n",
    "    # Optionally demean the data using the training mean.\n",
    "    if demean:\n",
    "        train_mean = torch.mean(train_spatial_maps, dim=0)\n",
    "        train_data = train_spatial_maps - train_mean\n",
    "        test_data = test_spatial_maps - train_mean\n",
    "    else:\n",
    "        train_data = train_spatial_maps\n",
    "        test_data = test_spatial_maps\n",
    "\n",
    "    if svd:\n",
    "        # SVD branch: Use SVD on the (optionally demeaned) training data.\n",
    "        # Note: torch.linalg.svd returns singular values in descending order.\n",
    "        _, S, Vh = torch.linalg.svd(train_data, full_matrices=False)\n",
    "        # Take the top n_comp components.\n",
    "        reduced_train = train_data @ Vh[:n_comp, :].T\n",
    "        reduced_test = test_data @ Vh[:n_comp, :].T\n",
    "        return reduced_train.cpu().numpy(), reduced_test.cpu().numpy()\n",
    "    else:\n",
    "        # EVD branch: Compute the dual covariance matrix and perform eigen-decomposition.\n",
    "        cov_train = train_data @ train_data.T / train_spatial_maps.size(1)\n",
    "        \n",
    "        # Perform eigenvalue decomposition. torch.linalg.eigh returns eigenvalues in ascending order.\n",
    "        e, U = torch.linalg.eigh(cov_train)\n",
    "        # Compute principal components in the original space using the dual formulation.\n",
    "        # We select the n_comp components with the largest eigenvalues (which are at the end).\n",
    "        V = ((train_data.T @ U) / torch.sqrt(e.unsqueeze(0) + 1e-10))[:, -n_comp:]\n",
    "        \n",
    "        # Project the (optionally demeaned) training and test data.\n",
    "        X_train_reduced = train_data @ V\n",
    "        X_test_reduced = test_data @ V\n",
    "        return X_train_reduced.cpu().numpy(), X_test_reduced.cpu().numpy()\n",
    "\n",
    "def spatial_discrimination(train_maps, train_labels, test_maps, test_labels,methods=[1,2,4,5],metric=\"riemann\",visualize=True,outputfolder=None,analysis=\"IFA\"):\n",
    "    # Note for method 1, 2, & 4 Vt == U.T, This is just done so the same code can be used for the grassmann dist\n",
    "    #           which operates on two different subspaces where U != V\n",
    "    \n",
    "    # First look at accuracy of individual maps that span the subspace\n",
    "    map_accs = []\n",
    "    for i in range(train_maps.shape[1]):\n",
    "        train_map_reduced, test_map_reduced = reduce_dimensionality_torch(train_maps[:, i, :], test_maps[:, i, :], device=None, n=100, svd=True, demean=True)\n",
    "        results = linear_classifier(train_map_reduced, train_labels, test_map_reduced, test_labels, clf_str='Logistic Regression', z_score=1)\n",
    "        map_accs.append(results)\n",
    "    print(\"Map Acc\")\n",
    "\n",
    "    # Compute the maximum separating directions within that subspace based on different heurstics\n",
    "    discrim_dir_acc = {}   # key: method code, value: list of accuracy result dicts (one per projection direction)\n",
    "    discrim_dir = {}       # key: method code, value: (U, Vt)\n",
    "    for method in methods:\n",
    "        # Maximize Between Class Distance Measured via Euclidean Distance\n",
    "        if method == 1:\n",
    "            U, Vt = spatial_fda(train_maps, train_labels,within=False)\n",
    "         # Maximize Between Class Distance and Minimize Within Class Spread Measured via Euclidean Distance\n",
    "        elif method == 2:\n",
    "            U, Vt = spatial_fda(train_maps, train_labels,within=True)\n",
    "        # Sparse Maximization of Between Class Distance Measured via Euclidean Distance\n",
    "        elif method == 3:\n",
    "            U, Vt = sparse_spatial_dist(train_maps, train_labels,n_components=None,alpha=.01,batch_size=100)\n",
    "        # Maximizition of Between Class Distance Measure via Cosine Similarity (i.e., Maximize Subspace Angles)\n",
    "        elif method == 4:\n",
    "            U, Vt = grassmann_dist(train_maps,train_labels)\n",
    "        # CSP (Maximize Distance Between Class Average Covariances)\n",
    "        elif method == 5:\n",
    "            cov_est = Covariances(estimator='oas')\n",
    "            train_covs = cov_est.transform(train_maps)\n",
    "            eigs, U, _, _ = TSSF(train_covs, train_labels, clf_str='Logistic Regression', metric=metric, deconf=False, con_confounder_train=None, cat_confounder_train=None, z_score=0, haufe=False, visualize=False, output_dir=None)\n",
    "            U = U[:,np.argsort(eigs)]\n",
    "            Vt = U.T\n",
    "\n",
    "        groupA_train = train_maps[train_labels==1]\n",
    "        groupB_train = train_maps[train_labels==0]\n",
    "\n",
    "        groupA_test = test_maps[test_labels==1]\n",
    "        groupB_test = test_maps[test_labels==0]\n",
    "        print(\"Found Direction\")\n",
    "        accs = []\n",
    "        for i in range(train_maps.shape[1]):\n",
    "            train_proj = np.vstack((U[:,i].T@groupA_train, Vt[i,:]@groupB_train))\n",
    "            proj_train_labels = np.hstack((np.ones(groupA_train.shape[0]),np.zeros(groupB_train.shape[0])))\n",
    "            test_proj = np.vstack((U[:,i].T@groupA_test, Vt[i,:]@groupB_test))\n",
    "            proj_test_labels = np.hstack((np.ones(groupA_test.shape[0]),np.zeros(groupB_test.shape[0])))\n",
    "            train_reduced,test_reduced = reduce_dimensionality_torch(train_proj,test_proj,device=\"cpu\")\n",
    "            direction_results = linear_classifier(train_reduced, proj_train_labels, test_reduced, proj_test_labels, clf_str='Logistic Regression', z_score=1)\n",
    "            accs.append(direction_results)\n",
    "        print(\"Direction Acc\")\n",
    "\n",
    "        discrim_dir_acc[method] = accs\n",
    "        discrim_dir[method] = (U, Vt)\n",
    "\n",
    "    if visualize:\n",
    "        spatial_vis(map_accs, discrim_dir_acc,outputfolder=outputfolder,analysis=analysis)\n",
    "    \n",
    "    \n",
    "    return (map_accs,discrim_dir_acc,discrim_dir)\n",
    "\n",
    "def spatial_comparison_vis(IFA_results, ICA_results, outputfolder=None, analysis=\"IFA_vs_ICA\"):\n",
    "    \"\"\"\n",
    "    Compares classification results for IFA (blue) and ICA (gold) across different classifiers and dimensionality reduction methods.\n",
    "    Generates separate histograms and scatter plots for:\n",
    "      - Map accuracy (IFA vs. ICA)\n",
    "      - Discriminative accuracy (IFA vs. ICA)\n",
    "\n",
    "    Parameters:\n",
    "        IFA_results: tuple of (map_accs, discrim_dir_acc, discrim_dir) for IFA\n",
    "        ICA_results: tuple of (map_accs, discrim_dir_acc, discrim_dir) for ICA\n",
    "        outputfolder: str, optional, folder to save plots (default: None)\n",
    "        analysis: str, optional, name prefix for saved plots (default: \"IFA_vs_ICA\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack results\n",
    "    IFA_map_accs, IFA_discrim_dir_acc, _ = IFA_results\n",
    "    ICA_map_accs, ICA_discrim_dir_acc, _ = ICA_results\n",
    "    \n",
    "    # Extract classifier names and methods\n",
    "    clf_names = list(IFA_map_accs[0].keys())  \n",
    "    methods = list(IFA_discrim_dir_acc.keys())  \n",
    "\n",
    "    # Define colors\n",
    "    IFA_color = \"blue\"\n",
    "    ICA_color = \"orange\"\n",
    "\n",
    "    for clf in clf_names:\n",
    "        for method in methods:\n",
    "\n",
    "            ### ======= MAP ACCURACY PLOTS (IFA vs. ICA) ======= ###\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "            # Prepare data\n",
    "            IFA_data_map = [res[clf]['accuracy'] for res in IFA_map_accs]\n",
    "            ICA_data_map = [res[clf]['accuracy'] for res in ICA_map_accs]\n",
    "\n",
    "            # Histogram (Map Accuracy)\n",
    "            ax = axes[0]\n",
    "            sns.histplot(IFA_data_map, bins=20, color=IFA_color, alpha=0.5, label=\"IFA\", ax=ax)\n",
    "            sns.histplot(ICA_data_map, bins=20, color=ICA_color, alpha=0.5, label=\"ICA\", ax=ax)\n",
    "            \n",
    "            ax.axvline(np.mean(IFA_data_map), color=IFA_color, linestyle=\"--\", linewidth=2, label=f\"IFA Mean: {np.mean(IFA_data_map):.2f}\")\n",
    "            ax.axvline(np.mean(ICA_data_map), color=ICA_color, linestyle=\"--\", linewidth=2, label=f\"ICA Mean: {np.mean(ICA_data_map):.2f}\")\n",
    "            \n",
    "            ax.set_title(f\"Histogram - {clf} (Method {method}) - Map Accuracy\")\n",
    "            ax.set_xlabel(\"Accuracy\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            ax.legend()\n",
    "\n",
    "            # Scatter Plot (Sorted map accuracy values)\n",
    "            ax = axes[1]\n",
    "            sorted_ifa_map = np.sort(IFA_data_map)\n",
    "            sorted_ica_map = np.sort(ICA_data_map)\n",
    "\n",
    "            ax.plot(range(len(sorted_ifa_map)), sorted_ifa_map, 'o--', alpha=0.6, color=IFA_color, label=\"IFA\")\n",
    "            ax.plot(range(len(sorted_ica_map)), sorted_ica_map, 'o--', alpha=0.6, color=ICA_color, label=\"ICA\")\n",
    "\n",
    "            ax.set_xticks(range(len(sorted_ifa_map)))\n",
    "            ax.set_xlabel(\"Sorted Instances\")\n",
    "            ax.set_title(f\"Scatter Plot - {clf} (Method {method}) - Map Accuracy\")\n",
    "            ax.set_ylabel(\"Accuracy\")\n",
    "            ax.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save Map Accuracy Plots\n",
    "            if outputfolder:\n",
    "                if not os.path.exists(outputfolder):\n",
    "                    os.makedirs(outputfolder)\n",
    "                plot_path = os.path.join(outputfolder, f\"{analysis}_{clf}_Method_{method}_Map_Accuracy.svg\")\n",
    "                plt.savefig(plot_path, format=\"svg\")\n",
    "                print(f\"Map Accuracy Plot saved to {plot_path}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "            ### ======= DISCRIMINATIVE ACCURACY PLOTS (IFA vs. ICA) ======= ###\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "            # Prepare data\n",
    "            IFA_data_disc = [res[clf]['accuracy'] for res in IFA_discrim_dir_acc[method]]\n",
    "            ICA_data_disc = [res[clf]['accuracy'] for res in ICA_discrim_dir_acc[method]]\n",
    "\n",
    "            # Histogram (Discriminative Accuracy)\n",
    "            ax = axes[0]\n",
    "            sns.histplot(IFA_data_disc, bins=20, color=IFA_color, alpha=0.6, label=\"IFA\", ax=ax)\n",
    "            sns.histplot(ICA_data_disc, bins=20, color=ICA_color, alpha=0.6, label=\"ICA\", ax=ax)\n",
    "            \n",
    "            ax.axvline(np.mean(IFA_data_disc), color=IFA_color, linestyle=\"--\", alpha=0.6, linewidth=2, label=f\"IFA Mean: {np.mean(IFA_data_disc):.2f}\")\n",
    "            ax.axvline(np.mean(ICA_data_disc), color=ICA_color, linestyle=\"--\", alpha=0.6, linewidth=2, label=f\"ICA Mean: {np.mean(ICA_data_disc):.2f}\")\n",
    "            \n",
    "            ax.set_title(f\"Histogram - {clf} (Method {method}) - Discriminative Accuracy\")\n",
    "            ax.set_xlabel(\"Accuracy\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            ax.legend()\n",
    "\n",
    "            # Scatter Plot (Sorted discriminative accuracy values)\n",
    "            ax = axes[1]\n",
    "            sorted_ifa_disc = np.sort(IFA_data_disc)\n",
    "            sorted_ica_disc = np.sort(ICA_data_disc)\n",
    "\n",
    "            ax.plot(range(len(sorted_ifa_disc)), sorted_ifa_disc, 'o--', alpha=0.6, color=IFA_color, label=\"IFA\")\n",
    "            ax.plot(range(len(sorted_ica_disc)), sorted_ica_disc, 'o--', alpha=0.6, color=ICA_color, label=\"ICA\")\n",
    "\n",
    "            ax.set_xticks(range(len(sorted_ifa_disc)))\n",
    "            ax.set_xlabel(\"Sorted Instances\")\n",
    "            ax.set_title(f\"Scatter Plot - {clf} (Method {method}) - Discriminative Accuracy\")\n",
    "            ax.set_ylabel(\"Accuracy\")\n",
    "            ax.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save Discriminative Accuracy Plots\n",
    "            if outputfolder:\n",
    "                plot_path = os.path.join(outputfolder, f\"{analysis}_{clf}_Method_{method}_Discriminative_Accuracy.svg\")\n",
    "                plt.savefig(plot_path, format=\"svg\")\n",
    "                print(f\"Discriminative Accuracy Plot saved to {plot_path}\")\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "from classification import linear_classifier\n",
    "\n",
    "# Function to compute subspace directions and plot alignment with spatial map directions\n",
    "def subspace_directions(method, time_norm=False):\n",
    "    for fold in range(1,2):\n",
    "        # Load spatial maps and test indices\n",
    "        # fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        # fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        # fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "\n",
    "        # if time_norm:\n",
    "        #     results = os.path.join(fold_results_nPCA, f\"Normalized\")\n",
    "        \n",
    "        # else:\n",
    "        #     results = os.path.join(fold_results_nPCA, f\"Demeaned\")\n",
    "                \n",
    "        # output = os.path.join(results, f\"{method}_SpatialMaps\")\n",
    "\n",
    "        # groupA_mean = np.load(os.path.join(output, f\"{method}_groupA_mean.npy\"))\n",
    "        # groupB_mean = np.load(os.path.join(output, f\"{method}_groupB_mean.npy\"))\n",
    "        # print(np.linalg.norm(np.load(os.path.join(output, f\"{method}_t_values.npy\"))))\n",
    "        # Load spatial maps and test indices\n",
    "        fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "        dual_dir = os.path.join(fold_output_dir, f\"Dual_Regression_{nPCA}\") #TODO Delete\n",
    "\n",
    "        indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "        train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "        test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "        labels = np.load(os.path.join(outputfolder,\"labels.npy\"))\n",
    "        train_labels = labels[train_idx]\n",
    "        test_labels = labels[test_idx]\n",
    "        if time_norm:\n",
    "            SpatialMaps = np.load(os.path.join(dual_dir, f\"{method}_spatial_maps.npy\"))\n",
    "        else:\n",
    "            SpatialMaps = np.load(os.path.join(dual_dir, f\"{method}_spatial_mapdm.npy\"))\n",
    "        train_maps = SpatialMaps[train_idx]\n",
    "        test_maps = SpatialMaps[test_idx]\n",
    "\n",
    "        print(\"Loaded Maps\")\n",
    "        thruple = spatial_discrimination(train_maps, train_labels, test_maps, test_labels,methods=[2],metric=metric)\n",
    "\n",
    "\n",
    "        return thruple\n",
    "\n",
    "# Compute angles for each method\n",
    "IFA_angles = subspace_directions(\"IFA\", time_norm=False)\n",
    "ICA_angles = subspace_directions(\"ICA\", time_norm=False)\n",
    "\n",
    "# # Plot 1: Histogram and KDE plots for each principal angle (n subplots)\n",
    "# n_angles = IFA_angles.shape[1]  # Number of principal angles\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# rows = int(np.ceil(np.sqrt(n_angles)))\n",
    "# cols = int(np.ceil(n_angles / rows))\n",
    "\n",
    "# for i in range(n_angles):\n",
    "#     plt.subplot(rows, cols, i + 1)\n",
    "#     plt.hist(IFA_angles[:, i], bins=20, label=f'IFA Angle {i+1}', color='blue', alpha=0.5, density=True)\n",
    "#     plt.hist(ICA_angles[:, i], bins=20, label=f'ICA Angle {i+1}', color='orange', alpha=0.5, density=True)\n",
    "#     sns.kdeplot(IFA_angles[:, i], label=f'IFA KDE {i+1}', color='blue', alpha=0.7)\n",
    "#     sns.kdeplot(ICA_angles[:, i], label=f'ICA KDE {i+1}', color='orange', alpha=0.7)\n",
    "#     plt.xlabel('Angle in Radians')\n",
    "#     plt.ylabel('Density')\n",
    "#     plt.title(f'Angle {i+1}')\n",
    "#     # plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot 2: Combined histogram and KDE for all principal angles\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.hist(IFA_angles.flatten(), bins=30, label='IFA Combined Histogram', color='blue', alpha=0.5, density=True)\n",
    "# plt.hist(ICA_angles.flatten(), bins=30, label='ICA Combined Histogram', color='orange', alpha=0.5, density=True)\n",
    "# sns.kdeplot(IFA_angles.flatten(), label='IFA Combined KDE', color='blue', alpha=0.7)\n",
    "# sns.kdeplot(ICA_angles.flatten(), label='ICA Combined KDE', color='orange', alpha=0.7)\n",
    "# plt.xlabel('Angle in Radians')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Combined Principal Angles')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_comparison_vis(IFA_angles, ICA_angles, outputfolder=None, analysis=\"IFA_vs_ICA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute subspace directions and plot alignment with spatial map directions\n",
    "def map_dist(method, time_norm=False):\n",
    "    all_angles = []\n",
    "    # a = np.zeros((2,91282))\n",
    "    # b = np.zeros((2,91282))\n",
    "    for fold in range(5):\n",
    "        # Load spatial maps and test indices\n",
    "        # fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        # fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        # fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "\n",
    "        # if time_norm:\n",
    "        #     results = os.path.join(fold_results_nPCA, f\"Normalized\")\n",
    "        # else:\n",
    "        #     results = os.path.join(fold_results_nPCA, f\"Demeaned\")\n",
    "                \n",
    "        # output = os.path.join(results, f\"{method}_SpatialMaps\")\n",
    "\n",
    "\n",
    "        fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "        dual_dir = os.path.join(fold_output_dir, f\"Dual_Regression_{nPCA}\") #TODO Delete\n",
    "\n",
    "        indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "        train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "        test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "        labels = np.load(os.path.join(outputfolder,\"labels.npy\"))\n",
    "        train_labels = labels[train_idx]\n",
    "        test_labels = labels[test_idx]\n",
    "        if time_norm:\n",
    "            SpatialMaps = np.load(os.path.join(dual_dir, f\"{method}_spatial_maps.npy\"))\n",
    "        else:\n",
    "            SpatialMaps = np.load(os.path.join(dual_dir, f\"{method}_spatial_mapdm.npy\"))\n",
    "        train_maps = SpatialMaps[train_idx]\n",
    "        test_maps = SpatialMaps[test_idx]\n",
    "        print(train_maps.shape,test_maps.shape)\n",
    "\n",
    "        groupA_mean = np.mean(test_maps[test_labels==1],axis=0)\n",
    "        groupB_mean = np.mean(test_maps[test_labels==0],axis=0)\n",
    "\n",
    "        # groupA_mean = np.load(os.path.join(output, f\"{method}_groupA_mean.npy\"))\n",
    "        # groupB_mean = np.load(os.path.join(output, f\"{method}_groupB_mean.npy\"))\n",
    "        # angles, U, Vt = grassmann_dist(groupA_mean, groupB_mean)\n",
    "        # groupA_mean = hcp.normalize(groupA_mean.T).T\n",
    "        # groupB_mean = hcp.normalize(groupB_mean.T).T\n",
    "        angles = np.diag((np.arccos(np.corrcoef(groupA_mean, groupB_mean, rowvar=True)))[:groupA_mean.shape[0], groupA_mean.shape[0]:])\n",
    "        # angles = ((np.linalg.norm(groupA_mean - groupB_mean,axis=-1)**2))\n",
    "        all_angles.append((angles))\n",
    "\n",
    "    return np.array(all_angles)\n",
    "\n",
    "def hist_maps(map1,map2,label_1, label_2):\n",
    "    # Plot 1: Histogram and KDE plots for each principal angle (n subplots)\n",
    "    n_angles = map1.shape[1]  # Number of principal angles\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    rows = int(np.ceil(np.sqrt(n_angles)))\n",
    "    cols = int(np.ceil(n_angles / rows))\n",
    "\n",
    "    for i in range(n_angles):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.hist(map1[:, i], bins=20, label=f'{label_1} Angle {i+1}', color='blue', alpha=0.5, density=True)\n",
    "        plt.hist(map2[:, i], bins=20, label=f'{label_2} Angle {i+1}', color='black', alpha=0.5, density=True)\n",
    "        sns.kdeplot(map1[:, i], label=f'{label_1} KDE {i+1}', color='blue', alpha=0.7)\n",
    "        sns.kdeplot(map2[:, i], label=f'{label_2} KDE {i+1}', color='black', alpha=0.7)\n",
    "        plt.xlabel('Angle in Radians')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'Angle {i+1}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Plot 2: Combined histogram and KDE for all principal angles\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(map1.flatten(), bins=40, label=f'{label_1} Combined Histogram', color='blue', alpha=0.5, density=True)\n",
    "    plt.hist(map2.flatten(), bins=40, label=f'{label_2} Combined Histogram', color='black', alpha=0.5, density=True)\n",
    "    sns.kdeplot(map1.flatten(), label=f'{label_1} Combined KDE', color='blue', alpha=0.7)\n",
    "    sns.kdeplot(map2.flatten(), label=f'{label_2} Combined KDE', color='black', alpha=0.7)\n",
    "    plt.xlabel('Angle in Radians')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Spatial Map arccos distance vs Subspace Angles')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "IFA_map_angles = map_dist(\"IFA\", time_norm=True)\n",
    "hist_maps(IFA_map_angles,IFA_angles,\"IFA Spatial Map\", \"IFA Subspace Angles\")\n",
    "# print(IFA_map_angles.shape)\n",
    "ICA_map_angles = map_dist(\"ICA\", time_norm=True)\n",
    "hist_maps(ICA_map_angles,ICA_angles,\"ICA Spatial Map\", \"ICA Subspace Angles\")\n",
    "hist_maps(ICA_map_angles,IFA_map_angles, \"ICA Spatial Map\",\"IFA Spatial Map\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from classification import linear_classifier\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def reduce_dimensionality_torch(spatial_maps, train_idx, test_idx, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction on spatial maps using PCA with GPU acceleration.\n",
    "\n",
    "    Parameters:\n",
    "    - spatial_maps: numpy.ndarray\n",
    "        The input data array with samples and features.\n",
    "    - train_idx: array-like\n",
    "        Indices of training samples.\n",
    "    - test_idx: array-like\n",
    "        Indices of test samples.\n",
    "    - device: str\n",
    "        The device to perform computations on (e.g., \"cuda\" for GPU or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    - X_train_reduced: torch.Tensor\n",
    "        Training data projected into reduced dimensionality.\n",
    "    - X_test_reduced: torch.Tensor\n",
    "        Test data projected into the reduced space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data to PyTorch tensors and move to the specified device\n",
    "    spatial_maps_train = torch.tensor(spatial_maps[train_idx], dtype=torch.float32, device=device)\n",
    "    spatial_maps_train_flat = spatial_maps_train.view(-1, spatial_maps_train.size(-1))\n",
    "    U,S,Vh = torch.linalg.svd(spatial_maps_train_flat,full_matrices=False)\n",
    "    spatial_maps = torch.tensor(spatial_maps, dtype=torch.float32, device=device)\n",
    "    reduced = spatial_maps@Vh[:300,:].T\n",
    "    reduced_cpu = reduced.cpu().numpy()\n",
    "    print(reduced_cpu.shape)\n",
    "    return reduced_cpu\n",
    "    # print(spatial_maps.view(-1, spatial_maps.size(-1)).T.shape)\n",
    "    print(torch.pca_lowrank(spatial_maps.view(-1, spatial_maps.size(-1)).T))\n",
    "    # # Flatten spatial maps\n",
    "    # flatten = spatial_maps.view(spatial_maps.size(0), -1)\n",
    "\n",
    "    # # Separate training and test data\n",
    "    # flatten_train = flatten[train_idx]\n",
    "    # flatten_test = flatten[test_idx]\n",
    "\n",
    "    # # Mean-center the training data\n",
    "    # flatten_train_mean = flatten_train.mean(dim=0, keepdim=True)\n",
    "    # flatten_train_dm = flatten_train - flatten_train_mean\n",
    "    # flatten_test_dm = flatten_test - flatten_train_mean\n",
    "    # print(flatten_train_dm.shape, flatten_train_mean.shape)\n",
    "    # # Compute dual covariance matrix\n",
    "    # print(\"Computing dual covariance matrix...\")\n",
    "    # flatten_cov_train = flatten_train_dm @ flatten_train_dm.T / flatten_train.size(1)\n",
    "\n",
    "    # # Singular Value Decomposition (SVD)\n",
    "    # print(\"Performing SVD...\")\n",
    "    # U, S, Vt = torch.linalg.svd(flatten_cov_train)\n",
    "\n",
    "    # # Compute principal components in the original space\n",
    "    # print(\"Finding V Reduced...\")\n",
    "    # V = (flatten_train_dm.T @ U) / torch.sqrt(S.unsqueeze(0) + 1e-10)\n",
    "\n",
    "    # # Project training and test data\n",
    "    # print(\"Reducing Train...\")\n",
    "    # X_train_reduced = flatten_train_dm @ V\n",
    "\n",
    "    # print(\"Reducing Test...\")\n",
    "    # X_test_reduced = flatten_test_dm @ V\n",
    "\n",
    "    # print(\"Dimensionality reduction complete.\")\n",
    "    # return X_train_reduced.cpu().numpy(), X_test_reduced.cpu().numpy()\n",
    "\n",
    "# def reduce_dimensionality(spatial_maps, train_idx, test_idx):\n",
    "#     \"\"\"\n",
    "#     Perform dimensionality reduction on spatial maps using PCA with custom dual covariance matrix.\n",
    "\n",
    "#     Parameters:\n",
    "#     - spatial_maps: numpy.ndarray\n",
    "#         The input data array with samples and features.\n",
    "#     - train_idx: array-like\n",
    "#         Indices of training samples.\n",
    "#     - test_idx: array-like\n",
    "#         Indices of test samples.\n",
    "\n",
    "#     Returns:\n",
    "#     - X_train_reduced: numpy.ndarray\n",
    "#         Training data projected into reduced dimensionality.\n",
    "#     - X_test_reduced: numpy.ndarray\n",
    "#         Test data projected into the reduced space.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Flatten spatial maps\n",
    "#     flatten = spatial_maps.reshape(spatial_maps.shape[0], -1)\n",
    "\n",
    "#     # Separate training and test data\n",
    "#     flatten_train = flatten[train_idx]\n",
    "#     flatten_test = flatten[test_idx]\n",
    "\n",
    "#     # Mean-center the training data\n",
    "#     flatten_train_mean = flatten_train.mean(axis=0, keepdims=True)\n",
    "#     flatten_train_dm = flatten_train - flatten_train_mean\n",
    "#     flatten_test_dm = flatten_test - flatten_train_mean\n",
    "\n",
    "#     # Compute dual covariance matrix\n",
    "#     print(\"Computing dual covariance matrix...\")\n",
    "#     flatten_cov_train = flatten_train_dm @ flatten_train_dm.T\n",
    "\n",
    "#     # Normalize covariance matrix\n",
    "#     num_features = flatten_train.shape[1]  # Number of features\n",
    "#     flatten_cov_norm = flatten_cov_train / num_features\n",
    "\n",
    "#     # Eigen-decomposition of the normalized covariance matrix\n",
    "#     print(\"Performing eigen-decomposition...\")\n",
    "#     eigs, vecs = eigh(flatten_cov_norm)\n",
    "\n",
    "#     # Sort eigenvalues and eigenvectors in descending order\n",
    "#     idx = np.argsort(eigs)[::-1]\n",
    "#     eigs = eigs[idx]\n",
    "#     vecs = vecs[:, idx]\n",
    "#     # eigs = eigs[:flatten_train.shape[0]]\n",
    "#     # vecs = vecs[:,:flatten_train.shape[0]]    \n",
    "#     print(\"Finding V Reduced\")\n",
    "\n",
    "#     # Compute principal components in the original space\n",
    "#     V = (flatten_train_dm.T @ vecs) / np.sqrt(eigs + 1e-10)\n",
    "\n",
    "#     # Project training and test data\n",
    "#     print(\"Reducing Train\")\n",
    "#     X_train_reduced = flatten_train_dm @ V\n",
    "    \n",
    "#     print(\"Reducing Test\")\n",
    "#     X_test_reduced = flatten_test_dm @ V\n",
    "\n",
    "#     print(\"Dimensionality reduction complete.\")\n",
    "#     return X_train_reduced, X_test_reduced\n",
    "\n",
    "def classify_space(time_norm=False):\n",
    "    IFA_acc = []\n",
    "    ICA_acc = []\n",
    "    for fold in range(5):\n",
    "        print(f\"Processing fold {fold}...\")\n",
    "        # Load spatial maps and test indices\n",
    "        fold_output_dir = os.path.join(outputfolder, f\"fold_{fold}\")\n",
    "        fold_results = os.path.join(fold_output_dir, \"Results\") #TODO Uncomment\n",
    "        fold_results_nPCA = os.path.join(fold_results, f\"Results_{nPCA}\") #TODO Delete\n",
    "        dual_dir = os.path.join(fold_output_dir, f\"Dual_Regression_{nPCA}\") #TODO Delete\n",
    "\n",
    "        indices_dir = os.path.join(fold_output_dir, \"Indices\")\n",
    "        train_idx = np.load(os.path.join(indices_dir, \"train_idx.npy\"))\n",
    "        test_idx = np.load(os.path.join(indices_dir, \"test_idx.npy\"))\n",
    "        labels = np.load(os.path.join(outputfolder,\"labels.npy\"))\n",
    "        test_labels = labels[test_idx]\n",
    "\n",
    "        if time_norm:\n",
    "            IFA_SpatialMaps = np.load(os.path.join(dual_dir, \"IFA_spatial_maps.npy\"))\n",
    "            ICA_SpatialMaps = np.load(os.path.join(dual_dir, \"ICA_spatial_maps.npy\"))\n",
    "        else:\n",
    "            IFA_SpatialMaps = np.load(os.path.join(dual_dir, \"IFA_spatial_mapdm.npy\"))\n",
    "            ICA_SpatialMaps = np.load(os.path.join(dual_dir, \"ICA_spatial_mapdm.npy\"))\n",
    "\n",
    "        print(\"Loaded Spatial Maps\")\n",
    "        X_reduced_IFA = reduce_dimensionality_torch(IFA_SpatialMaps, train_idx, test_idx)\n",
    "        IFA_fold = []\n",
    "        for i in range(IFA_SpatialMaps.shape[1]):\n",
    "            results_IFA = linear_classifier(X_reduced_IFA[train_idx][:,i,:], labels[train_idx], X_reduced_IFA[test_idx][:,i,:], labels[test_idx], clf_str='SVM (C=0.01)', z_score=0)\n",
    "            IFA_fold.append(results_IFA['SVM (C=0.01)']['accuracy'])\n",
    "        print(IFA_fold)\n",
    "        IFA_acc.append(np.array(IFA_fold))\n",
    "        \n",
    "        X_reduced_ICA = reduce_dimensionality_torch(ICA_SpatialMaps, train_idx, test_idx)\n",
    "        ICA_fold = []\n",
    "        for i in range(ICA_SpatialMaps.shape[1]):\n",
    "            results_ICA = linear_classifier(X_reduced_ICA[train_idx][:,i,:], labels[train_idx], X_reduced_ICA[test_idx][:,i,:], labels[test_idx], clf_str='SVM (C=0.01)', z_score=0)\n",
    "            ICA_fold.append(results_ICA['SVM (C=0.01)']['accuracy'])\n",
    "        print(ICA_fold)\n",
    "        ICA_acc.append(np.array(ICA_fold))\n",
    "    \n",
    "    return np.array(IFA_acc), np.array(ICA_acc)\n",
    "\n",
    "IFA_acc, ICA_acc = classify_space(time_norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_maps(map1,map2,label_1, label_2):\n",
    "    # Plot 1: Histogram and KDE plots for each principal angle (n subplots)\n",
    "    n_angles = map1.shape[1]  # Number of principal angles\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    rows = int(np.ceil(np.sqrt(n_angles)))\n",
    "    cols = int(np.ceil(n_angles / rows))\n",
    "\n",
    "    for i in range(n_angles):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.hist(map1[:, i], bins=20, label=f'{label_1} Angle {i+1}', color='blue', alpha=0.5, density=True)\n",
    "        plt.hist(map2[:, i], bins=20, label=f'{label_2} Angle {i+1}', color='black', alpha=0.5, density=True)\n",
    "        sns.kdeplot(map1[:, i], label=f'{label_1} KDE {i+1}', color='blue', alpha=0.7)\n",
    "        sns.kdeplot(map2[:, i], label=f'{label_2} KDE {i+1}', color='black', alpha=0.7)\n",
    "        plt.xlabel('Angle in Radians')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'Angle {i+1}')\n",
    "        # plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Plot 2: Combined histogram and KDE for all principal angles\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(map1.flatten(), bins=30, label=f'{label_1} Combined Histogram', color='blue', alpha=0.5, density=True)\n",
    "    plt.hist(map2.flatten(), bins=30, label=f'{label_2} Combined Histogram', color='black', alpha=0.5, density=True)\n",
    "    sns.kdeplot(map1.flatten(), label=f'{label_1} Combined KDE', color='blue', alpha=0.7)\n",
    "    sns.kdeplot(map2.flatten(), label=f'{label_2} Combined KDE', color='black', alpha=0.7)\n",
    "    plt.xlabel('Angle in Radians')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Combined Principal Angles')\n",
    "    plt.legend()\n",
    "\n",
    "hist_maps(IFA_acc, ICA_acc,\"IFA\", \"ICA\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_accuracies(results):\n",
    "    return [fold['SVM (C=0.01)']['accuracy'] for fold in results]\n",
    "\n",
    "# Extract accuracies for IFA and ICA\n",
    "ifa_accuracies = extract_accuracies(IFA_acc)\n",
    "ica_accuracies = extract_accuracies(ICA_acc)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(ifa_accuracies, bins=10, alpha=0.7, label='IFA', color='blue', edgecolor='black')\n",
    "plt.hist(ica_accuracies, bins=10, alpha=0.7, label='ICA', color='orange', edgecolor='black')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Accuracy', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Classification Accuracy Comparison', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import reduce_dimensionality_torch\n",
    "from mne.stats import permutation_t_test, permutation_cluster_1samp_test, ttest_1samp_no_p\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "# from procrustes import permutation\n",
    "\n",
    "\n",
    "def iterative_consensus_matching(fold_maps, metric='correlation', max_iter=10, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Resources: \n",
    "        https://medium.com/@olga_kravchenko/generalized-procrustes-analysis-with-python-numpy-c571e8e8a421\n",
    "        https://procrustes.qcdevs.org/api/permutation.html\n",
    "    \"\"\"\n",
    "    num_folds, num_components, num_voxels = fold_maps.shape\n",
    "    \n",
    "    consensus = fold_maps[0].copy() \n",
    "    assignments = np.zeros((num_folds, num_components), dtype=int)\n",
    "    assignments[0] = np.arange(num_components)\n",
    "    matched_fold_maps = fold_maps.copy()\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        prev_consensus = consensus.copy()\n",
    "        \n",
    "        for f in range(num_folds):\n",
    "            cost_matrix = cdist(consensus, matched_fold_maps[f], metric=metric)\n",
    "            _, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            print(col_ind)\n",
    "            matched_fold_maps[f] = matched_fold_maps[f][col_ind, :]\n",
    "            assignments[f] = col_ind\n",
    "        consensus = matched_fold_maps.mean(axis=0)  # shape [10, num_voxels]\n",
    "        \n",
    "        diff = np.linalg.norm(consensus - prev_consensus)\n",
    "        if diff < tol:\n",
    "            print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "    \n",
    "    return matched_fold_maps, assignments\n",
    "\n",
    "condition = \"WM_Face_v_Place\"\n",
    "# fold = 0\n",
    "sets = []\n",
    "for fold in range(5):\n",
    "#    maps = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/ICA_8/IFA/IFA_zmaps.npy\").T\n",
    "   print(fold, \"loaded fold\",)\n",
    "   maps = np.load(\"/project/3022057.01/WM_2back_v_0back/fold_0/Dual_Regression_8/IFA_spatial_mapdm.npy\")\n",
    "   maps = maps.reshape(maps.shape[0],-1)\n",
    "   _, map_p_vals, _ = permutation_t_test(\n",
    "    maps,\n",
    "    n_permutations=1000,\n",
    "    tail=0,  # Two-tailed test,\n",
    "    n_jobs=-1,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    "    )\n",
    "   print(fold, \"pvalues\",)\n",
    "\n",
    "   sets.append(-np.log(map_p_vals).reshape(10,-1))\n",
    "\n",
    "fold_maps = np.array(sets)\n",
    "\n",
    "# all_maps = np.array(sets).reshape(50,-1)\n",
    "# all_maps_reduced, _ = reduce_dimensionality_torch(all_maps, all_maps, device=None, n=50, svd=True, demean=True)\n",
    "# fold_maps_reduced = all_maps_reduced.reshape(5,10,-1)\n",
    "# num_clusters = 10\n",
    "# labels = AgglomerativeClustering(n_clusters=num_clusters, metric='euclidean', linkage='ward').fit_predict(all_maps_reduced)\n",
    "# labels = KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\").fit_predict(all_maps)\n",
    "# labels = labels.reshape(5,-1)\n",
    "# print(labels)\n",
    "\n",
    "reordered, labels = iterative_consensus_matching(fold_maps, metric='correlation', max_iter=10, tol=1e-15)\n",
    "print(fold, \"munkres complete\",)\n",
    "\n",
    "for i in range(10):\n",
    "    for pair in (np.argwhere(labels==i).tolist()):\n",
    "        x,y = pair\n",
    "        view_filter = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(reordered[x,y,:]),\n",
    "            bg_map=hcp.mesh.sulc,\n",
    "            title=f\"{i} in Fold {x} is {y}\"\n",
    "        )\n",
    "        display(view_filter)\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)  # Clear the previous output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import reduce_dimensionality_torch\n",
    "from mne.stats import permutation_t_test, permutation_cluster_1samp_test, ttest_1samp_no_p\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "# from procrustes import permutation\n",
    "\n",
    "\n",
    "def iterative_consensus_matching(fold_maps, metric='correlation', max_iter=10, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Resources: \n",
    "        https://medium.com/@olga_kravchenko/generalized-procrustes-analysis-with-python-numpy-c571e8e8a421\n",
    "        https://procrustes.qcdevs.org/api/permutation.html\n",
    "    \"\"\"\n",
    "    num_folds, num_components, num_voxels = fold_maps.shape\n",
    "    \n",
    "    consensus = fold_maps[0].copy() \n",
    "    assignments = np.zeros((num_folds, num_components), dtype=int)\n",
    "    assignments[0] = np.arange(num_components)\n",
    "    matched_fold_maps = fold_maps.copy()\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        prev_consensus = consensus.copy()\n",
    "        \n",
    "        for f in range(num_folds):\n",
    "            cost_matrix = cdist(consensus, matched_fold_maps[f], metric=metric)\n",
    "            _, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            print(col_ind)\n",
    "            matched_fold_maps[f] = matched_fold_maps[f][col_ind, :]\n",
    "            assignments[f] = col_ind\n",
    "        consensus = matched_fold_maps.mean(axis=0)  # shape [10, num_voxels]\n",
    "        \n",
    "        diff = np.linalg.norm(consensus - prev_consensus)\n",
    "        if diff < tol:\n",
    "            print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "    \n",
    "    return matched_fold_maps, assignments\n",
    "\n",
    "condition = \"WM_Face_v_Place\"\n",
    "# fold = 0\n",
    "sets = []\n",
    "for fold in range(5):\n",
    "#    maps = np.load(f\"/project/3022057.01/{condition}/fold_{fold}/ICA_8/IFA/IFA_zmaps.npy\").T\n",
    "   print(fold, \"loaded fold\",)\n",
    "   maps = np.load(\"/project/3022057.01/WM_2back_v_0back/fold_0/Dual_Regression_8/IFA_spatial_mapdm.npy\")\n",
    "   maps = maps.reshape(maps.shape[0],-1)\n",
    "   _, map_p_vals, _ = permutation_t_test(\n",
    "    maps,\n",
    "    n_permutations=1000,\n",
    "    tail=0,  # Two-tailed test,\n",
    "    n_jobs=-1,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    "    )\n",
    "   print(fold, \"pvalues\",)\n",
    "\n",
    "   sets.append(-np.log(map_p_vals).reshape(10,-1))\n",
    "\n",
    "fold_maps = np.array(sets)\n",
    "\n",
    "# all_maps = np.array(sets).reshape(50,-1)\n",
    "# all_maps_reduced, _ = reduce_dimensionality_torch(all_maps, all_maps, device=None, n=50, svd=True, demean=True)\n",
    "# fold_maps_reduced = all_maps_reduced.reshape(5,10,-1)\n",
    "# num_clusters = 10\n",
    "# labels = AgglomerativeClustering(n_clusters=num_clusters, metric='euclidean', linkage='ward').fit_predict(all_maps_reduced)\n",
    "# labels = KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\").fit_predict(all_maps)\n",
    "# labels = labels.reshape(5,-1)\n",
    "# print(labels)\n",
    "\n",
    "reordered, labels = iterative_consensus_matching(fold_maps, metric='correlation', max_iter=10, tol=1e-15)\n",
    "print(fold, \"munkres complete\",)\n",
    "\n",
    "for i in range(10):\n",
    "    for pair in (np.argwhere(labels==i).tolist()):\n",
    "        x,y = pair\n",
    "        view_filter = view_surf(\n",
    "            surf_mesh=hcp.mesh.inflated,\n",
    "            surf_map=hcp.cortex_data(reordered[x,y,:]),\n",
    "            bg_map=hcp.mesh.sulc,\n",
    "            title=f\"{i} in Fold {x} is {y}\"\n",
    "        )\n",
    "        display(view_filter)\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)  # Clear the previous output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
