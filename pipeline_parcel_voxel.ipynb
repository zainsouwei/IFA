{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "# import cupy as cp\n",
    "import torch\n",
    "import hcp_utils as hcp # https://rmldj.github.io/hcp-utils/\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matlab.engine\n",
    "import os\n",
    "import json\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.linalg import eigh, svd\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from nilearn import image as nimg\n",
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "from pyriemann.utils.tangentspace import tangent_space, untangent_space, log_map_riemann, unupper\n",
    "from pyriemann.utils.distance import distance_riemann, distance\n",
    "from pyriemann.utils.base import logm, expm\n",
    "from concurrent.futures import ProcessPoolExecutor, TimeoutError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings have been saved to Gender_TangentSVMC1_logeuclid/settings.json\n"
     ]
    }
   ],
   "source": [
    "# Define your settings\n",
    "settings = {\n",
    "    \"outputfolder\": \"Gender_TangentSVMC1_logeuclid\",\n",
    "    \"n_folds\": 5,\n",
    "    \"TanSVM_C\": 1,\n",
    "    \"random_state\": 42,\n",
    "    \"n_filters_per_group\": 1,\n",
    "    \"Tangent_Class\": True,\n",
    "    \"metric\": \"logeuclid\"\n",
    "}\n",
    "\n",
    "# Ensure the output folder exists\n",
    "outputfolder = settings[\"outputfolder\"]\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)\n",
    "\n",
    "# Define the path for the settings file\n",
    "settings_filepath = os.path.join(outputfolder, \"settings.json\")\n",
    "\n",
    "# Save the settings to a JSON file\n",
    "with open(settings_filepath, \"w\") as f:\n",
    "    json.dump(settings, f, indent=4)\n",
    "\n",
    "print(f\"Settings have been saved to {settings_filepath}\")\n",
    "# Define the output folder\n",
    "n_folds = settings[\"n_folds\"]\n",
    "TanSVM_C = settings[\"TanSVM_C\"]\n",
    "random_state = settings[\"random_state\"]\n",
    "n_filters_per_group = settings[\"n_filters_per_group\"]\n",
    "Tangent_Class = settings[\"Tangent_Class\"]\n",
    "# Pyriemannian Mean https://github.com/pyRiemann/pyRiemann/blob/master/pyriemann/utils/mean.py#L633 Metric for mean estimation, can be: \"ale\", \"alm\", \"euclid\", \"harmonic\", \"identity\", \"kullback_sym\", \"logdet\", \"logeuclid\", \"riemann\", \"wasserstein\", or a callable function.\n",
    "# https://link.springer.com/article/10.1007/s12021-020-09473-9 <---- best descriptions/plots\n",
    "# Geometric means in a novel vector space structure on symmetric positive-definite matrices <https://epubs.siam.org/doi/abs/10.1137/050637996?journalCode=sjmael>`_\n",
    "metric = settings[\"metric\"]\n",
    "\n",
    "def load_array_from_outputfolder(filename):\n",
    "    filepath = os.path.join(outputfolder, filename)\n",
    "    return np.load(filepath)\n",
    "# Function to save an array to the output folder\n",
    "def save_array_to_outputfolder(filename, array):\n",
    "    filepath = os.path.join(outputfolder, filename)\n",
    "    np.save(filepath, array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory and Processor Usage/Limits Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\n",
    "#Open terminal for job\n",
    "# srun --jobid=68974 --overlap --pty /bin/bash \n",
    "\n",
    "# #SLURM RAM\n",
    "!cgget -r memory.limit_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "#SLURM VM\n",
    "!cgget -r memory.memsw.limit_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "#SLURM USAGE\n",
    "!cgget -r memory.memsw.usage_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "!echo \"SLURM_JOB_ID: $SLURM_JOB_ID\"\n",
    "!echo \"SLURM_JOB_NAME: $SLURM_JOB_NAME\"\n",
    "!echo \"SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST\"\n",
    "!echo \"SLURM_MEM_PER_NODE: $SLURM_MEM_PER_NODE\"\n",
    "!echo \"SLURM_CPUS_ON_NODE: $SLURM_CPUS_ON_NODE\"\n",
    "!echo \"SLURM_MEM_PER_CPU: $SLURM_MEM_PER_CPU\"\n",
    "\n",
    "!free -h\n",
    "\n",
    "import resource\n",
    "\n",
    "# Get the soft and hard limits of virtual memory (address space)\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "print(f\"Soft limit: {soft / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Hard limit: {hard / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "# Get the soft and hard limits of the data segment (physical memory usage)\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_DATA)\n",
    "print(f\"Soft limit: {soft / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Hard limit: {hard / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "#TORQUE Virtual Memory\n",
    "# !cgget -r memory.memsw.limit_in_bytes /torque/$PBS_JOBID\n",
    "\n",
    "# #TORQUE RAM\n",
    "# !cgget -r memory.limit_in_bytes /torque/$PBS_JOBID\n",
    "\n",
    "# #TORQUE USAGE\n",
    "# !cgget -r memory.memsw.usage_in_bytes /torque/$PBS_JOBID\n",
    "# print(int(os.environ['PBS_NP']))\n",
    "!nvidia-smi\n",
    "\n",
    "def gpu_mem():\n",
    "    # Memory usage information\n",
    "    print(f\"Total memory available: {(torch.cuda.get_device_properties('cuda').total_memory / 1024**3):.2f} GB\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "def cpu_mem():\n",
    "   # Display memory information\n",
    "    print(f\"Total Memory: { psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available Memory: { psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used Memory: { psutil.virtual_memory().used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory Usage: { psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Paths, Parcellate, Standardize, and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(folder1=0):\n",
    "    \"\"\"\n",
    "    Load data for a specified number of subjects and fMRI tasks, only if they have not been parcellated.\n",
    "    \"\"\"\n",
    "\n",
    "    base_directory = \"/project_cephfs/3022017.01/S1200\"\n",
    "    subdirectory = \"MNINonLinear/Results\"\n",
    "    \n",
    "    folders = [\n",
    "        \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\", \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "        \"tfMRI_EMOTION_LR\", \"tfMRI_EMOTION_RL\", \"tfMRI_GAMBLING_LR\", \"tfMRI_GAMBLING_RL\",\n",
    "        \"tfMRI_LANGUAGE_LR\", \"tfMRI_LANGUAGE_RL\", \"tfMRI_MOTOR_LR\", \"tfMRI_MOTOR_RL\",\n",
    "        \"tfMRI_RELATIONAL_LR\", \"tfMRI_RELATIONAL_RL\", \"tfMRI_SOCIAL_LR\", \"tfMRI_SOCIAL_RL\",\n",
    "        \"tfMRI_WM_LR\", \"tfMRI_WM_RL\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    if folder1 + 1 >= len(folders):\n",
    "        raise IndexError(f\"Invalid folder1 index: {folder1}. Check folder list.\")\n",
    "\n",
    "    if folder1 + 2 >= len(folders):\n",
    "        raise IndexError(f\"Invalid folder1 index: {folder1}. Check folder list.\")\n",
    "\n",
    "    if folder1 + 3 >= len(folders):\n",
    "        raise IndexError(f\"Invalid folder1 index: {folder1}. Check folder list.\")\n",
    "\n",
    "    subids = [sub for sub in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, sub))]\n",
    "\n",
    "    file_path_restricted = r'../HCP/RESTRICTED_zainsou_8_6_2024_2_11_21.csv'\n",
    "    file_path_unrestricted = r'../HCP/unrestricted_zainsou_8_2_2024_6_13_22.csv'\n",
    "\n",
    "    try:\n",
    "        # Load the data from CSV files\n",
    "        data_r = pd.read_csv(file_path_restricted)\n",
    "        data_ur = pd.read_csv(file_path_unrestricted)\n",
    "        print(\"Files loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path_restricted} or {file_path_unrestricted}\")\n",
    "        raise\n",
    "\n",
    "    # Combine restricted and unrestricted data on Subject ID\n",
    "    data = pd.merge(data_r, data_ur, on='Subject', how='outer')\n",
    "\n",
    "    filtered_data = data[data['Subject'].astype(str).isin(subids)]\n",
    "    # plt.hist(filtered_data[\"SSAGA_FTND_Score\"], bins=10)\n",
    "    # plt.show()\n",
    "    # picvocab_low_threshold = filtered_data[\"PicVocab_AgeAdj\"].quantile(.2)\n",
    "    # picvocab_high_threshold = filtered_data[\"PicVocab_AgeAdj\"].quantile(0.8)\n",
    "   \n",
    "    # pmat24_low_threshold = filtered_data[\"PMAT24_A_CR\"].quantile(.2)\n",
    "    # pmat24_high_threshold = filtered_data[\"PMAT24_A_CR\"].quantile(.8)\n",
    "\n",
    "    # psqi_low_threshold = filtered_data[\"PSQI_Score\"].quantile(.7)\n",
    "    # psqi_high_threshold = filtered_data[\"PSQI_Score\"].quantile(.3)\n",
    "\n",
    "    # ssaga_educ_low_threshold = filtered_data[\"SSAGA_Educ\"].quantile(.3)\n",
    "    # ssaga_educ_high_threshold = filtered_data[\"SSAGA_Educ\"].quantile(.7)\n",
    "\n",
    "    # lifesatisf_low_threshold = filtered_data[\"LifeSatisf_Unadj\"].quantile(.2)\n",
    "    # lifesatisf_high_threshold = filtered_data[\"LifeSatisf_Unadj\"].quantile(.8)\n",
    "\n",
    "    # DDisc_low_threshold = filtered_data[\"DDisc_AUC_200\"].quantile(.2)\n",
    "    # DDisc_high_threshold = filtered_data[\"DDisc_AUC_200\"].quantile(.8)\n",
    "\n",
    "\n",
    "    # # Handedness_low_threshold = 0\n",
    "    # Handedness_high_threshold = 0\n",
    "    # tobacco_low_threshold = 4\n",
    "    # tobacco_high_threshold = 0\n",
    "    # # Binary variables - set the thresholds manually\n",
    "    # fam_hist_threshold_high = 0\n",
    "    # fam_hist_threshold_low = 1\n",
    "\n",
    "    # thc_threshold_high = 0\n",
    "    # thc_threshold_low = 1\n",
    "\n",
    "\n",
    "    # Filtering Group 1 (high tail) and Group 2 (low tail)\n",
    "    group_1 = np.array(filtered_data[\n",
    "        # (filtered_data[\"PicVocab_AgeAdj\"] >= picvocab_high_threshold) &\n",
    "        # (filtered_data[\"PMAT24_A_CR\"] >= pmat24_high_threshold) \n",
    "        # (filtered_data[\"PSQI_Score\"] <= psqi_high_threshold) \n",
    "        # (filtered_data[\"SSAGA_FTND_Score\"] <= tobacco_high_threshold) \n",
    "        # (filtered_data[\"THC\"] <= thc_threshold_high)\n",
    "        # (filtered_data[\"LifeSatisf_Unadj\"] >= lifesatisf_high_threshold)\n",
    "        # (filtered_data[\"DDisc_AUC_200\"] >= DDisc_high_threshold)\n",
    "        (filtered_data[\"Gender\"] == 'M')\n",
    "    ]['Subject']).astype(str)\n",
    "\n",
    "    group_2 = np.array(filtered_data[\n",
    "        # (filtered_data[\"PicVocab_AgeAdj\"] <= picvocab_low_threshold) &\n",
    "        # (filtered_data[\"PMAT24_A_CR\"] <= pmat24_low_threshold) \n",
    "        # (filtered_data[\"PSQI_Score\"] >= psqi_low_threshold) \n",
    "        # (filtered_data[\"SSAGA_FTND_Score\"] >= tobacco_low_threshold) \n",
    "        # (filtered_data[\"THC\"] >= thc_threshold_low)\n",
    "        # (filtered_data[\"LifeSatisf_Unadj\"] <= lifesatisf_low_threshold)\n",
    "        # (filtered_data[\"DDisc_AUC_200\"] <= DDisc_low_threshold)\n",
    "        (filtered_data[\"Gender\"] == 'F')\n",
    "    ]['Subject']).astype(str)\n",
    "\n",
    "    # Print the number of subjects in each group\n",
    "    print(f\"Group 1 (high tail): {len(group_1)} subjects\")\n",
    "    print(f\"Group 2 (low tail): {len(group_2)} subjects\")\n",
    "\n",
    "    group_1_paths = []\n",
    "    for subject in group_1:\n",
    "        subject_data1 = os.path.join(base_directory, subject, subdirectory, folders[folder1], folders[folder1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data2 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 1], folders[folder1 + 1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data3 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 2], folders[folder1 + 2] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data4 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 3], folders[folder1 + 3] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "\n",
    "        if os.path.exists(subject_data1) and os.path.exists(subject_data2) and os.path.exists(subject_data3) and os.path.exists(subject_data4):\n",
    "            group_1_paths.append((subject_data1, subject_data2,subject_data3,subject_data4))\n",
    "    \n",
    "    group_2_paths = []\n",
    "    for subject in group_2:\n",
    "        subject_data1 = os.path.join(base_directory, subject, subdirectory, folders[folder1], folders[folder1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data2 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 1], folders[folder1 + 1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data3 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 2], folders[folder1 + 2] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data4 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 3], folders[folder1 + 3] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "\n",
    "        if os.path.exists(subject_data1) and os.path.exists(subject_data2) and os.path.exists(subject_data3) and os.path.exists(subject_data4):\n",
    "            group_2_paths.append((subject_data1, subject_data2,subject_data3,subject_data4))\n",
    "\n",
    "    \n",
    "    print(\"Length of Group 1:\", len(group_1_paths))\n",
    "    print(\"Length of Group 2:\", len(group_2_paths))\n",
    "\n",
    "    # # Determine the minimum length\n",
    "    # min_length = min(len(group_1_paths), len(group_2_paths))\n",
    "\n",
    "    # # Randomly sample from the larger group to match the size of the smaller group\n",
    "    # if len(group_1_paths) > min_length:\n",
    "    #     group_1_paths = random.sample(group_1_paths, min_length)\n",
    "    # elif len(group_2_paths) > min_length:\n",
    "    #     group_2_paths = random.sample(group_2_paths, min_length)\n",
    "\n",
    "    # Print the new sizes of both groups\n",
    "    print(\"New Length of Group 1:\", len(group_1_paths))\n",
    "    print(\"New Length of Group 2:\", len(group_2_paths))\n",
    "    return group_1_paths,group_2_paths\n",
    "\n",
    "groupA_paths,groupB_paths = load(folder1=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def process_subject(sub):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub:\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "\n",
    "        # Concatenate data along the first axis\n",
    "        subject = np.concatenate(concatenated_data, axis=0)\n",
    "        del concatenated_data  # Explicitly delete the concatenated data list\n",
    "\n",
    "        Xp = hcp.parcellate(hcp.normalize(subject - subject.mean(axis=1,keepdims=True)), hcp.mmp)\n",
    "        Xp = hcp.normalize(Xp - Xp.mean(axis=1,keepdims=True))\n",
    "        del subject  # Explicitly delete the subject array\n",
    "\n",
    "        return Xp\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None\n",
    "\n",
    "\n",
    "def parcellate(group):\n",
    "    try:\n",
    "        with ProcessPoolExecutor(max_workers=(int(os.cpu_count()*.5))) as executor:\n",
    "            # Use map to process subjects in parallel\n",
    "            group_parcellated = list(executor.map(process_subject, group))\n",
    "        \n",
    "        # Filter out any None results to continue with successful parcellations\n",
    "        group_parcellated = [result for result in group_parcellated if result is not None]\n",
    "        return group_parcellated\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in parcellation process: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    cpu_mem()  # Monitor CPU and memory usage before the operation\n",
    "    groupA_parcellated = parcellate(groupA_paths)\n",
    "    cpu_mem()  # Monitor CPU and memory usage after processing group A\n",
    "    groupB_parcellated = parcellate(groupB_paths)\n",
    "    cpu_mem()  # Monitor CPU and memory usage after processing group B\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the outer loop: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "target_shape = (4800, 379)\n",
    "# Initialize lists to collect indices of mismatched arrays\n",
    "mismatched_indices_A = []\n",
    "mismatched_indices_B = []\n",
    "\n",
    "# Create the array for group A, collecting indices of mismatches\n",
    "groupA_parcellated_array = np.array([\n",
    "    array for index, array in enumerate(groupA_parcellated) \n",
    "    if array.shape == target_shape or mismatched_indices_A.append(index)\n",
    "])\n",
    "\n",
    "# Create the array for group B, collecting indices of mismatches\n",
    "groupB_parcellated_array = np.array([\n",
    "    array for index, array in enumerate(groupB_parcellated) \n",
    "    if array.shape == target_shape or mismatched_indices_B.append(index)\n",
    "])\n",
    "# Print the indices of arrays that did not match the target shape\n",
    "print(\"Mismatched indices in group A:\", mismatched_indices_A)\n",
    "print(\"Mismatched indices in group B:\", mismatched_indices_B)\n",
    "groupA_paths_filtered = np.array([path for i, path in enumerate(groupA_paths) if i not in mismatched_indices_A])\n",
    "groupB_paths_filtered = np.array([path for i, path in enumerate(groupB_paths) if i not in mismatched_indices_B])\n",
    "print(len(groupA_parcellated_array))\n",
    "print(len(groupB_parcellated_array))\n",
    "# Save the arrays in the specified output folder\n",
    "# Example usage to save the arrays\n",
    "save_array_to_outputfolder(\"groupA_parcellated_array.npy\", groupA_parcellated_array)\n",
    "save_array_to_outputfolder(\"groupB_parcellated_array.npy\", groupB_parcellated_array)\n",
    "save_array_to_outputfolder(\"groupA_paths_filtered.npy\", groupA_paths_filtered)\n",
    "save_array_to_outputfolder(\"groupB_paths_filtered.npy\", groupB_paths_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Paths & Parcellated and Split into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupA_parcellated_array = load_array_from_outputfolder(\"groupA_parcellated_array.npy\")\n",
    "groupB_parcellated_array = load_array_from_outputfolder(\"groupB_parcellated_array.npy\")\n",
    "groupA_paths_filtered = load_array_from_outputfolder(\"groupA_paths_filtered.npy\")\n",
    "groupB_paths_filtered = load_array_from_outputfolder(\"groupB_paths_filtered.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(groupA_parcellated_array.shape)\n",
    "print(groupB_parcellated_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Linear Seperability of Groups Full Parcellated Tangent Covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample_weights(train_subject_ids, y_train):\n",
    "    \"\"\"\n",
    "    Compute sample weights based on normalized phenotype scores for the training data,\n",
    "    ensuring equal total contribution from both groups.\n",
    "    \"\"\"\n",
    "    file_path_restricted = '../HCP/RESTRICTED_zainsou_8_6_2024_2_11_21.csv'\n",
    "    file_path_unrestricted = '../HCP/unrestricted_zainsou_8_2_2024_6_13_22.csv'\n",
    "\n",
    "    try:\n",
    "        data_r = pd.read_csv(file_path_restricted)\n",
    "        data_ur = pd.read_csv(file_path_unrestricted)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path_restricted} or {file_path_unrestricted}\")\n",
    "        raise\n",
    "\n",
    "    # Combine restricted and unrestricted data on Subject ID\n",
    "    data = pd.merge(data_r, data_ur, on='Subject', how='outer')\n",
    "\n",
    "    # Convert Subject IDs to string for consistency\n",
    "    data['Subject'] = data['Subject'].astype(str)\n",
    "    train_subject_ids = train_subject_ids.astype(str)\n",
    "\n",
    "    # Filter data for training subjects\n",
    "    train_data = data[data['Subject'].isin(train_subject_ids)]\n",
    "\n",
    "    # Ensure the order matches the training data\n",
    "    train_data = train_data.set_index('Subject').loc[train_subject_ids].reset_index()\n",
    "\n",
    "    # Extract individual phenotype scores\n",
    "    pic_vocab_scores = train_data['PicVocab_AgeAdj']\n",
    "    pmat24_scores = train_data['PMAT24_A_CR']\n",
    "\n",
    "    # Normalize each phenotype score individually between 0 and 1\n",
    "    pic_vocab_min = pic_vocab_scores.min()\n",
    "    pic_vocab_max = pic_vocab_scores.max()\n",
    "    pic_vocab_norm = (pic_vocab_scores - pic_vocab_min) / (pic_vocab_max - pic_vocab_min)\n",
    "\n",
    "    pmat24_min = pmat24_scores.min()\n",
    "    pmat24_max = pmat24_scores.max()\n",
    "    pmat24_norm = (pmat24_scores - pmat24_min) / (pmat24_max - pmat24_min)\n",
    "\n",
    "    # Combine the normalized scores (e.g., by averaging)\n",
    "    phenotype_scores = (pic_vocab_norm + pmat24_norm) / 2\n",
    "\n",
    "    # Initialize sample weights array\n",
    "    sample_weights = np.zeros(len(y_train))\n",
    "\n",
    "    # Assign weights based on group labels\n",
    "    for idx, label in enumerate(y_train):\n",
    "        if label == 1:\n",
    "            # For Group 1: Higher scores ⇒ Higher weights\n",
    "            sample_weights[idx] = phenotype_scores.iloc[idx]\n",
    "        else:\n",
    "            # For Group 2: Higher scores ⇒ Lower weights\n",
    "            sample_weights[idx] = 1 - phenotype_scores.iloc[idx]\n",
    "\n",
    "    # Optional: Raise weights to a power to accentuate differences (if desired)\n",
    "    # sample_weights = sample_weights ** exponent  # Adjust the exponent as needed\n",
    "\n",
    "    # Ensure total weights for each group are equal\n",
    "    group1_indices = np.where(y_train == 1)[0]\n",
    "    group2_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "    sum_group1_weights = np.sum(sample_weights[group1_indices])\n",
    "    sum_group2_weights = np.sum(sample_weights[group2_indices])\n",
    "\n",
    "    # Compute the scaling factor for each group\n",
    "    total_weight = (sum_group1_weights + sum_group2_weights) / 2\n",
    "    scale_group1 = total_weight / sum_group1_weights if sum_group1_weights != 0 else 1\n",
    "    scale_group2 = total_weight / sum_group2_weights if sum_group2_weights != 0 else 1\n",
    "\n",
    "    # Apply scaling factors to ensure equal total weight per group\n",
    "    sample_weights[group1_indices] *= scale_group1\n",
    "    sample_weights[group2_indices] *= scale_group2\n",
    "\n",
    "    # Plotting to visualize the sample weights\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(phenotype_scores, sample_weights, c=y_train, cmap='coolwarm', alpha=0.7)\n",
    "    plt.xlabel('Normalized Combined Phenotype Score')\n",
    "    plt.ylabel('Sample Weight')\n",
    "    plt.title('Sample Weights After Scaling')\n",
    "    plt.colorbar(label='Group Label')\n",
    "    plt.show()\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "    \n",
    "def tangent_transform(group1_train_cov, group1_test_cov, group2_train_cov, group2_test_cov, metric):        \n",
    "    # Compute the Fréchet mean using only the training data from both groups\n",
    "    Frechet_Mean = mean_covariance(\n",
    "        np.concatenate((group1_train_cov, group2_train_cov)), \n",
    "        metric=metric\n",
    "    )\n",
    "    # Perform tangent space projection\n",
    "    train_1 = tangent_space(group1_train_cov, Frechet_Mean, metric=metric)\n",
    "    train_2 = tangent_space(group2_train_cov, Frechet_Mean, metric=metric)\n",
    "    test_1 = tangent_space(group1_test_cov, Frechet_Mean, metric=metric)\n",
    "    test_2 = tangent_space(group2_test_cov, Frechet_Mean, metric=metric)\n",
    "\n",
    "    return train_1, train_2, test_1, test_2\n",
    "\n",
    "def test_classifiers(train_1, test_1, train_2, test_2, sample_weights_train=None):\n",
    "    # Dictionary with keys for each classifier\n",
    "    clf_dict = {\n",
    "        \"SVM (C=1)\": SVC(kernel='linear', C=1, class_weight='balanced'),\n",
    "        \"SVM (C=0.1)\": SVC(kernel='linear', C=0.1, class_weight='balanced'),\n",
    "        \"SVM (C=0.01)\": SVC(kernel='linear', C=0.01, class_weight='balanced'),\n",
    "        \"L2 SVM (C=1)\": LinearSVC(penalty='l2',loss='squared_hinge',C=1,class_weight='balanced'),\n",
    "        \"L2 SVM (C=0.1)\": LinearSVC(penalty='l2',loss='squared_hinge',C=.1,class_weight='balanced'),\n",
    "        \"L2 SVM (C=0.01)\":  LinearSVC(penalty='l2',loss='squared_hinge',C=.01,class_weight='balanced'),\n",
    "        \"L2 SVM Hinge (C=1)\": LinearSVC(penalty='l2',loss='hinge',C=1,class_weight='balanced'),\n",
    "        \"L2 SVM Hinge (C=0.1)\": LinearSVC(penalty='l2',loss='hinge',C=.1,class_weight='balanced'),\n",
    "        \"L2 SVM Hinge (C=0.01)\":  LinearSVC(penalty='l2',loss='hinge',C=.01,class_weight='balanced'),\n",
    "        # \"L1 SVM (C=1)\": LinearSVC(penalty='l1',loss='squared_hinge',dual=False,C=1,class_weight='balanced'),\n",
    "        # \"L1 SVM (C=0.1)\": LinearSVC(penalty='l1',loss='squared_hinge',dual=False,C=.1,class_weight='balanced'),\n",
    "        # \"L1 SVM (C=0.01)\":  LinearSVC(penalty='l1',loss='squared_hinge',dual=False,C=.01,class_weight='balanced'),\n",
    "        # \"LDA\": LDA(),\n",
    "        \"Logistic Regression (default)\": LogisticRegression(),\n",
    "        \"Logistic Regression (l2)\": LogisticRegression(penalty='l2', class_weight='balanced'),\n",
    "    #     \"Logistic Regression (l1)\": LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced'),\n",
    "    #     \"Logistic Regression (elasticnet)\": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.2, class_weight='balanced')\n",
    "    }\n",
    "\n",
    "     # Combine training data and labels\n",
    "    X_train = np.concatenate((train_1, train_2))\n",
    "    y_train = np.concatenate((np.ones(len(train_1)), np.zeros(len(train_2))))\n",
    "\n",
    "    # Combine test data and labels\n",
    "    X_test = np.concatenate((test_1, test_2))\n",
    "    y_test = np.concatenate((np.ones(len(test_1)), np.zeros(len(test_2))))\n",
    "\n",
    "    # Ensure sample_weights_train aligns with X_train and y_train\n",
    "    if sample_weights_train is not None:\n",
    "        assert len(sample_weights_train) == len(y_train), \"Sample weights length mismatch.\"\n",
    "\n",
    "\n",
    "    # Calculate the distance between the two class means\n",
    "    # mean_group1_train = np.mean(group1_test, axis=0)\n",
    "    # mean_group2_train = np.mean(group2_test, axis=0)\n",
    "    # distance_vars = np.linalg.norm(mean_group1_train - mean_group2_train)\n",
    "\n",
    "    # Initialize a dictionary to store the metrics\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # Iterate through each classifier and calculate accuracy\n",
    "    for key, clf in clf_dict.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        # clf.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "\n",
    "        predictions = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        correct_predictions = np.sum(predictions == y_test)\n",
    "        total_predictions = len(y_test)\n",
    "        # Confusion matrix to get per-class accuracy\n",
    "        cm = confusion_matrix(y_test, predictions, labels=[1, 0])\n",
    "        per_class_correct = np.diag(cm)\n",
    "        per_class_total = np.sum(cm, axis=1)\n",
    "        per_class_accuracy = per_class_correct / per_class_total\n",
    "\n",
    "        metrics_dict[key] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct_predictions': correct_predictions,\n",
    "            'total_predictions': total_predictions,\n",
    "            'per_class_correct': per_class_correct,\n",
    "            'per_class_total': per_class_total,\n",
    "            'per_class_accuracy': per_class_accuracy\n",
    "        }\n",
    "    # if test_data.shape[1] == 2:\n",
    "    #     # Plot when n=1\n",
    "    #     plt.figure(figsize=(8, 6))\n",
    "    #     plt.scatter(group1_test[:, 0], group1_test[:, 1], label='Group 1 Log Variance (Test)', color='blue')\n",
    "    #     plt.scatter(group2_test[:, 0], group2_test[:, 1], label='Group 2 Log Variance (Test)', color='red')\n",
    "\n",
    "    #     # Plot the line connecting the two means\n",
    "    #     plt.plot([mean_group1_train[0], mean_group2_train[0]], [mean_group1_train[1], mean_group2_train[1]], 'k--', label=f'Mean Distance: {distance_vars:.2f}')\n",
    "\n",
    "    #     # Decision boundary\n",
    "    #     x_values = np.array([data[:, 0].min(), data[:, 0].max()])\n",
    "    #     y_values = -(max_clf.intercept_ + max_clf.coef_[0][0] * x_values) / max_clf.coef_[0][1]\n",
    "    #     plt.plot(x_values, y_values, 'g-', label='Decision Boundary')\n",
    "\n",
    "    #     # Display plot\n",
    "    #     plt.xlabel('Log Variance Feature 1')\n",
    "    #     plt.ylabel('Log Variance Feature 2')\n",
    "    #     plt.title(f'Log Variance Comparison and {max_key} Decision Boundary')\n",
    "\n",
    "    #     # Display classification accuracy on the plot\n",
    "    #     plt.text(0.05, 0.95, f'Accuracy: {accuracy:.2f}', transform=plt.gca().transAxes, fontsize=12,\n",
    "    #     verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='lightgrey'))\n",
    "\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True)\n",
    "    #     plt.show()\n",
    "    return metrics_dict\n",
    "\n",
    "    \n",
    "def test_linear_sep(groupA_parcellated_array, groupB_parcellated_array,groupA_paths, groupB_paths,metric='riemann',n_splits=10):\n",
    "    cov_est = Covariances(estimator='lwf')\n",
    "    groupA_parcellated_covs = cov_est.transform(np.transpose(groupA_parcellated_array, (0, 2, 1)))\n",
    "    groupB_parcellated_covs = cov_est.transform(np.transpose(groupB_parcellated_array, (0, 2, 1)))\n",
    "\n",
    "    data = np.concatenate((groupA_parcellated_covs, groupB_parcellated_covs))\n",
    "    labels = np.concatenate((np.ones(len(groupA_parcellated_covs)), np.zeros(len(groupB_parcellated_covs))))\n",
    "\n",
    "    all_paths = np.concatenate((groupA_paths, groupB_paths))\n",
    "    subject_ids = np.array([re.search(r'/(\\d+)/', path[0]).group(1) for path in all_paths])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(data, labels)):\n",
    "        print(f\"\\nProcessing Fold {fold + 1}/{n_splits}\")\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        group1_train_cov = data_train[labels_train == 1]\n",
    "        group2_train_cov = data_train[labels_train == 0]\n",
    "        group1_test_cov = data_test[labels_test == 1]\n",
    "        group2_test_cov = data_test[labels_test == 0]\n",
    "\n",
    "        # train_subject_ids = subject_ids[train_index]\n",
    "        # # Compute sample weights for the training data\n",
    "        # sample_weights_train = extract_sample_weights(train_subject_ids, labels_train)\n",
    "\n",
    "        # Perform tangent space transformation\n",
    "        train_1, train_2, test_1, test_2 = tangent_transform(\n",
    "            group1_train_cov, group1_test_cov,\n",
    "            group2_train_cov, group2_test_cov,\n",
    "            metric=metric \n",
    "        )\n",
    "\n",
    "        fold_metrics = test_classifiers(train_1, test_1, train_2, test_2, sample_weights_train=None)\n",
    "\n",
    "        # Initialize metrics_dict keys on first fold\n",
    "        if fold == 0:\n",
    "            for clf_name in fold_metrics.keys():\n",
    "                metrics_dict[clf_name] = {\n",
    "                    'correct_predictions': 0,\n",
    "                    'total_predictions': 0,\n",
    "                    'accuracies': [],\n",
    "                    'per_class_correct': np.array([0, 0]),\n",
    "                    'per_class_total': np.array([0, 0])\n",
    "                }\n",
    "\n",
    "        # Aggregate metrics\n",
    "        for clf_name, metrics in fold_metrics.items():\n",
    "            # Update total correct predictions and total samples\n",
    "            metrics_dict[clf_name]['correct_predictions'] += metrics['correct_predictions']\n",
    "            metrics_dict[clf_name]['total_predictions'] += metrics['total_predictions']\n",
    "            # Store accuracies for mean and std calculation\n",
    "            metrics_dict[clf_name]['accuracies'].append(metrics['accuracy'])\n",
    "            # Update per-class correct and total counts\n",
    "            metrics_dict[clf_name]['per_class_correct'] += metrics['per_class_correct']\n",
    "            metrics_dict[clf_name]['per_class_total'] += metrics['per_class_total']\n",
    "\n",
    "            # Print per-fold metrics\n",
    "            print(f\"Classifier: {clf_name}\")\n",
    "            print(f\"  Fold Accuracy: {metrics['accuracy'] * 100:.2f}%\")\n",
    "            print(f\"  Correct Predictions: {metrics['correct_predictions']}/{metrics['total_predictions']}\")\n",
    "            print(f\"  Per-Class Accuracy: {metrics['per_class_accuracy'] * 100}\")\n",
    "\n",
    "    # After all folds, compute overall metrics\n",
    "    print(\"\\nOverall Metrics Across All Folds:\")\n",
    "    for clf_name, clf_metrics in metrics_dict.items():\n",
    "        overall_accuracy = clf_metrics['correct_predictions'] / clf_metrics['total_predictions']\n",
    "        mean_accuracy = np.mean(clf_metrics['accuracies'])\n",
    "        std_accuracy = np.std(clf_metrics['accuracies'])\n",
    "        per_class_accuracy = clf_metrics['per_class_correct'] / clf_metrics['per_class_total']\n",
    "\n",
    "        print(f\"\\nClassifier: {clf_name}\")\n",
    "        print(f\"  Total Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "        print(f\"  Average Fold Accuracy: {mean_accuracy * 100:.2f}%\")\n",
    "        print(f\"  Fold Accuracy Std Dev: {std_accuracy * 100:.2f}%\")\n",
    "        print(f\"  Per-Class Accuracy: {per_class_accuracy * 100}\")\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_cov_cross_val = test_linear_sep(groupA_parcellated_array, groupB_parcellated_array,groupA_paths_filtered, groupB_paths_filtered, metric=metric,n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FKT Functions and Cross Validate Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tangent_CSP(group1_covs=None, group2_covs=None, Frechet_Mean=None,tangent_projected_1=None, tangent_projected_2=None, tangent_calc=True, metric=\"riemann\",k=1, visualize=False):\n",
    "#     if tangent_calc:\n",
    "#         all_covs = np.concatenate((group1_covs, group2_covs))\n",
    "#         Frechet_Mean = mean_covariance(all_covs, metric=metric)\n",
    "#         tangent_projected_1 = tangent_space(group1_covs, Frechet_Mean, metric=metric)\n",
    "#         tangent_projected_2 = tangent_space(group2_covs, Frechet_Mean, metric=metric)\n",
    "        \n",
    "\n",
    "#     # Initialize the Covariances estimator\n",
    "#     cov_estimator = Covariances(estimator='lwf')\n",
    "#     tangent_projected_mean = np.mean(np.concatenate((tangent_projected_1, tangent_projected_2)),axis=0,keepdims=True)  \n",
    "    \n",
    "#     tangent_1_mean = np.mean(tangent_projected_1,axis=0,keepdims=True) - tangent_projected_mean\n",
    "#     tangent_2_mean = np.mean(tangent_projected_2,axis=0,keepdims=True) - tangent_projected_mean\n",
    "#     tangent_between_scatter = tangent_1_mean.T@tangent_1_mean + tangent_2_mean.T@tangent_2_mean\n",
    "    \n",
    "#     # Estimate the covariance matrices\n",
    "#     cov_tangent_projected_1 = cov_estimator.transform(np.transpose(tangent_projected_1[np.newaxis,:,:],(0,2,1)))[0]\n",
    "#     cov_tangent_projected_2 = cov_estimator.transform(np.transpose(tangent_projected_2[np.newaxis,:,:],(0,2,1)))[0]\n",
    "    \n",
    "\n",
    "#     # Convert to PyTorch tensors and move to GPU\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print((\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "#     gpu_mem()\n",
    "#     sum = torch.tensor(cov_tangent_projected_1, dtype=torch.float32, device=\"cuda\") + torch.tensor(cov_tangent_projected_2, dtype=torch.float32, device=\"cuda\")\n",
    "#     gpu_mem()\n",
    "#     # cov_tangent_projected_1 = torch.tensor(cov_tangent_projected_1, dtype=torch.float32, device=device)\n",
    "#     # gpu_mem()\n",
    "#     # cov_tangent_projected_2 = torch.tensor(cov_tangent_projected_2, dtype=torch.float32, device=device) \n",
    "#     # gpu_mem()\n",
    "#     gpu_mem()\n",
    "\n",
    "#     tangent_between_scatter = torch.tensor(tangent_between_scatter, dtype=torch.float32, device=device)\n",
    "#     gpu_mem()\n",
    "    \n",
    "#     # For filtersA\n",
    "#     _, eigvecs = torch.lobpcg(tangent_between_scatter, B=sum, k=k, largest=True)\n",
    "#     del sum, tangent_between_scatter\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gpu_mem()\n",
    "#     eigvecs_array = eigvecs.clone().cpu().numpy()\n",
    "#     filters = untangent_space(eigvecs_array.T, Frechet_Mean)\n",
    "#     fkt_riem_eigs, filters, filtersA, filtersB = FKT(filters[0,:,:], Frechet_Mean, mean=metric, average=False, visualize=visualize)\n",
    "\n",
    "#     return fkt_riem_eigs, filters, filtersA, filtersB\n",
    "    \n",
    "def FKT(groupA_cov_matrices, groupB_cov_matrices, mean=\"riemann\", average=True, visualize=True, n=0):\n",
    "    # Eigenvalues in ascending order from scipy eigh https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html\n",
    "    if average:\n",
    "        groupA_cov = mean_covariance(groupA_cov_matrices, metric=mean)\n",
    "        groupB_cov = mean_covariance(groupB_cov_matrices, metric=mean)    \n",
    "        # eigs, filters  = eigh(groupA_cov, groupA_cov + groupB_cov + gamma*np.identity(groupB_cov.shape[0]),eigvals_only=False)\n",
    "    else:\n",
    "        groupA_cov = groupA_cov_matrices\n",
    "        groupB_cov = groupB_cov_matrices\n",
    "    if n > 0:\n",
    "        eigsA, filtersA  = eigh(groupA_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_index=[groupA_cov.shape[0]-n, groupA_cov.shape[0]-1])\n",
    "        eigsB, filtersB = eigh(groupB_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_index=[groupB_cov.shape[0]-n, groupB_cov.shape[0]-1])\n",
    "    else:\n",
    "        eigsA, filtersA  = eigh(groupA_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "        eigsB, filtersB = eigh(groupB_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "       \n",
    "    eigs = np.concatenate((eigsB[::-1], eigsA))\n",
    "    filters = np.concatenate((filtersB[:, ::-1], filtersA), axis=1)\n",
    "    fkt_riem_eigs = np.abs(np.log(eigs/(1-eigs)))**2\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(range(0,fkt_riem_eigs.shape[0]),fkt_riem_eigs)\n",
    "        plt.show()\n",
    "    return fkt_riem_eigs, filters, filtersA, filtersB\n",
    "\n",
    "def tangent_classifier(group1_covs=None, group2_covs=None, Frechet_Mean=None, tangent_projected_1=None, tangent_projected_2=None, TSVM=True, TLDA=False, tangent_calc=True,metric=\"riemann\",visualize=False,n=0):\n",
    "    if tangent_calc:\n",
    "        all_covs = np.concatenate((group1_covs, group2_covs))\n",
    "        Frechet_Mean = mean_covariance(all_covs, metric=metric)\n",
    "        tangent_projected_1 = tangent_space(group1_covs, Frechet_Mean, metric=metric)\n",
    "        tangent_projected_2 = tangent_space(group2_covs, Frechet_Mean, metric=metric)\n",
    "    # Create labels for each group\n",
    "    labels_1 = np.ones(len(tangent_projected_1))  # Labels for group 1\n",
    "    labels_2 = np.zeros(len(tangent_projected_2))   # Labels for group 2\n",
    "\n",
    "    data = np.concatenate((tangent_projected_1, tangent_projected_2))\n",
    "    labels = np.concatenate((labels_1, labels_2))\n",
    "\n",
    "    if TSVM:\n",
    "        # Create SVM classifier (adjust kernel and parameters as needed)\n",
    "        # C_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "        # clf = LinearSVC(penalty='l1',loss='squared_hinge',dual=False,C=.1,class_weight='balanced')\n",
    "        # clf =  LogisticRegression(class_weight='balanced')\n",
    "\n",
    "        # clf = LinearSVC(penalty='l2',loss='hinge',C=1,class_weight='balanced')  \n",
    "        clf = SVC(kernel='linear', C=1, class_weight='balanced')  \n",
    "        # clf = LogisticRegression()\n",
    "\n",
    "        # Train the classifier\n",
    "        clf.fit(data, labels)\n",
    "        # normalized_coef = normalize(clf.coef_, axis=1)\n",
    "        filters_SVM = untangent_space(clf.coef_, Frechet_Mean)[0,:,:]\n",
    "        fkt_riem_eigs_tangent_SVM, fkt_filters_tangent_SVM  = eigh(filters_SVM, Frechet_Mean)\n",
    "        \n",
    "        # If test data is provided, project the test data to tangent space\n",
    "        # tangent_projected_1_discrim_reconstruction = np.mean(tangent_projected_1@np.linalg.pinv(clf.coef_)@clf.coef_,axis=0)\n",
    "        # print(tangent_projected_1_discrim_reconstruction.shape)\n",
    "        # tangent_projected_2_discrim_reconstruction = np.mean(tangent_projected_2@np.linalg.pinv(clf.coef_)@clf.coef_,axis=0)\n",
    "        # print(tangent_projected_2_discrim_reconstruction.shape)\n",
    "        # group1_discrim_mean = untangent_space(tangent_projected_1_discrim_reconstruction, Frechet_Mean)\n",
    "        # print(group1_discrim_mean.shape)\n",
    "        # group2_discrim_mean = untangent_space(tangent_projected_2_discrim_reconstruction, Frechet_Mean)\n",
    "        # print(group2_discrim_mean.shape)\n",
    "        # fkt_riem_eigs_tangent_SVM, fkt_filters_tangent_SVM, filtersA, filtersB = FKT(group1_discrim_mean, group2_discrim_mean, mean=metric, average=False, visualize=visualize, n=n)\n",
    "\n",
    "\n",
    "        # Return accuracy along with filters\n",
    "        # fkt_riem_eigs_tangent_SVM, fkt_filters_tangent_SVM, filtersA, filtersB = FKT(filters_SVM[0, :, :], Frechet_Mean, mean=metric, average=False, visualize=visualize, n=n)\n",
    "        # eigs1, filters1  = eigh(filters_SVM, mean_covariance(group2_covs, metric=metric) ,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "        # eigs2, filters2 = eigh(filters_SVM, mean_covariance(group1_covs, metric=metric) ,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "        # eigs = np.concatenate((eigs2[::-1], eigs1))\n",
    "        # fkt_filters_tangent_SVM = np.concatenate((filters2[:, ::-1], filters1), axis=1)\n",
    "        # fkt_riem_eigs_tangent_SVM = np.abs(np.log(eigs))**2\n",
    "        plt.scatter(range(0,fkt_riem_eigs_tangent_SVM.shape[0]),np.abs(np.log(fkt_riem_eigs_tangent_SVM)))\n",
    "        plt.show()\n",
    "\n",
    "        return fkt_riem_eigs_tangent_SVM, fkt_filters_tangent_SVM, _, _\n",
    "\n",
    "    if TLDA:\n",
    "        # Create LDA classifier\n",
    "        lda = LDA()\n",
    "        # Train the classifier\n",
    "        lda.fit(data, labels)\n",
    "        # Get the coefficients from LDA\n",
    "        normalized_coef = normalize(lda.coef_, axis=1)\n",
    "        filters_LDA = untangent_space(normalized_coef, Frechet_Mean)\n",
    "        fkt_filters_tangent_LDA, fkt_riem_eigs_tangent_LDA, filtersA, filtersB = FKT(filters_LDA[0,:,:], Frechet_Mean, mean=metric, average=False, visualize=visualize,n=n)\n",
    "        return fkt_filters_tangent_LDA, fkt_riem_eigs_tangent_LDA, filtersA, filtersB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(group1_train,group1_test, group2_train, group2_test, filters,method='log-var',metric='riemann'):\n",
    "    group1_train_transformed = group1_train @ filters\n",
    "    group2_train_transformed = group2_train @ filters\n",
    "    group1_test_transformed = group1_test @ filters\n",
    "    group2_test_transformed = group2_test @ filters\n",
    "\n",
    "    if method == 'log-var':\n",
    "        train_1 = np.log(np.var(group1_train_transformed, axis=1))\n",
    "        train_2 = np.log(np.var(group2_train_transformed, axis=1))\n",
    "        test_1 = np.log(np.var(group1_test_transformed, axis=1))\n",
    "        test_2 = np.log(np.var(group2_test_transformed, axis=1))\n",
    "    \n",
    "    elif method == 'log-cov':\n",
    "        cov_est = Covariances(estimator='lwf')\n",
    "        train_1_cov = cov_est.transform(np.transpose(group1_train_transformed, (0, 2, 1)))\n",
    "        train_2_cov = cov_est.transform(np.transpose(group2_train_transformed, (0, 2, 1)))\n",
    "        test_1_cov = cov_est.transform(np.transpose(group1_test_transformed, (0, 2, 1)))\n",
    "        test_2_cov = cov_est.transform(np.transpose(group2_test_transformed, (0, 2, 1)))\n",
    "        train_1, train_2, test_1, test_2 = tangent_transform(train_1_cov, test_1_cov, train_2_cov, test_2_cov, metric)\n",
    "    \n",
    "    return train_1, test_1, train_2, test_2\n",
    "\n",
    "\n",
    "def validate_parcellated_filters(groupA_parcellated_array, groupB_parcellated_array, metric='riemann',n_splits=10):\n",
    "    cov_est = Covariances(estimator='lwf')\n",
    "    groupA_parcellated_covs = cov_est.transform(np.transpose(groupA_parcellated_array, (0, 2, 1)))\n",
    "    groupB_parcellated_covs = cov_est.transform(np.transpose(groupB_parcellated_array, (0, 2, 1)))\n",
    "\n",
    "    covs = np.concatenate((groupA_parcellated_covs, groupB_parcellated_covs))\n",
    "    data = np.concatenate((groupA_parcellated_array, groupB_parcellated_array))\n",
    "    labels = np.concatenate((np.ones(len(groupA_parcellated_covs)), np.zeros(len(groupB_parcellated_covs))))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(data, labels)):\n",
    "        print(f\"\\nProcessing Fold {fold + 1}/{n_splits}\")\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        covs_train, covs_test = covs[train_index], covs[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        data1_train = data_train[labels_train == 1]\n",
    "        data2_train = data_train[labels_train == 0]\n",
    "        data1_test = data_test[labels_test == 1]\n",
    "        data2_test = data_test[labels_test == 0]\n",
    "\n",
    "        cov1_train = covs_train[labels_train == 1]\n",
    "        cov2_train = covs_train[labels_train == 0]\n",
    "\n",
    "        # _, filters, _, _ = FKT(cov1_train, cov2_train, mean=metric, average=True, visualize=False, n=0)\n",
    "        _, filters, _, _ = tangent_classifier(cov1_train,  cov2_train, TSVM=True, TLDA=False, tangent_calc=True, metric=metric,visualize=False,n=0)\n",
    "\n",
    "        \n",
    "        train_1, test_1, train_2, test_2 = feature_generation(data1_train, data1_test, data2_train, data2_test, filters[:, [0,1,-2,-1]],method='log-cov',metric=metric)\n",
    "        fold_metrics = test_classifiers(train_1, test_1, train_2, test_2, sample_weights_train=None)\n",
    "        \n",
    "        if fold == 0:\n",
    "            for clf_name in fold_metrics.keys():\n",
    "                metrics_dict[clf_name] = {\n",
    "                    'correct_predictions': 0,\n",
    "                    'total_predictions': 0,\n",
    "                    'accuracies': [],\n",
    "                    'per_class_correct': np.array([0, 0]),\n",
    "                    'per_class_total': np.array([0, 0])\n",
    "                }\n",
    "\n",
    "        # Aggregate metrics\n",
    "        for clf_name, metrics in fold_metrics.items():\n",
    "            # Update total correct predictions and total samples\n",
    "            metrics_dict[clf_name]['correct_predictions'] += metrics['correct_predictions']\n",
    "            metrics_dict[clf_name]['total_predictions'] += metrics['total_predictions']\n",
    "            # Store accuracies for mean and std calculation\n",
    "            metrics_dict[clf_name]['accuracies'].append(metrics['accuracy'])\n",
    "            # Update per-class correct and total counts\n",
    "            metrics_dict[clf_name]['per_class_correct'] += metrics['per_class_correct']\n",
    "            metrics_dict[clf_name]['per_class_total'] += metrics['per_class_total']\n",
    "\n",
    "            # Print per-fold metrics\n",
    "            print(f\"Classifier: {clf_name}\")\n",
    "            print(f\"  Fold Accuracy: {metrics['accuracy'] * 100:.2f}%\")\n",
    "            print(f\"  Correct Predictions: {metrics['correct_predictions']}/{metrics['total_predictions']}\")\n",
    "            print(f\"  Per-Class Accuracy: {metrics['per_class_accuracy'] * 100}\")\n",
    "\n",
    "    # After all folds, compute overall metrics\n",
    "    print(\"\\nOverall Metrics Across All Folds:\")\n",
    "    for clf_name, clf_metrics in metrics_dict.items():\n",
    "        overall_accuracy = clf_metrics['correct_predictions'] / clf_metrics['total_predictions']\n",
    "        mean_accuracy = np.mean(clf_metrics['accuracies'])\n",
    "        std_accuracy = np.std(clf_metrics['accuracies'])\n",
    "        per_class_accuracy = clf_metrics['per_class_correct'] / clf_metrics['per_class_total']\n",
    "\n",
    "        print(f\"\\nClassifier: {clf_name}\")\n",
    "        print(f\"  Total Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "        print(f\"  Average Fold Accuracy: {mean_accuracy * 100:.2f}%\")\n",
    "        print(f\"  Fold Accuracy Std Dev: {std_accuracy * 100:.2f}%\")\n",
    "        print(f\"  Per-Class Accuracy: {per_class_accuracy * 100}\")\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters_cross_val = validate_parcellated_filters(groupA_parcellated_array, groupB_parcellated_array, metric=metric,n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 1 Fold and Filters for Rest of Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((groupA_parcellated_array, groupB_parcellated_array))\n",
    "labels = np.concatenate((np.ones(len(groupA_parcellated_array)), np.zeros(len(groupB_parcellated_array))))\n",
    "paths = np.concatenate((groupA_paths_filtered, groupB_paths_filtered))\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "splits = list(skf.split(data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "fold_outputfolder = f\"fold_{fold}\"\n",
    "if not os.path.exists(os.path.join(outputfolder, f\"fold_{fold}\")):\n",
    "    os.makedirs(os.path.join(outputfolder, f\"fold_{fold}\"))\n",
    "\n",
    "train_parcellated, test_parcellated = data[splits[fold][0]], data[splits[fold][1]]\n",
    "train_labels, test_labels = labels[splits[fold][0]], labels[splits[fold][1]]\n",
    "train_paths, test_paths = paths[splits[fold][0]], paths[splits[fold][1]]\n",
    "\n",
    "groupA_train_parcellated = train_parcellated[train_labels == 1]\n",
    "groupA_test_parcellated = test_parcellated[test_labels == 1]\n",
    "groupA_train_paths = train_paths[train_labels == 1]\n",
    "groupA_test_paths = test_paths[test_labels == 1]\n",
    "\n",
    "groupB_train_parcellated = train_parcellated[train_labels == 0]\n",
    "groupB_test_parcellated = test_parcellated[test_labels == 0]\n",
    "groupB_train_paths = train_paths[train_labels == 0]\n",
    "groupB_test_paths = test_paths[test_labels == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pyriemann.readthedocs.io/en/latest/auto_examples/signal/plot_covariance_estimation.html\n",
    "cov_est = Covariances(estimator='lwf')\n",
    "groupA_train_parcellated_covs = cov_est.transform(np.transpose(groupA_train_parcellated, (0, 2, 1)))\n",
    "groupB_train_parcellated_covs = cov_est.transform(np.transpose(groupB_train_parcellated, (0, 2, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Tangent_Class:\n",
    "    fkt_riem_eigs, filters, _, _ =  tangent_classifier(groupA_train_parcellated_covs,  groupB_train_parcellated_covs, TSVM=True, TLDA=False, tangent_calc=True, metric=metric,visualize=True,n=0)\n",
    "else:\n",
    "    fkt_riem_eigs, filters, filtersA, filtersB = FKT(groupA_train_parcellated_covs, groupB_train_parcellated_covs, mean=metric, average=True, visualize=True, n=0)\n",
    "\n",
    "filters = filters[:, [0,-1]]\n",
    "filtersA = filters[:, -1:]\n",
    "filtersB = filters[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_filters(group1_train, group1_test, group_2_train,group2_test, filters, metric=\"riemann\", method='log-cov'):\n",
    "    train_1, test_1, train_2, test_2 = feature_generation(group1_train, group1_test, group_2_train, group2_test, filters,method=method,metric=metric)\n",
    "    fold_metrics = test_classifiers(train_1, test_1, train_2, test_2, sample_weights_train=None)\n",
    "    return fold_metrics\n",
    "\n",
    "print(test_filters(groupA_train_parcellated, groupA_test_parcellated, groupB_train_parcellated, groupB_test_parcellated, filters, metric=metric,method='log-cov'))\n",
    "print(test_filters(groupA_train_parcellated, groupA_test_parcellated, groupB_train_parcellated, groupB_test_parcellated, filters, metric=metric,method='log-var'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_visualize_variance(train_1, train_2, two_filter):\n",
    "    train_1_transform = np.var(train_1@two_filter,axis=1)\n",
    "    train_2_transform = np.var(train_2@two_filter,axis=1)\n",
    "    print(train_1_transform.shape)\n",
    "    print(train_2_transform.shape)\n",
    "\n",
    "    # Plot when n=1\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(train_1_transform[:, 0], train_1_transform[:, 1], label='Group A Variance', color='blue')\n",
    "    plt.scatter(train_2_transform[:, 0], train_2_transform[:, 1], label='Group B Variance', color='red')\n",
    "\n",
    "    # Display plot\n",
    "    plt.xlabel('Variance Feature B')\n",
    "    plt.ylabel('Variance Feature A')\n",
    "    plt.title(f'Variance Comparison')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "test_visualize_variance(groupA_train_parcellated, groupB_train_parcellated,filters[:,[0,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(hcp.unparcellate(filters[:,0], hcp.mmp)), threshold=np.percentile(np.abs(filters[:,0]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MIGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migp(subs, batch_size=2, m=4800):\n",
    "    W_gpu = None\n",
    "    for batch_start in range(0, len(subs), batch_size):\n",
    "        # Select the current batch of subjects\n",
    "        batch_subs = subs[batch_start:batch_start + batch_size]\n",
    "        batch_paths = [path for sublist in batch_subs for path in sublist]\n",
    "\n",
    "        concatenated_data = []\n",
    "\n",
    "        for task in batch_paths:\n",
    "            X = nib.load(task).get_fdata()\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            # print(Xn.mean(axis=0).mean())\n",
    "            # print(Xn.std(axis=0).mean())\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "            \n",
    "        try:\n",
    "            # Concatenate data along the first axis using numpy\n",
    "            batch = np.concatenate(concatenated_data, axis=0)\n",
    "            batch = hcp.normalize(batch - batch.mean(axis=1,keepdims=True))\n",
    "            del concatenated_data\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Convert to torch tensor and move to GPU\n",
    "                batch_gpu = torch.tensor(batch, dtype=torch.float32, device=\"cuda\")\n",
    "                del batch\n",
    "                if torch.isnan(batch_gpu).any():\n",
    "                    print(\"NaNs detected in the batch data. Aborting SVD operation.\")\n",
    "                    del batch_gpu\n",
    "                    torch.cuda.empty_cache()\n",
    "                    return None\n",
    "                if W_gpu is None:\n",
    "                    combined_data_gpu = batch_gpu\n",
    "                else:\n",
    "                    combined_data_gpu = torch.cat([W_gpu, batch_gpu], dim=0)\n",
    "                del batch_gpu\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # # Calculate size in GB\n",
    "                # size_in_gb = combined_data_gpu.element_size() * combined_data_gpu.nelement() / (1024**3)\n",
    "                # print(f\"Size of the array: {size_in_gb:.2f} GB\")\n",
    "                # cpu_mem()\n",
    "                # gpu_mem()\n",
    "                # Perform SVD on the GPU\n",
    "                # Check for NaNs in the data\n",
    "\n",
    "                # _, S_gpu, Vh_gpu = torch.linalg.svd(combined_data_gpu, full_matrices=False)\n",
    "                _, Q = torch.linalg.eigh(combined_data_gpu@combined_data_gpu.T)\n",
    "                # cpu_mem()\n",
    "                # gpu_mem()\n",
    "                # Compute the updated W on the GPU\n",
    "                # W_gpu = torch.diag(S_gpu[:m]) @ Vh_gpu[:m, :]\n",
    "                # Returned in Ascending order\n",
    "                W_gpu = Q[:, -m:].T@combined_data_gpu\n",
    "                del Q, combined_data_gpu  # Free up GPU memory\n",
    "                torch.cuda.empty_cache()\n",
    "                print(batch_start, \"done\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed during GPU processing: {e}\")\n",
    "            if \"combined_data_gpu\" in locals():\n",
    "                del combined_data_gpu\n",
    "            if \"Q\" in locals():\n",
    "                del Q\n",
    "            if \"W_gpu\" in locals():\n",
    "                del W_gpu\n",
    "            torch.cuda.empty_cache()\n",
    "            return None\n",
    "\n",
    "    # Transfer W back to CPU only at the end\n",
    "    W = W_gpu.cpu().numpy()\n",
    "    del W_gpu  # Free up GPU memory\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubsA = migp((groupA_train_paths))\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,'reducedsubsA.npy'), reducedsubsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubsB = migp((groupB_train_paths))\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,'reducedsubsB.npy'), reducedsubsB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MIGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"reducedsubsA\" in locals():\n",
    "    del reducedsubsA\n",
    "if \"reducedsubsB\" in locals():\n",
    "    del reducedsubsB\n",
    "reducedsubsA_loaded = load_array_from_outputfolder(os.path.join(fold_outputfolder,'reducedsubsA.npy'))\n",
    "reducedsubsB_loaded = load_array_from_outputfolder(os.path.join(fold_outputfolder,'reducedsubsB.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubs_combined_gpu = torch.tensor(np.concatenate((reducedsubsA_loaded,reducedsubsB_loaded),axis=0),dtype=torch.float32,device=\"cuda\")\n",
    "# Returned in Descending Order\n",
    "Urc,_,_ = torch.linalg.svd(reducedsubs_combined_gpu, full_matrices=False)\n",
    "reducedsubs_combined = (Urc[:,:4800].T@reducedsubs_combined_gpu).cpu().numpy()\n",
    "del Urc, reducedsubs_combined_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubs_gpu = torch.tensor(reducedsubs_combined, dtype=torch.float32, device=\"cuda\")\n",
    "U,_,_ = torch.linalg.svd(reducedsubs_gpu, full_matrices=False)\n",
    "reducedsubs= (U[:,:1000].T@reducedsubs_gpu).cpu().numpy()\n",
    "del U, reducedsubs_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Haufe Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Should I pinv(F) or just multiply by F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_haufe(sub,pinv_TF):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub:\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "\n",
    "        # Concatenate data along the first axis\n",
    "        subject = np.concatenate(concatenated_data, axis=0)\n",
    "        del concatenated_data  # Explicitly delete the concatenated data list\n",
    "\n",
    "        Xp = hcp.normalize(subject - subject.mean(axis=1, keepdims=True))\n",
    "        del subject\n",
    "        Xpf = pinv_TF@Xp\n",
    "        del Xp\n",
    "        return Xpf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None\n",
    "\n",
    "\n",
    "def haufe_transform(F, parcellated,paths):\n",
    "    \n",
    "    # Ensure the tensors are on the correct device\n",
    "    pinv_TF = np.linalg.pinv(parcellated.reshape(-1,parcellated.shape[-1]) @ np.linalg.pinv(F.T))\n",
    "\n",
    "    pinv_TF_list = pinv_TF.reshape(len(paths),pinv_TF.shape[0],-1)\n",
    "    with ProcessPoolExecutor(max_workers=(int(os.cpu_count()*.5))) as executor:\n",
    "        # Use map to process subjects in parallel\n",
    "        blocks = np.array(list(executor.map(process_subject_haufe, paths,pinv_TF_list)))\n",
    "        print(blocks.shape)\n",
    "        return (blocks.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtersA_transform = haufe_transform(filtersA[:,-n_filters_per_group:],groupA_train_parcellated,groupA_train_paths)\n",
    "# save_array_to_outputfolder(\"filtersA_transform.npy\", filtersA_transform)\n",
    "\n",
    "filtersA_transform = haufe_transform(filtersA,groupA_train_parcellated,groupA_train_paths)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"filtersA_transform.npy\"), filtersA_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtersB_transform = haufe_transform(filtersB[:,-n_filters_per_group:],groupB_train_parcellated,groupB_train_paths)\n",
    "# save_array_to_outputfolder(\"filtersB_transform.npy\", filtersB_transform)\n",
    "\n",
    "filtersB_transform = haufe_transform(filtersB,groupB_train_parcellated,groupB_train_paths)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"filtersB_transform.npy\"), filtersB_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Haufe Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtersA_transform = load_array_from_outputfolder(os.path.join(fold_outputfolder,'filtersA_transform.npy'))\n",
    "filtersB_transform = load_array_from_outputfolder(os.path.join(fold_outputfolder,'filtersB_transform.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthonormalize Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthonormalize_filters(W1, W2):\n",
    "    # Stack the two filters into a single matrix\n",
    "    W = np.concatenate((W1, W2)).T  # shape: (features x 2)\n",
    "    print(W.shape)\n",
    "    \n",
    "    # Perform QR decomposition to orthonormalize the filters\n",
    "    Q, _ = np.linalg.qr(W)\n",
    "    \n",
    "    print(Q.shape)\n",
    "\n",
    "    # Verify that the inner product between the two orthonormalized vectors is 0 (orthogonality)\n",
    "    print(f'Inner product between Q[:, 0] and Q[:, 1]: {np.dot(Q[:, 0].T, Q[:, 1])} (should be 0)')\n",
    "    \n",
    "    # Verify that the inner product within each vector is 1 (normalization)\n",
    "    print(f'Norm of Q[:, 0]: {np.dot(Q[:, 0].T, Q[:, 0])} (should be 1)')\n",
    "    print(f'Norm of Q[:, 1]: {np.dot(Q[:, 1].T, Q[:, 1])} (should be 1)')\n",
    "    \n",
    "    return Q\n",
    "# Example usage\n",
    "\n",
    "filters = orthonormalize_filters(filtersA_transform, filtersB_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(filters[:,0]), threshold=np.percentile(np.abs(filters[:,0]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(filters[:,1]), threshold=np.percentile(np.abs(filters[:,1]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_pca_dim(Data=None,eigs=None,N=None):\n",
    "   # Start MATLAB engine\n",
    "    eng = matlab.engine.start_matlab()\n",
    "    \n",
    "    # Add the path to the MATLAB function\n",
    "    eng.addpath(\"/project/3022057.01/IFA/melodic\", nargout=0)\n",
    "    \n",
    "    if Data is not None:\n",
    "      # Call the MATLAB function\n",
    "      prob = eng.pca_dim(matlab.double(Data))\n",
    "      eig_vectors = np.array(prob['E'])\n",
    "    else:\n",
    "      prob = eng.pca_dim_eigs(matlab.double(eigs.tolist()), matlab.double([N]))\n",
    "\n",
    "    # Extract and convert each variable\n",
    "    lap = np.array(prob['lap']).flatten().reshape(-1, 1)\n",
    "    bic = np.array(prob['bic']).flatten().reshape(-1, 1)\n",
    "    rrn = np.array(prob['rrn']).flatten().reshape(-1, 1)\n",
    "    AIC = np.array(prob['AIC']).flatten().reshape(-1, 1)\n",
    "    MDL = np.array(prob['MDL']).flatten().reshape(-1, 1)\n",
    "    eig = np.array(prob['eig']).flatten()\n",
    "    orig_eig = np.array(prob['orig_eig']).flatten()\n",
    "    leig = np.array(prob['leig']).flatten()\n",
    "\n",
    "    # Stop MATLAB engine\n",
    "    eng.eval('clearvars', nargout=0)\n",
    "    eng.quit()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.arange(len(eig)),eig,label=\"Adjusted Eigenspectrum\")\n",
    "    plt.scatter(np.arange(len(orig_eig)),orig_eig,label=\"Eigenspectrum\")\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.legend()\n",
    "    plt.title('Scree Plot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Use SimpleImputer to handle any missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    lap = imputer.fit_transform(lap)\n",
    "    bic = imputer.fit_transform(bic)\n",
    "    rrn = imputer.fit_transform(rrn)\n",
    "    AIC = imputer.fit_transform(AIC)\n",
    "    MDL = imputer.fit_transform(MDL)\n",
    "    \n",
    "    # Use StandardScaler to standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    lap_std = scaler.fit_transform(lap)\n",
    "    bic_std = scaler.fit_transform(bic)\n",
    "    rrn_std = scaler.fit_transform(rrn)\n",
    "    AIC_std = scaler.fit_transform(AIC)\n",
    "    MDL_std = scaler.fit_transform(MDL)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.arange(len(lap_std)), lap_std, label='Laplacian')\n",
    "    plt.scatter(np.arange(len(bic_std)), bic_std, label='BIC')\n",
    "    plt.scatter(np.arange(len(rrn_std)), rrn_std, label='RRN')\n",
    "    plt.scatter(np.arange(len(AIC_std)), AIC_std, label='AIC')\n",
    "    plt.scatter(np.arange(len(MDL_std)), MDL_std, label='MDL')\n",
    "    \n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Standardized Value')\n",
    "    plt.legend()\n",
    "    plt.title('Scatter Plot of Standardized Eigenvalues and Model Order Selection Values')\n",
    "    plt.show()\n",
    "   \n",
    "    return np.argmax(rrn_std)+1\n",
    "\n",
    "def get_n_and_some(data):\n",
    "    # Check the shape of the data and determine the axis for mean subtraction\n",
    "\n",
    "    # Move data to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gpu = data.to(device, dtype=torch.float32)\n",
    "    groupN = data_gpu.shape[1] - 1\n",
    "\n",
    "    # Subtract the mean along the specified axis\n",
    "    data_centered = data_gpu - torch.mean(data_gpu, dim=1, keepdim=True)\n",
    "    del data_gpu  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    # Perform SVD decomposition\n",
    "    _, d, v = torch.svd(data_centered)\n",
    "    del data_centered  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert singular values to eigenvalues\n",
    "    e = (d ** 2) / groupN\n",
    "\n",
    "    # Move eigenvalues to CPU and convert to NumPy array\n",
    "    e_np = e.cpu().numpy()\n",
    "    del e, d  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Determine the number of components\n",
    "    n_components = torch.tensor(call_pca_dim(eigs=e_np, N=groupN),device=device,dtype=torch.int32)\n",
    "\n",
    "    return n_components, v.T\n",
    "\n",
    "def PPCA(data, filters=None, threshold=1.6, niters=10, n=-1):\n",
    "    n_components = -1\n",
    "    n_prev = -2\n",
    "    i = 0\n",
    "\n",
    "    # Move data to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gpu = torch.tensor(data,device=device, dtype=torch.float32)\n",
    "\n",
    "    while n_components != n_prev and i < niters:\n",
    "        n_prev = n_components\n",
    "        if filters is not None:\n",
    "            basis_gpu =  torch.tensor(filters.T,device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            n_components, vt = get_n_and_some(data_gpu)\n",
    "            if n <= 0:\n",
    "                basis_gpu = vt[:n_components, :]\n",
    "            else:\n",
    "                print(n)\n",
    "                basis_gpu = vt[:n, :]\n",
    "            del vt\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(n_prev, n_components)\n",
    "\n",
    "        # Estimate noise and residual standard deviation\n",
    "        est_noise = data_gpu - (data_gpu @ torch.linalg.pinv(basis_gpu)) @ basis_gpu\n",
    "        est_residual_std = torch.std(est_noise,dim=0,correction=torch.linalg.matrix_rank(basis_gpu))\n",
    "        del est_noise\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Normalize the data\n",
    "        data_gpu = (data_gpu / est_residual_std)\n",
    "        i += 1\n",
    "\n",
    "    data = data_gpu.cpu().numpy()\n",
    "    basis = basis_gpu.cpu().numpy()\n",
    "    # del data_gpu, basis_gpu, est_residual_std\n",
    "    del data_gpu, basis_gpu\n",
    "    torch.cuda.empty_cache()\n",
    "    return data, basis\n",
    "\n",
    "subs_data_VN, vt = PPCA(reducedsubs.copy(), threshold=0.0, niters=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"subs_data_VN.npy\"), subs_data_VN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_data_VN = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"subs_data_VN.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"vt.npy\"), vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"vt.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns are samples i.e. XXT is the covariance matrix formed\n",
    "def whiten(X,n_components, method=\"SVD\", visualize=False):\n",
    "    # -1 to account for demean\n",
    "    n_samples = X.shape[-1]-1\n",
    "    X_mean = X.mean(axis=-1)\n",
    "    X -= X_mean[:, np.newaxis]\n",
    "\n",
    "    if method == \"SVD\":\n",
    "        u, d = svd(X, full_matrices=False, check_finite=False)[:2]\n",
    "        # Give consistent eigenvectors for both svd solvers\n",
    "        # u *= np.sign(u[0])\n",
    "        K = (u / d).T[:n_components]  # see (6.33) p.140\n",
    "        del u, d\n",
    "        whitening_matrix = np.sqrt(n_samples)*K\n",
    "    elif method == \"Cholesky\":\n",
    "    # Does not Orthogonalize, just has unit covariance\n",
    "        # Step 2: Perform Cholesky decomposition\n",
    "        L = np.linalg.cholesky(np.cov(X,ddof=1))\n",
    "        # Step 3:\n",
    "        whitening_matrix = np.linalg.inv(L)\n",
    "    elif method == \"InvCov\":\n",
    "        # Calculate the covariance matrix of the centered data\n",
    "        cov_matrix = np.cov(X)\n",
    "        # Perform eigenvalue decomposition of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "        # Calculate the whitening matrix\n",
    "        D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n",
    "        whitening_matrix = eigvecs @ D_inv_sqrt @ eigvecs.T\n",
    "   \n",
    "    whitened_data = whitening_matrix@X\n",
    "\n",
    "    return whitened_data, whitening_matrix\n",
    "\n",
    "# Combine Basis\n",
    "combined_spatial = np.vstack((vt,filters.T))\n",
    "\n",
    "# Whiten\n",
    "whitened_basis, whitening_matrix_pre = whiten(combined_spatial,n_components=combined_spatial.shape[0],method=\"InvCov\",visualize=True)\n",
    "subs_data_com_VN, _ = PPCA(reducedsubs_combined.copy(), filters=whitened_basis.T, threshold=0.0, niters=1)\n",
    "\n",
    "# tempbasis = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))@subs_data_com_VN\n",
    "# whitened_basis, _ = whiten(tempbasis,n_components=tempbasis.shape[0],method=\"InvCov\",visualize=True)\n",
    "\n",
    "# for i in range(0,3):\n",
    "#     # Readjust the MiGP data based on the new basis\n",
    "#     subs_data_com_VN, _ = PPCA(subs_data_com_VN.copy(), filters=whitened_basis.T, threshold=0.0, niters=1)\n",
    "\n",
    "#     # Recalculate the basis via Haufe transform based on adjusted MIGP data\n",
    "#     tempbasis = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))@subs_data_com_VN\n",
    "\n",
    "#     # Rewhiten the basis\n",
    "#     whitened_basis, whitening_matrix = whiten(tempbasis,n_components=combined_spatial.shape[0],method=\"InvCov\",visualize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scree(data, n_components=None):\n",
    "    \"\"\"\n",
    "    Perform PCA on the provided dataset and plot a scree plot.\n",
    "\n",
    "    Parameters:\n",
    "    - data: np.array or pd.DataFrame, the dataset to perform PCA on.\n",
    "    - n_components: int or None, the number of principal components to compute. \n",
    "                    If None, all components are computed.\n",
    "\n",
    "    Returns:\n",
    "    - pca: PCA object after fitting to the data.\n",
    "    \"\"\"    \n",
    "    # # Standardize the data\n",
    "    # # Initialize PCA\n",
    "    # pca = PCA()\n",
    "    \n",
    "    # # Fit PCA on the data\n",
    "    # pca.fit(data)\n",
    "    \n",
    "    # # Calculate explained variance ratio\n",
    "    # explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    _, S, _ = np.linalg.svd(data, full_matrices=False)\n",
    "    e = (S ** 2) / (data.shape[-1]-1)\n",
    "    # Create the scree plot\n",
    "    print(e)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(S) + 1), e, marker='o', linestyle='--')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.xticks(range(1, len(S) + 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "Atemp = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))\n",
    "plot_scree(Atemp@subs_data_com_VN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICA(data,whitened_data):\n",
    "    ica = FastICA(whiten=False)\n",
    "    # Takes in array-like of shape (n_samples, n_features) and returns ndarray of shape (n_samples, n_components)\n",
    "    IFA_components = ica.fit_transform(whitened_data.T).T\n",
    "    A = data@np.linalg.pinv(IFA_components)\n",
    "    W = np.linalg.pinv(A)\n",
    "    print(\"The combined unmixing matrix correctly calculates the components: \", np.allclose(W@data, IFA_components))\n",
    "    print(\"The combined mixing matrix correctly reconstructs the low rank data_demean: \", np.allclose(A@IFA_components, A@(W@data)))\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    # Heat map for the combined unmixing matrix\n",
    "    sns.heatmap(W@data, cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Combined Unmixing Matrix (W @ data)')\n",
    "    axes[0].set_xlabel('Components')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "\n",
    "    # Heat map for the IFA components\n",
    "    sns.heatmap(IFA_components, cmap='viridis', ax=axes[1])\n",
    "    axes[1].set_title('IFA Components')\n",
    "    axes[1].set_xlabel('Components')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return IFA_components, A, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_components_combined, A_combined, W_combined = ICA(subs_data_com_VN,whitened_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtwhiten,_ = whiten(vt,n_components=vt.shape[0],method=\"SVD\")\n",
    "subs_data_VN, _ = PPCA(reducedsubs_combined.copy(), filters=vtwhiten.T, threshold=0.0, niters=1)\n",
    "raw_components_major, A_major, W_major = ICA(subs_data_VN,vtwhiten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_data_VN_more, vtmore = PPCA(reducedsubs.copy(), threshold=0.0, niters=1,n=vt.shape[0]+filters.shape[1])\n",
    "vtmorewhiten,_ = whiten(vtmore,n_components=vtmore.shape[0],method=\"SVD\")\n",
    "subs_data_VN_more, _ = PPCA(reducedsubs_combined.copy(), filters=vtmorewhiten.T, threshold=0.0, niters=1)\n",
    "\n",
    "raw_components_major_more, A_major_more, W_major_more = ICA(subs_data_VN_more,vtmorewhiten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"raw_components_combined.npy\"), raw_components_combined)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"A_combined.npy\"), A_combined)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"W_combined.npy\"), W_combined)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"raw_components_major.npy\"), raw_components_major)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"A_major.npy\"), A_major)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"W_major.npy\"), W_major)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"raw_components_major_more.npy\"), raw_components_major_more)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"A_major_more.npy\"), A_major_more)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"W_major_more.npy\"), W_major_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_components_combined = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"raw_components_combined.npy\"))\n",
    "A_combined = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"A_combined.npy\"))\n",
    "W_combined = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"W_combined.npy\"))\n",
    "raw_components_major = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"raw_components_major.npy\"))\n",
    "A_major = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"A_major.npy\"))\n",
    "W_major = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"W_major.npy\"))\n",
    "raw_components_major_more = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"raw_components_major_more.npy\"))\n",
    "A_major_more = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"A_major_more.npy\"))\n",
    "W_major_more = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"W_major_more.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_projection(W,data, visualize=True):\n",
    "\n",
    "    Signals = np.linalg.pinv(W)@(W@data)\n",
    "    Residuals = data - Signals\n",
    "    residual_std = np.std(Residuals,axis=0,ddof=np.linalg.matrix_rank(W))\n",
    "    # Trace of I-pinv(W)(W) is equal to the nullity (n-m gvien n > m) of the reconstructed matrix \n",
    "    # trace = data.shape[0] - np.linalg.matrix_rank(W)\n",
    "    # residual_std2 = (np.einsum('ij,ij->j', Residuals, Residuals)/(trace))**.5\n",
    "\n",
    "\n",
    "    if visualize:\n",
    "        n=1000\n",
    "        plt.figure()\n",
    "        plt.plot(Signals[:n,0:1])\n",
    "        plt.plot(Residuals[:n,0:1])\n",
    "        # plt.plot(data[:n,0:1])\n",
    "        # plt.plot(data[:n,0:1] - (Signals[:n,0:1]+Residuals[:n,0:1]))\n",
    "        plt.legend(['Signal','Noise', 'Data' ,'Reconstruction Error'])\n",
    "        plt.title(\"Calculations based on pinv(W)W Projection Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(range(0,residual_std.shape[0]), residual_std)\n",
    "        plt.title(\"Noise std Per Voxel based on pinv(W)W Projection Matrix\")\n",
    "        plt.show()\n",
    "    return residual_std\n",
    "\n",
    "\n",
    "def threshold_and_visualize(data, W, components,visualize=False):\n",
    "    \n",
    "    voxel_noise = noise_projection(W,data)[:, np.newaxis]\n",
    "    z_scores_array = np.zeros_like(components)\n",
    "    z_scores = np.zeros_like(components)\n",
    "\n",
    "    # Process each filter individually\n",
    "    for i in range(components.shape[1]):\n",
    "        z_score = ((components[:, i:i+1]))/voxel_noise\n",
    "        # P(Z < -z \\text{ or } Z > z) = (1 - \\text{CDF}(z)) + (1 - \\text{CDF}(z)) = 2 \\times (1 - \\text{CDF}(z))\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(z_score)))\n",
    "        # Apply multiple comparisons correction for the current filter https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "        reject, pvals_corrected, _, _ = multipletests(p_values.flatten(), alpha=0.05, method='fdr_bh')\n",
    "        masked_comp = z_score*(reject[:,np.newaxis])\n",
    "        # print(masked_comp, reject[:,np.newaxis],z_score)\n",
    "        z_scores_array[:, i:i+1] = masked_comp        \n",
    "        z_scores[:,i:i+1] = z_score\n",
    "\n",
    "       # Skip the iteration if there are no significant values\n",
    "        if not np.any(reject) and visualize:\n",
    "            print(f'Component {i} did not contain any significant values')\n",
    "            plt.figure()\n",
    "            plt.hist(z_score, bins=30, color='blue', alpha=0.7)\n",
    "            plt.title(f\"Histogram for Filter {i} NO SIGNIFICANT VALUES\")\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "        else:\n",
    "            if visualize:\n",
    "                # Create a figure and axes for subplots (1 row of 2 plots per filter)\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "                ax_hist1 = axes[0]\n",
    "                ax_img = axes[1]\n",
    "\n",
    "                # Plot the histogram of the current filter\n",
    "                ax_hist1.hist(z_score, bins=30, color='blue', alpha=0.7)\n",
    "                ax_hist1.set_title(f\"Histogram for Filter {i}\")\n",
    "                ax_hist1.set_xlabel('Value')\n",
    "                ax_hist1.set_ylabel('Frequency')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    # Heat map for the combined unmixing matrix\n",
    "    sns.heatmap(z_scores, cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('z_score')\n",
    "    axes[0].set_xlabel('Components')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "\n",
    "    # Heat map for the IFA components\n",
    "    sns.heatmap(z_scores_array, cmap='viridis', ax=axes[1])\n",
    "    axes[1].set_title('z_score thresh')\n",
    "    axes[1].set_xlabel('Components')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return z_scores, z_scores_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh, z_scores_thresh = threshold_and_visualize(subs_data_com_VN, W_combined, raw_components_combined.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh_major, z_scores_thresh_major = threshold_and_visualize(subs_data_VN, W_major, raw_components_major.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh_major_more, z_scores_thresh_major_more = threshold_and_visualize(subs_data_VN_more, W_major_more, raw_components_major_more.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh.npy\"), z_scores_unthresh)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh.npy\"), z_scores_thresh)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh_major.npy\"), z_scores_unthresh_major)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh_major.npy\"), z_scores_thresh_major)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh_major_more.npy\"), z_scores_unthresh_major_more)\n",
    "save_array_to_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh_major_more.npy\"), z_scores_thresh_major_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh.npy\"))\n",
    "z_scores_thresh = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh.npy\"))\n",
    "z_scores_unthresh_major = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh_major.npy\"))\n",
    "z_scores_thresh_major = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh_major.npy\"))\n",
    "z_scores_unthresh_major_more = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_unthresh_major_more.npy\"))\n",
    "z_scores_thresh_major_more = load_array_from_outputfolder(os.path.join(fold_outputfolder,\"z_scores_thresh_major_more.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(z_scores_unthresh[:,20]), threshold=0, bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and Save Netmats + Dual Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00115/full\n",
    "\n",
    "def calculate_netmat_and_spatial_map(Xn, z_maps):\n",
    "    \"\"\"\n",
    "    Calculate the network matrix (netmat) and spatial map for a given subject and z_maps.\n",
    "    \n",
    "    Parameters:\n",
    "    Xn (array): Time x Grayordinates normalized data matrix (Time x V)\n",
    "    z_maps (array): Grayordinates x Components map (V x C)\n",
    "\n",
    "    Returns:\n",
    "    netmat (array): Components x Components network matrix (C x C)\n",
    "    spatial_map (array): Components x Grayordinates matrix (C x V)\n",
    "    \"\"\"\n",
    "    # Time x Components\n",
    "    # Demean the regressors (z_maps)\n",
    "    z_maps_demeaned = z_maps - z_maps.mean(axis=0, keepdims=True)  # Demean the columns of z_maps (V x C)\n",
    "    \n",
    "    # Time x Components\n",
    "    A = (Xn @ np.linalg.pinv(z_maps_demeaned.T))  # A is Time x Components (T x C)\n",
    "   \n",
    "    \n",
    "    # Normalized Time x Components matrix\n",
    "    An = hcp.normalize(A)  # An is Time x Components (T x C)\n",
    "    del A\n",
    "\n",
    "    # Components x Components network matrix\n",
    "    netmat = (An.T @ An) / (Xn.shape[0] - 1)  # Netmat is Components x Components (C x C)\n",
    "\n",
    "    # Components x Grayordinates spatial map\n",
    "    spatial_map = np.linalg.pinv(An) @ Xn  # Spatial map is Components x Grayordinates (C x V)\n",
    "\n",
    "    return An, netmat, spatial_map\n",
    "\n",
    "def dual_regress_sub(sub_path, z_maps_1, z_maps_2):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub_path:\n",
    "            # Load and preprocess each task\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)  # Grayordinates x Time (V x T)\n",
    "            Xn = hcp.normalize(X - X.mean(axis=1, keepdims=True))  # Normalizing (V x T)\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "        \n",
    "        # Concatenate data along the first axis (all tasks into one big matrix)\n",
    "        subject = np.concatenate(concatenated_data, axis=0)  # Time x Grayordinates (T x V)\n",
    "        del concatenated_data\n",
    "        \n",
    "        # Normalize the concatenated data\n",
    "        Xn = hcp.normalize(subject - subject.mean(axis=1,keepdims=True))  # Time x Grayordinates normalized data (T x V)\n",
    "        del subject\n",
    "        \n",
    "        # Calculate netmat and spatial map for the first set of z_maps\n",
    "        An_1, netmat_1, spatial_map_1 = calculate_netmat_and_spatial_map(Xn, z_maps_1)\n",
    "\n",
    "        # Calculate netmat and spatial map for the second set of z_maps\n",
    "        An_2, netmat_2, spatial_map_2 = calculate_netmat_and_spatial_map(Xn, z_maps_2)\n",
    "\n",
    "        return (An_1, netmat_1, spatial_map_1), (An_2, netmat_2, spatial_map_2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def dual_regress(group_paths, z_maps_1, z_maps_2):\n",
    "    # Use partial to avoid duplicating z_maps in memory\n",
    "    with ProcessPoolExecutor(max_workers=int(os.cpu_count() * 0.7)) as executor:\n",
    "        # Create a partial function that \"binds\" the z_maps_1 and z_maps_2 without duplicating them\n",
    "        partial_func = partial(dual_regress_sub, z_maps_1=z_maps_1, z_maps_2=z_maps_2)\n",
    "\n",
    "        # Pass the subject paths to the executor without copying z_maps\n",
    "        results = list(executor.map(partial_func, group_paths))\n",
    "        \n",
    "        # Separate the results for the two bases, collecting An, netmat, and spatial_map\n",
    "        An_1, netmats_1, spatial_maps_1 = zip(*[(res[0][0], res[0][1], res[0][2]) for res in results if res[0] is not None])\n",
    "        An_2, netmats_2, spatial_maps_2 = zip(*[(res[1][0], res[1][1], res[1][2]) for res in results if res[1] is not None])\n",
    "\n",
    "        return (np.array(An_1), np.array(netmats_1), np.array(spatial_maps_1)), (np.array(An_2), np.array(netmats_2), np.array(spatial_maps_2))\n",
    "\n",
    "# Save function for An, netmats, and spatial maps\n",
    "def save_numpy_arrays(output_prefix, An_1, netmats_1, spatial_maps_1, An_2, netmats_2, spatial_maps_2):\n",
    "    \"\"\"\n",
    "    Saves the An arrays, netmats, and spatial maps to disk using np.save.\n",
    "    \n",
    "    Parameters:\n",
    "    output_prefix (str): Prefix for the output files.\n",
    "    An_1 (np.array): Time x Components matrix for z_maps_1.\n",
    "    netmats_1 (np.array): Network matrices for z_maps_1.\n",
    "    spatial_maps_1 (np.array): Spatial maps for z_maps_1.\n",
    "    An_2 (np.array): Time x Components matrix for z_maps_2.\n",
    "    netmats_2 (np.array): Network matrices for z_maps_2.\n",
    "    spatial_maps_2 (np.array): Spatial maps for z_maps_2.\n",
    "    \"\"\"\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_An_1.npy\", An_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_netmats_1.npy\", netmats_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_spatial_maps_1.npy\", spatial_maps_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_An_2.npy\", An_2)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_netmats_2.npy\", netmats_2)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_spatial_maps_2.npy\", spatial_maps_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group A - Training Set\n",
    "(groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train), (groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train) = dual_regress(groupA_train_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupA_train\", groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train, groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group A - Test Set\n",
    "(groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test), (groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test) = dual_regress(groupA_test_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupA_test\", groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test, groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group B - Training Set\n",
    "(groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train), (groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train) = dual_regress(groupB_train_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupB_train\", groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train, groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group B - Test Set\n",
    "(groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test), (groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test) = dual_regress(groupB_test_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupB_test\", groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test, groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group A - Training Set\n",
    "(groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train), (groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train) = dual_regress(groupA_train_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(os.path.join(fold_outputfolder,\"groupA_train\"), groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train, groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train)\n",
    "# For Group A - Test Set\n",
    "(groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test), (groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test) = dual_regress(groupA_test_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(os.path.join(fold_outputfolder,\"groupA_test\"), groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test, groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test)\n",
    "# For Group B - Training Set\n",
    "(groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train), (groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train) = dual_regress(groupB_train_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(os.path.join(fold_outputfolder,\"groupB_train\"), groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train, groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train)\n",
    "# For Group B - Test Set\n",
    "(groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test), (groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test) = dual_regress(groupB_test_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(os.path.join(fold_outputfolder,\"groupB_test\"), groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test, groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Netmats + Dual Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load function for An, netmats, and spatial maps\n",
    "def load_numpy_arrays(input_prefix):\n",
    "    \"\"\"\n",
    "    Loads the An arrays, netmats, and spatial maps from disk using load_array_from_outputfolder.\n",
    "    \n",
    "    Parameters:\n",
    "    input_prefix (str): Prefix for the input files.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Six numpy arrays (An_1, netmats_1, spatial_maps_1, An_2, netmats_2, spatial_maps_2).\n",
    "    \"\"\"\n",
    "    An_1 = load_array_from_outputfolder(f\"{input_prefix}_An_1.npy\")\n",
    "    netmats_1 = load_array_from_outputfolder(f\"{input_prefix}_netmats_1.npy\")\n",
    "    spatial_maps_1 = load_array_from_outputfolder(f\"{input_prefix}_spatial_maps_1.npy\")\n",
    "    An_2 = load_array_from_outputfolder(f\"{input_prefix}_An_2.npy\")\n",
    "    netmats_2 = load_array_from_outputfolder(f\"{input_prefix}_netmats_2.npy\")\n",
    "    spatial_maps_2 = load_array_from_outputfolder(f\"{input_prefix}_spatial_maps_2.npy\")\n",
    "    \n",
    "    return An_1, netmats_1, spatial_maps_1, An_2, netmats_2, spatial_maps_2\n",
    "\n",
    "# Example usage for loading Group A train and test results\n",
    "groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train, groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train = load_numpy_arrays(os.path.join(fold_outputfolder,\"groupA_train\"))\n",
    "groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test, groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test = load_numpy_arrays(os.path.join(fold_outputfolder,\"groupA_test\"))\n",
    "\n",
    "# Example usage for loading Group B train and test results\n",
    "groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train, groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train = load_numpy_arrays(os.path.join(fold_outputfolder,\"groupB_train\"))\n",
    "groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test, groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test = load_numpy_arrays(os.path.join(fold_outputfolder,\"groupB_test\"))\n",
    "\n",
    "# Sanity check for Group A train data\n",
    "print(\"Group A Train:\")\n",
    "print(groupA_An_1_train.shape, groupA_netmats_1_train.shape, groupA_spatial_maps_1_train.shape)\n",
    "print(groupA_An_2_train.shape, groupA_netmats_2_train.shape, groupA_spatial_maps_2_train.shape)\n",
    "\n",
    "# Sanity check for Group A test data\n",
    "print(\"Group A Test:\")\n",
    "print(groupA_An_1_test.shape, groupA_netmats_1_test.shape, groupA_spatial_maps_1_test.shape)\n",
    "print(groupA_An_2_test.shape, groupA_netmats_2_test.shape, groupA_spatial_maps_2_test.shape)\n",
    "\n",
    "# Sanity check for Group B train data\n",
    "print(\"Group B Train:\")\n",
    "print(groupB_An_1_train.shape, groupB_netmats_1_train.shape, groupB_spatial_maps_1_train.shape)\n",
    "print(groupB_An_2_train.shape, groupB_netmats_2_train.shape, groupB_spatial_maps_2_train.shape)\n",
    "\n",
    "# Sanity check for Group B test data\n",
    "print(\"Group B Test:\")\n",
    "print(groupB_An_1_test.shape, groupB_netmats_1_test.shape, groupB_spatial_maps_1_test.shape)\n",
    "print(groupB_An_2_test.shape, groupB_netmats_2_test.shape, groupB_spatial_maps_2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Discriminant Information via Netmats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netmat(group_data,basis):\n",
    "    A = ((group_data@np.linalg.pinv(basis.T)))\n",
    "    # Normalized Time x Components matrix\n",
    "    An = hcp.normalize(A)  # An is Time x Components (T x C)\n",
    "    del A\n",
    "\n",
    "    timepoints = An.shape[0]\n",
    "    group_netmat = (An.T@An)/(timepoints-1)\n",
    "    return group_netmat\n",
    "\n",
    "def group_dist(group_data1,group_data2,basis,metric=\"riemann\"):\n",
    "    netmat1 = netmat(group_data1,basis)\n",
    "    netmat2 = netmat(group_data2,basis)\n",
    "    print(\"Distance between Group Netmats:\", distance(netmat1,netmat2,metric=metric))\n",
    "\n",
    "group_dist(reducedsubsA_loaded,reducedsubsB_loaded,z_scores_unthresh_major,metric=metric)\n",
    "group_dist(reducedsubsA_loaded,reducedsubsB_loaded,z_scores_unthresh_major_more,metric=metric)\n",
    "group_dist(reducedsubsA_loaded,reducedsubsB_loaded,z_scores_unthresh,metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5662067\n",
    "def var_diff(group1_train, group1_test, group1_cov_train,group1_cov_test,group2_train, group2_test, group2_cov_train, group2_cov_test, metric):\n",
    "    # clf = SVC(kernel='linear', class_weight='balanced')\n",
    "    # clf = SVC(kernel='linear', C=1,class_weight='balanced')\n",
    "    # clf = SVC(kernel='linear', C=.1,class_weight='balanced')\n",
    "    # clf = SVC(kernel='linear', C=.01,class_weight='balanced')\n",
    "    # clf = LDA()\n",
    "    clf = LogisticRegression()\n",
    "    # clf = LogisticRegression(penalty='l2',class_weight='balanced')\n",
    "    # clf = LogisticRegression(penalty='l1',solver='saga',class_weight='balanced')\n",
    "    # clf = LogisticRegression(penalty='elasticnet',solver='saga',l1_ratio=0.5,class_weight='balanced')\n",
    "\n",
    "\n",
    "    \n",
    "    # Compute the mean covariances using the training data only\n",
    "    group1_mean = mean_covariance(group1_cov_train, metric=metric)\n",
    "    group2_mean = mean_covariance(group2_cov_train, metric=metric)\n",
    "\n",
    "    # Eigen decomposition to get features\n",
    "    _, feature_all = eigh(group1_mean, group2_mean + group2_mean, eigvals_only=False)\n",
    "\n",
    "    # _, feature_all, _, _ = tangent_classifier(group1_cov_train,group1_cov_test,  group2_cov_train, group2_cov_test, TSVM=True, TLDA=False, tangent_calc=True, metric=metric,visualize=True,n=0)\n",
    "\n",
    "\n",
    "    # Initialize list to store results (accuracy and distance)\n",
    "    results = []\n",
    "\n",
    "    # Loop from n=1 to n=15 for selecting top and bottom eigenvectors\n",
    "    for n in range(1, 15):\n",
    "        # Perform eigen decomposition based on top and bottom n eigenvectors\n",
    "        features = np.hstack([feature_all[:, :n], feature_all[:, -n:]])  # Select top and bottom n eigenvectors\n",
    "        group1_train_transformed = group1_train @ features\n",
    "        group2_train_transformed = group2_train @ features\n",
    "        group1_test_transformed = group1_test @ features\n",
    "        group2_test_transformed = group2_test @ features\n",
    "\n",
    "        # Calculate log variance for both groups\n",
    "        group1_train_logvar = np.log(np.var(group1_train_transformed, axis=1))\n",
    "        group2_train_logvar = np.log(np.var(group2_train_transformed, axis=1))\n",
    "        group1_test_logvar = np.log(np.var(group1_test_transformed, axis=1))\n",
    "        group2_test_logvar = np.log(np.var(group2_test_transformed, axis=1))\n",
    "\n",
    "        # Prepare the dataset for classification\n",
    "        X_train = np.vstack([group1_train_logvar, group2_train_logvar])\n",
    "        y_train = np.hstack([np.zeros(group1_train_logvar.shape[0]), np.ones(group2_train_logvar.shape[0])])\n",
    "        X_test = np.vstack([group1_test_logvar, group2_test_logvar])\n",
    "        y_test = np.hstack([np.zeros(group1_test_logvar.shape[0]), np.ones(group2_test_logvar.shape[0])])\n",
    "\n",
    "        # Train logistic regression classifier on training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test data and calculate accuracy\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Calculate class means for distance (using the training data)\n",
    "        mean_group1_test = np.mean(group1_test_logvar, axis=0)\n",
    "        mean_group2_test = np.mean(group2_test_logvar, axis=0)\n",
    "\n",
    "        # Calculate the distance between the two class means\n",
    "        distance_vars = np.linalg.norm(mean_group1_test - mean_group2_test)\n",
    "\n",
    "        # Store accuracy and Riemannian distance for this n\n",
    "        results.append((n, distance_vars, accuracy))\n",
    "        # Plot when n=1\n",
    "        if n == 1:\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(group1_test_logvar[:, 0], group1_test_logvar[:, 1], label='Group 1 Log Variance (Test)', color='blue')\n",
    "            plt.scatter(group2_test_logvar[:, 0], group2_test_logvar[:, 1], label='Group 2 Log Variance (Test)', color='red')\n",
    "\n",
    "            # Plot the line connecting the two means\n",
    "            plt.plot([mean_group1_test[0], mean_group2_test[0]], [mean_group1_test[1], mean_group2_test[1]], 'k--', label=f'Mean Distance: {distance_vars:.2f}')\n",
    "\n",
    "            # Decision boundary\n",
    "            x_values = np.array([X_train[:, 0].min(), X_train[:, 0].max()])\n",
    "            y_values = -(clf.intercept_ + clf.coef_[0][0] * x_values) / clf.coef_[0][1]\n",
    "            plt.plot(x_values, y_values, 'g-', label='Decision Boundary')\n",
    "\n",
    "            # Display plot\n",
    "            plt.xlabel('Log Variance Feature 1')\n",
    "            plt.ylabel('Log Variance Feature 2')\n",
    "            plt.title('Log Variance Comparison and Logistic Regression Decision Boundary')\n",
    "\n",
    "            # Display classification accuracy on the plot\n",
    "            plt.text(0.05, 0.95, f'Accuracy: {accuracy:.2f}', transform=plt.gca().transAxes, fontsize=12,\n",
    "                     verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='lightgrey'))\n",
    "\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    # Return the list of accuracies and distances for each n\n",
    "    return results\n",
    "\n",
    "# Updated tangent_class_test function\n",
    "def tangent_class_test(group1_cov_train, group1_cov_test, group2_cov_train, group2_cov_test, metric):\n",
    "    clf = SVC(kernel='linear', C=.01,class_weight='balanced')\n",
    "    # clf = LDA()\n",
    "    # clf = LogisticRegression()\n",
    "\n",
    "    # Compute the mean covariance using ONLY the training data\n",
    "    group_Mean = mean_covariance(np.concatenate((group1_cov_train, group2_cov_train)), metric=metric)\n",
    "\n",
    "    # Project the training covariances into the tangent space\n",
    "    tangent_projected_1_train = tangent_space(group1_cov_train, group_Mean, metric=metric)\n",
    "    tangent_projected_2_train = tangent_space(group2_cov_train, group_Mean, metric=metric)\n",
    "\n",
    "    # Project the test covariances into the tangent space\n",
    "    tangent_projected_1_test = tangent_space(group1_cov_test, group_Mean, metric=metric)\n",
    "    tangent_projected_2_test = tangent_space(group2_cov_test, group_Mean, metric=metric)\n",
    "\n",
    "    # Combine the tangent projections for training and testing\n",
    "    X_train = np.vstack((tangent_projected_1_train, tangent_projected_2_train))\n",
    "    X_test = np.vstack((tangent_projected_1_test, tangent_projected_2_test))\n",
    "    y_train = np.hstack((np.zeros(tangent_projected_1_train.shape[0]), np.ones(tangent_projected_2_train.shape[0])))\n",
    "    y_test = np.hstack((np.zeros(tangent_projected_1_test.shape[0]), np.ones(tangent_projected_2_test.shape[0])))\n",
    "\n",
    "    # Dimensionality reduction\n",
    "    max_dim = np.min((X_train.shape[0], X_train.shape[1]))\n",
    "    dims = [2, 3, int((max_dim-1)/20), int((max_dim-1)/17), int((max_dim-1)/15),\n",
    "            int((max_dim-1)/13), int((max_dim-1)/12), int((max_dim-1)/10), \n",
    "            int((max_dim-1)/7), int((max_dim-1)/5), int((max_dim-1)/3), \n",
    "            int((max_dim-1)/2), int((max_dim-1)/1.7), int((max_dim-1)/1.5), \n",
    "            int((max_dim-1)/1.3), int((max_dim-1)/1.1), max_dim-1]\n",
    "\n",
    "    logistic_accuracies = []\n",
    "    mean_dist_array = []\n",
    "    for i in dims:\n",
    "        # Reduce dimensionality using PCA\n",
    "        pca = PCA(n_components=i)\n",
    "        X_train_reduced = pca.fit_transform(X_train)\n",
    "        X_test_reduced = pca.transform(X_test)\n",
    "        mean_dist = np.linalg.norm(np.mean(pca.transform(tangent_projected_1_test),axis=0) - np.mean(pca.transform(tangent_projected_2_test),axis=0))\n",
    "        mean_dist_array.append(mean_dist)\n",
    "        # Train logistic regression classifier\n",
    "        clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "        # Test accuracy\n",
    "        y_pred = clf.predict(X_test_reduced)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        logistic_accuracies.append(test_accuracy)\n",
    "\n",
    "    return dims, logistic_accuracies, mean_dist_array\n",
    "\n",
    "def mean_diff(group1_covs_red,group2_covs_red,metric):\n",
    "    group_1 = mean_covariance(group1_covs_red, metric=metric)\n",
    "    group_2 = mean_covariance(group2_covs_red, metric=metric)\n",
    "    return distance(group_1,group_2,metric=metric)\n",
    "\n",
    "   \n",
    "def PSD_diff_all(group1_train, group1_test, group1_cov_train, group1_cov_test, group2_train, group2_test, group2_cov_train, group2_cov_test, metric):\n",
    "\n",
    "    group1_covs_red = cov_est.transform(np.transpose(group1_cov_test, (0, 2, 1)))\n",
    "    group2_covs_red = cov_est.transform(np.transpose(group2_cov_test, (0, 2, 1)))\n",
    "    psd_mean_distance = mean_diff(group1_covs_red, group2_covs_red, metric)\n",
    "    dims, accuracies, mean_dist_array = tangent_class_test(group1_cov_train, group1_cov_test, group2_cov_train, group2_cov_test, metric)\n",
    "    fkt_results = var_diff(group1_train, group1_test, group1_cov_train,group1_cov_test, group2_train, group2_test, group2_cov_train, group2_cov_train, metric)\n",
    "\n",
    "\n",
    "    result = {\n",
    "        \"psd_mean_distance\": psd_mean_distance,\n",
    "        \"dims\": dims,\n",
    "        \"accuracies\": accuracies,\n",
    "        \"mean_dist\": mean_dist_array,\n",
    "        \"var_diff_n_distance_accuracy\": fkt_results\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first comparison (using Group 1 data):\n",
    "IFA_result = PSD_diff_all(groupA_An_1_train, groupA_An_1_test, groupA_netmats_1_train, groupA_netmats_1_test, groupB_An_1_train, groupB_An_1_test, groupB_netmats_1_train, groupB_netmats_1_test, metric=metric)\n",
    "\n",
    "# For the second comparison (using Group 2 data):\n",
    "major_result = PSD_diff_all(groupA_An_2_train, groupA_An_2_test, groupA_netmats_2_train, groupA_netmats_2_test, groupB_An_2_train, groupB_An_2_test, groupB_netmats_2_train, groupB_netmats_2_test, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_with_lines(x1, y1, x2, y2, label1='Series 1', label2='Series 2', xlabel='X', ylabel='Y', title='Scatter Plot with Connecting Lines'):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot with lines connecting corresponding points from two series.\n",
    "\n",
    "    Parameters:\n",
    "    - x1, y1: The x and y values for the first series.\n",
    "    - x2, y2: The x and y values for the second series.\n",
    "    - label1, label2: Labels for the two series.\n",
    "    - xlabel, ylabel: Labels for the x and y axes.\n",
    "    - title: Title for the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,6))  # Set the size of the figure\n",
    "    plt.scatter(x1, y1, label=label1, color='blue')\n",
    "    plt.scatter(x2, y2, label=label2, color='orange')\n",
    "    \n",
    "    # Draw lines connecting corresponding points\n",
    "    for x_1, y_1, x_2, y_2 in zip(x1, y1, x2, y2):\n",
    "        plt.plot([x_1, x_2], [y_1, y_2], color='gray', linestyle='--')\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print(IFA_result[\"psd_mean_distance\"], major_result[\"psd_mean_distance\"])\n",
    "# Example usage for your first plot\n",
    "scatter_with_lines(IFA_result[\"dims\"], IFA_result[\"accuracies\"],\n",
    "                   major_result[\"dims\"], major_result[\"accuracies\"],\n",
    "                   label1='IFA Components', label2='ICA Components',\n",
    "                   xlabel='Dimension', ylabel='Accuracy',\n",
    "                   title='Tangent Classifier on Tangent Netmats Reduced via PCA')\n",
    "\n",
    "scatter_with_lines(IFA_result[\"dims\"], IFA_result[\"mean_dist\"],\n",
    "                   major_result[\"dims\"], major_result[\"mean_dist\"],\n",
    "                   label1='IFA Components', label2='ICA Components',\n",
    "                   xlabel='Dimension', ylabel='Tangent Distance',\n",
    "                   title='Difference of of Class Means Tangent Netmats Reduced via PCA')\n",
    "# Example usage for your second plot\n",
    "scatter_with_lines([tup[0]*2 for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   [tup[2] for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   [tup[0]*2 for tup in major_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   [tup[2] for tup in major_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   label1='IFA Components', label2='ICA Components',\n",
    "                   xlabel='Dimension', ylabel='Accuracy',\n",
    "                   title='Linear Classifier Accuracy of Netmats Transformed via FKT Filters')\n",
    "\n",
    "# Example usage for your third plot\n",
    "scatter_with_lines([tup[0]*2 for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   [tup[1] for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   [tup[0]*2 for tup in major_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   [tup[1] for tup in major_result[\"var_diff_n_distance_accuracy\"]],\n",
    "                   label1='IFA Components', label2='ICA Components',\n",
    "                   xlabel='Dimension', ylabel='Distance',\n",
    "                   title='Distance Between Means of Group Netmats Transformed via FKT Filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupA_train_1_tangent, groupB_train_1_tangent, groupA_test_1_tangent, groupB_test_1_tangent = tangent_transform(groupA_netmats_1_train, groupA_netmats_1_test, groupB_netmats_1_train, groupB_netmats_1_test, metric=metric)\n",
    "groupA_train_2_tangent, groupB_train_2_tangent, groupA_test_2_tangent, groupB_test_2_tangent = tangent_transform(groupA_netmats_2_train, groupA_netmats_2_test, groupB_netmats_2_train, groupB_netmats_2_test, metric=metric)\n",
    "\n",
    "IFA_Class_Result = test_classifiers(group1_train=groupA_train_1_tangent, group1_test=groupA_test_1_tangent, group2_train=groupB_train_1_tangent, group2_test=groupB_test_1_tangent)\n",
    "ICA_Class_Result = test_classifiers(group1_train=groupA_train_2_tangent, group1_test=groupA_test_2_tangent, group2_train=groupB_train_2_tangent, group2_test=groupB_test_2_tangent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_with_lines_dict(result1, result2, label1='Series 1', label2='Series 2', \n",
    "                            xlabel='Classifier', ylabel='Accuracy', title='Accuracy Comparison'):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot with lines connecting corresponding points from two dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    - result1: Dictionary containing classifiers and accuracies for the first series.\n",
    "    - result2: Dictionary containing classifiers and accuracies for the second series.\n",
    "    - label1, label2: Labels for the two series.\n",
    "    - xlabel, ylabel: Labels for the x and y axes.\n",
    "    - title: Title for the plot.\n",
    "    \"\"\"\n",
    "    # Extract classifier names\n",
    "    classifiers1 = list(result1.keys())\n",
    "    classifiers2 = list(result2.keys())\n",
    "\n",
    "    # Ensure both dictionaries have the same classifiers\n",
    "    assert classifiers1 == classifiers2, \"The classifiers (keys) must match between the two result dictionaries.\"\n",
    "\n",
    "    # Extract accuracy values\n",
    "    accuracies1 = [metrics['accuracy'] for metrics in result1.values()]\n",
    "    accuracies2 = [metrics['accuracy'] for metrics in result2.values()]\n",
    "    \n",
    "    # Debug: Print extracted accuracies\n",
    "    print(\"Accuracies for Series 1:\", accuracies1)\n",
    "    print(\"Accuracies for Series 2:\", accuracies2)\n",
    "    \n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Number of classifiers\n",
    "    num_classifiers = len(classifiers1)\n",
    "    x_positions = range(num_classifiers)\n",
    "    \n",
    "    # Create scatter plots\n",
    "    plt.scatter(x_positions, accuracies1, label=label1, color='blue', s=100)\n",
    "    plt.scatter(x_positions, accuracies2, label=label2, color='orange', s=100)\n",
    "    \n",
    "    # Draw lines connecting corresponding points\n",
    "    for i in range(num_classifiers):\n",
    "        plt.plot([x_positions[i], x_positions[i]], [accuracies1[i], accuracies2[i]], \n",
    "                 color='gray', linestyle='--')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel(xlabel, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    \n",
    "    # Set x-ticks to classifier names\n",
    "    plt.xticks(x_positions, classifiers1, rotation=45, ha='right', fontsize=12)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust layout to ensure everything fits\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "scatter_with_lines_dict(IFA_Class_Result, ICA_Class_Result, label1='IFA', label2='ICA', title='Accuracy Comparison Between Tangent Space Classification of Netmats formed via IFA and ICA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z Scores on Dual Regressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Assuming group1 and group2 are numpy arrays with shapes:\n",
    "# group1.shape = (subjects1, components, grayordinates)\n",
    "# group2.shape = (subjects2, components, grayordinates)\n",
    "\n",
    "# Define the number of permutations\n",
    "n_permutations = 10  # Adjust based on computational resources\n",
    "\n",
    "# Function to perform t-test for a specific component\n",
    "def perm_ttest_for_component(component_idx, data_group1, data_group2, n_permutations):\n",
    "    # Extract data for this component from both groups\n",
    "    group1_comp_data = data_group1[:, component_idx, :]  # Shape: (subjects1, grayordinates)\n",
    "    group2_comp_data = data_group2[:, component_idx, :]  # Shape: (subjects2, grayordinates)\n",
    "\n",
    "    # Perform the permutation t-test for each grayordinate\n",
    "    t_stat, p_val = ttest_ind(\n",
    "        group1_comp_data, group2_comp_data, axis=0, equal_var=True, nan_policy='omit',\n",
    "        permutations=n_permutations, random_state=None, alternative='two-sided'\n",
    "    )\n",
    "    return t_stat, p_val\n",
    "\n",
    "# Use Parallel to parallelize over components\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(perm_ttest_for_component)(component_idx, groupA_spatial_maps_1_train, groupB_spatial_maps_1_train, n_permutations)\n",
    "    for component_idx in range(groupA_spatial_maps_1_train.shape[1])\n",
    ")\n",
    "\n",
    "# Unpack results\n",
    "t_statistics, p_values = zip(*results)\n",
    "t_statistics = np.array(t_statistics)  # Shape: (components, grayordinates)\n",
    "p_values = np.array(p_values)          # Shape: (components, grayordinates)\n",
    "\n",
    "# # Flatten p-values for multiple comparison correction\n",
    "# p_values_flat = p_values.flatten()\n",
    "\n",
    "# # Apply FDR correction\n",
    "# _, p_values_corrected, _, _ = multipletests(p_values_flat, method='fdr_bh')\n",
    "\n",
    "# # Reshape the corrected p-values back to the original (components, grayordinates) shape\n",
    "# p_values_corrected = p_values_corrected.reshape(p_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_flat = p_values.flatten()\n",
    "reject,p_values_corrected,_,_  = multipletests(p_values_flat, method='fdr_bh')\n",
    "p_values_corrected = p_values_corrected.reshape(p_values.shape)\n",
    "reject = reject.reshape(p_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(p_values_corrected[20,:]), threshold=0, bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_values_corrected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupA_spatial_maps_1_train[10,20,:]), threshold=np.percentile(np.abs(groupA_spatial_maps_1_train[10,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupA_spatial_maps_1_train[20,20,:]), threshold=np.percentile(np.abs(groupA_spatial_maps_1_train[20,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupB_spatial_maps_1_train[11,20,:]), threshold=np.percentile(np.abs(groupB_spatial_maps_1_train[11,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupB_spatial_maps_1_train[1,20,:]), threshold=np.percentile(np.abs(groupB_spatial_maps_1_train[1,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(z_scores_unthresh[:,20]), threshold=np.percentile(np.abs(z_scores_unthresh[:,20]), 95), bg_map=hcp.mesh.sulc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
