{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as cp\n",
    "import torch\n",
    "import hcp_utils as hcp # https://rmldj.github.io/hcp-utils/\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matlab.engine\n",
    "import os\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.linalg import eigh, svd\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from nilearn import image as nimg\n",
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "from pyriemann.utils.tangentspace import tangent_space, untangent_space, log_map_riemann, unupper\n",
    "from pyriemann.utils.distance import distance_riemann, distance\n",
    "from pyriemann.utils.base import logm, expm\n",
    "from concurrent.futures import ProcessPoolExecutor, TimeoutError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder\n",
    "outputfolder = \"PicVocab_AgeAdj_FKT_logeuclid\"\n",
    "\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)\n",
    "def load_array_from_outputfolder(filename):\n",
    "    filepath = os.path.join(outputfolder, filename)\n",
    "    return np.load(filepath)\n",
    "# Function to save an array to the output folder\n",
    "def save_array_to_outputfolder(filename, array):\n",
    "    filepath = os.path.join(outputfolder, filename)\n",
    "    np.save(filepath, array)\n",
    "\n",
    "n_filters_per_group = 1\n",
    "Tangent_Class = False\n",
    "Tangent_CSP = False\n",
    "riem_filters = True\n",
    "all_subs = True\n",
    "concatenate_spatial_bases = True\n",
    "# Pyriemannian Mean https://github.com/pyRiemann/pyRiemann/blob/master/pyriemann/utils/mean.py#L633 Metric for mean estimation, can be: \"ale\", \"alm\", \"euclid\", \"harmonic\", \"identity\", \"kullback_sym\", \"logdet\", \"logeuclid\", \"riemann\", \"wasserstein\", or a callable function.\n",
    "# https://link.springer.com/article/10.1007/s12021-020-09473-9 <---- best descriptions/plots\n",
    "# Geometric means in a novel vector space structure on symmetric positive-definite matrices <https://epubs.siam.org/doi/abs/10.1137/050637996?journalCode=sjmael>`_\n",
    "metric = \"logeuclid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory and Processor Usage/Limits Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\n",
    "#Open terminal for job\n",
    "# srun --jobid=68974 --overlap --pty /bin/bash \n",
    "\n",
    "# #SLURM RAM\n",
    "!cgget -r memory.limit_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "#SLURM VM\n",
    "!cgget -r memory.memsw.limit_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "#SLURM USAGE\n",
    "!cgget -r memory.memsw.usage_in_bytes /slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID\n",
    "\n",
    "!echo \"SLURM_JOB_ID: $SLURM_JOB_ID\"\n",
    "!echo \"SLURM_JOB_NAME: $SLURM_JOB_NAME\"\n",
    "!echo \"SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST\"\n",
    "!echo \"SLURM_MEM_PER_NODE: $SLURM_MEM_PER_NODE\"\n",
    "!echo \"SLURM_CPUS_ON_NODE: $SLURM_CPUS_ON_NODE\"\n",
    "!echo \"SLURM_MEM_PER_CPU: $SLURM_MEM_PER_CPU\"\n",
    "\n",
    "!free -h\n",
    "\n",
    "import resource\n",
    "\n",
    "# Get the soft and hard limits of virtual memory (address space)\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "print(f\"Soft limit: {soft / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Hard limit: {hard / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "# Get the soft and hard limits of the data segment (physical memory usage)\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_DATA)\n",
    "print(f\"Soft limit: {soft / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Hard limit: {hard / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "#TORQUE Virtual Memory\n",
    "# !cgget -r memory.memsw.limit_in_bytes /torque/$PBS_JOBID\n",
    "\n",
    "# #TORQUE RAM\n",
    "# !cgget -r memory.limit_in_bytes /torque/$PBS_JOBID\n",
    "\n",
    "# #TORQUE USAGE\n",
    "# !cgget -r memory.memsw.usage_in_bytes /torque/$PBS_JOBID\n",
    "# print(int(os.environ['PBS_NP']))\n",
    "!nvidia-smi\n",
    "\n",
    "def gpu_mem():\n",
    "    # Memory usage information\n",
    "    print(f\"Total memory available: {(torch.cuda.get_device_properties('cuda').total_memory / 1024**3):.2f} GB\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "def cpu_mem():\n",
    "   # Display memory information\n",
    "    print(f\"Total Memory: { psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available Memory: { psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used Memory: { psutil.virtual_memory().used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory Usage: { psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Paths, Parcellate, Standardize, and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(folder1=0):\n",
    "    \"\"\"\n",
    "    Load data for a specified number of subjects and fMRI tasks, only if they have not been parcellated.\n",
    "    \"\"\"\n",
    "\n",
    "    base_directory = \"/project_cephfs/3022017.01/S1200\"\n",
    "    subdirectory = \"MNINonLinear/Results\"\n",
    "    \n",
    "    folders = [\n",
    "        \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\", \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "        \"tfMRI_EMOTION_LR\", \"tfMRI_EMOTION_RL\", \"tfMRI_GAMBLING_LR\", \"tfMRI_GAMBLING_RL\",\n",
    "        \"tfMRI_LANGUAGE_LR\", \"tfMRI_LANGUAGE_RL\", \"tfMRI_MOTOR_LR\", \"tfMRI_MOTOR_RL\",\n",
    "        \"tfMRI_RELATIONAL_LR\", \"tfMRI_RELATIONAL_RL\", \"tfMRI_SOCIAL_LR\", \"tfMRI_SOCIAL_RL\",\n",
    "        \"tfMRI_WM_LR\", \"tfMRI_WM_RL\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    if folder1 + 1 >= len(folders):\n",
    "        raise IndexError(f\"Invalid folder1 index: {folder1}. Check folder list.\")\n",
    "\n",
    "    if folder1 + 2 >= len(folders):\n",
    "        raise IndexError(f\"Invalid folder1 index: {folder1}. Check folder list.\")\n",
    "\n",
    "    if folder1 + 3 >= len(folders):\n",
    "        raise IndexError(f\"Invalid folder1 index: {folder1}. Check folder list.\")\n",
    "\n",
    "    subids = [sub for sub in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, sub))]\n",
    "\n",
    "    file_path_restricted = r'../HCP/RESTRICTED_zainsou_8_6_2024_2_11_21.csv'\n",
    "    file_path_unrestricted = r'../HCP/unrestricted_zainsou_8_2_2024_6_13_22.csv'\n",
    "\n",
    "    try:\n",
    "        # Load the data from CSV files\n",
    "        data_r = pd.read_csv(file_path_restricted)\n",
    "        data_ur = pd.read_csv(file_path_unrestricted)\n",
    "        print(\"Files loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path_restricted} or {file_path_unrestricted}\")\n",
    "        raise\n",
    "\n",
    "    # Combine restricted and unrestricted data on Subject ID\n",
    "    data = pd.merge(data_r, data_ur, on='Subject', how='outer')\n",
    "\n",
    "    filtered_data = data[data['Subject'].astype(str).isin(subids)]\n",
    "\n",
    "    picvocab_low_threshold = filtered_data[\"PicVocab_AgeAdj\"].quantile(.1)\n",
    "    picvocab_high_threshold = filtered_data[\"PicVocab_AgeAdj\"].quantile(0.9)\n",
    "\n",
    "    # Filtering Group 1 (high tail) and Group 2 (low tail)\n",
    "    group_1 = np.array(filtered_data[\n",
    "        (filtered_data[\"PicVocab_AgeAdj\"] >= picvocab_high_threshold)\n",
    "    ]['Subject']).astype(str)\n",
    "\n",
    "    group_2 = np.array(filtered_data[\n",
    "        (filtered_data[\"PicVocab_AgeAdj\"] <= picvocab_low_threshold)\n",
    "    ]['Subject']).astype(str)\n",
    "\n",
    "    # Print the number of subjects in each group\n",
    "    print(f\"Group 1 (high tail): {len(group_1)} subjects\")\n",
    "    print(f\"Group 2 (low tail): {len(group_2)} subjects\")\n",
    "\n",
    "    group_1_paths = []\n",
    "    for subject in group_1:\n",
    "        subject_data1 = os.path.join(base_directory, subject, subdirectory, folders[folder1], folders[folder1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data2 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 1], folders[folder1 + 1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data3 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 2], folders[folder1 + 2] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data4 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 3], folders[folder1 + 3] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "\n",
    "        if os.path.exists(subject_data1) and os.path.exists(subject_data2) and os.path.exists(subject_data3) and os.path.exists(subject_data4):\n",
    "            group_1_paths.append((subject_data1, subject_data2,subject_data3,subject_data4))\n",
    "    \n",
    "    group_2_paths = []\n",
    "    for subject in group_2:\n",
    "        subject_data1 = os.path.join(base_directory, subject, subdirectory, folders[folder1], folders[folder1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data2 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 1], folders[folder1 + 1] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data3 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 2], folders[folder1 + 2] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "        subject_data4 = os.path.join(base_directory, subject, subdirectory, folders[folder1 + 3], folders[folder1 + 3] + \"_Atlas_MSMAll_hp2000_clean.dtseries.nii\")\n",
    "\n",
    "        if os.path.exists(subject_data1) and os.path.exists(subject_data2) and os.path.exists(subject_data3) and os.path.exists(subject_data4):\n",
    "            group_2_paths.append((subject_data1, subject_data2,subject_data3,subject_data4))\n",
    "\n",
    "    \n",
    "    print(\"Length of Group 1:\", len(group_1_paths))\n",
    "    print(\"Length of Group 2:\", len(group_2_paths))\n",
    "    return group_1_paths,group_2_paths\n",
    "\n",
    "groupA_paths,groupB_paths = load(folder1=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def process_subject(sub):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub:\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "\n",
    "        # Concatenate data along the first axis\n",
    "        subject = np.concatenate(concatenated_data, axis=0)\n",
    "        del concatenated_data  # Explicitly delete the concatenated data list\n",
    "\n",
    "        Xp = hcp.parcellate(subject, hcp.mmp)\n",
    "        Xp = hcp.normalize(Xp - Xp.mean(axis=1,keepdims=True))\n",
    "        del subject  # Explicitly delete the subject array\n",
    "\n",
    "        return Xp\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None\n",
    "\n",
    "\n",
    "def parcellate(group):\n",
    "    try:\n",
    "        with ProcessPoolExecutor(max_workers=(int(os.cpu_count()*.75))) as executor:\n",
    "            # Use map to process subjects in parallel\n",
    "            group_parcellated = list(executor.map(process_subject, group))\n",
    "        \n",
    "        # Filter out any None results to continue with successful parcellations\n",
    "        group_parcellated = [result for result in group_parcellated if result is not None]\n",
    "        return group_parcellated\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in parcellation process: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    cpu_mem()  # Monitor CPU and memory usage before the operation\n",
    "    groupA_parcellated = parcellate(groupA_paths)\n",
    "    cpu_mem()  # Monitor CPU and memory usage after processing group A\n",
    "    groupB_parcellated = parcellate(groupB_paths)\n",
    "    cpu_mem()  # Monitor CPU and memory usage after processing group B\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the outer loop: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "target_shape = (4800, 379)\n",
    "# Initialize lists to collect indices of mismatched arrays\n",
    "mismatched_indices_A = []\n",
    "mismatched_indices_B = []\n",
    "\n",
    "# Create the array for group A, collecting indices of mismatches\n",
    "groupA_parcellated_array = np.array([\n",
    "    array for index, array in enumerate(groupA_parcellated) \n",
    "    if array.shape == target_shape or mismatched_indices_A.append(index)\n",
    "])\n",
    "\n",
    "# Create the array for group B, collecting indices of mismatches\n",
    "groupB_parcellated_array = np.array([\n",
    "    array for index, array in enumerate(groupB_parcellated) \n",
    "    if array.shape == target_shape or mismatched_indices_B.append(index)\n",
    "])\n",
    "# Print the indices of arrays that did not match the target shape\n",
    "print(\"Mismatched indices in group A:\", mismatched_indices_A)\n",
    "print(\"Mismatched indices in group B:\", mismatched_indices_B)\n",
    "groupA_paths_filtered = np.array([path for i, path in enumerate(groupA_paths) if i not in mismatched_indices_A])\n",
    "groupB_paths_filtered = np.array([path for i, path in enumerate(groupB_paths) if i not in mismatched_indices_B])\n",
    "print(len(groupA_parcellated_array))\n",
    "print(len(groupB_parcellated_array))\n",
    "# Save the arrays in the specified output folder\n",
    "# Example usage to save the arrays\n",
    "save_array_to_outputfolder(\"groupA_parcellated_array.npy\", groupA_parcellated_array)\n",
    "save_array_to_outputfolder(\"groupB_parcellated_array.npy\", groupB_parcellated_array)\n",
    "save_array_to_outputfolder(\"groupA_paths_filtered.npy\", groupA_paths_filtered)\n",
    "save_array_to_outputfolder(\"groupB_paths_filtered.npy\", groupB_paths_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Paths & Parcellated and Split into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupA_parcellated_array = load_array_from_outputfolder(\"groupA_parcellated_array.npy\")\n",
    "groupB_parcellated_array = load_array_from_outputfolder(\"groupB_parcellated_array.npy\")\n",
    "groupA_paths_filtered = load_array_from_outputfolder(\"groupA_paths_filtered.npy\")\n",
    "groupB_paths_filtered = load_array_from_outputfolder(\"groupB_paths_filtered.npy\")\n",
    "\n",
    "# Check if arrays and paths have the same length for safety\n",
    "assert len(groupA_parcellated_array) == len(groupA_paths_filtered), \"Mismatch between groupA parcellated array and paths\"\n",
    "assert len(groupB_parcellated_array) == len(groupB_paths_filtered), \"Mismatch between groupB parcellated array and paths\"\n",
    "\n",
    "# Split Group A into train and test sets\n",
    "groupA_train_parcellated, groupA_test_parcellated, groupA_train_paths, groupA_test_paths = train_test_split(\n",
    "    groupA_parcellated_array, groupA_paths_filtered, test_size=0.3, random_state=None\n",
    ")\n",
    "\n",
    "# Split Group B into train and test sets\n",
    "groupB_train_parcellated, groupB_test_parcellated, groupB_train_paths, groupB_test_paths = train_test_split(\n",
    "    groupB_parcellated_array, groupB_paths_filtered, test_size=0.3, random_state=None\n",
    ")\n",
    "\n",
    "# Print sizes of train and test splits\n",
    "print(f\"Group A train size: {len(groupA_train_parcellated)}, test size: {len(groupA_test_parcellated)}\")\n",
    "print(f\"Group B train size: {len(groupB_train_parcellated)}, test size: {len(groupB_test_parcellated)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form Parcellated Connectomes and Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pyriemann.readthedocs.io/en/latest/auto_examples/signal/plot_covariance_estimation.html\n",
    "cov_est = Covariances(estimator='lwf')\n",
    "\n",
    "# Compute the covariance matrices for preprocessed_GroupA and preprocessed_GroupB\n",
    "groupA_train_parcellated_covs = cov_est.transform(np.transpose(groupA_train_parcellated, (0, 2, 1)))\n",
    "groupB_train_parcellated_covs = cov_est.transform(np.transpose(groupB_train_parcellated, (0, 2, 1)))\n",
    "print(groupA_train_parcellated_covs.shape, groupB_train_parcellated_covs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKT(groupA_cov_matrices, groupB_cov_matrices, mean=\"riemann\", average=True, visualize=True, n=0):\n",
    "    # Eigenvalues in ascending order from scipy eigh https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html\n",
    "    if average:\n",
    "        groupA_cov = mean_covariance(groupA_cov_matrices, metric=mean)\n",
    "        groupB_cov = mean_covariance(groupB_cov_matrices, metric=mean)    \n",
    "        # eigs, filters  = eigh(groupA_cov, groupA_cov + groupB_cov + gamma*np.identity(groupB_cov.shape[0]),eigvals_only=False)\n",
    "    else:\n",
    "        groupA_cov = groupA_cov_matrices\n",
    "        groupB_cov = groupB_cov_matrices\n",
    "    if n > 0:\n",
    "        eigsA, filtersA  = eigh(groupA_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_index=[groupA_cov.shape[0]-n, groupA_cov.shape[0]-1])\n",
    "        eigsB, filtersB = eigh(groupB_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_index=[groupB_cov.shape[0]-n, groupB_cov.shape[0]-1])\n",
    "    else:\n",
    "        eigsA, filtersA  = eigh(groupA_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "        eigsB, filtersB = eigh(groupB_cov, groupA_cov + groupB_cov,eigvals_only=False,subset_by_value=[0.5,np.inf])\n",
    "       \n",
    "    eigs = np.concatenate((eigsA[::-1], eigsB))\n",
    "    filters = np.concatenate((filtersB[:, ::-1], filtersA), axis=1)\n",
    "    fkt_riem_eigs = np.abs(np.log(eigs/(1-eigs)))**2\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(range(0,fkt_riem_eigs.shape[0]),fkt_riem_eigs)\n",
    "        plt.show()\n",
    "    \n",
    "    return fkt_riem_eigs, filters, filtersA, filtersB\n",
    "\n",
    "def tangent_classifier(group1_covs=None, group2_covs=None, Frechet_Mean=None, tangent_projected_1=None, tangent_projected_2=None, TSVM=True, TLDA=False, tangent_calc=True,metric=\"riemann\",visualize=False,n=0):\n",
    "    if tangent_calc:\n",
    "        all_covs = np.concatenate((group1_covs, group2_covs))\n",
    "        Frechet_Mean = mean_covariance(all_covs, metric=metric)\n",
    "        tangent_projected_1 = tangent_space(group1_covs, Frechet_Mean, metric=metric)\n",
    "        tangent_projected_2 = tangent_space(group2_covs, Frechet_Mean, metric=metric)\n",
    "    # Create labels for each group\n",
    "    labels_1 = np.ones(len(tangent_projected_1))  # Labels for group 1\n",
    "    labels_2 = np.zeros(len(tangent_projected_2))   # Labels for group 2\n",
    "\n",
    "    data = np.concatenate((tangent_projected_1, tangent_projected_2))\n",
    "    labels = np.concatenate((labels_1, labels_2))\n",
    "\n",
    "    if TSVM:\n",
    "        # Create SVM classifier (adjust kernel and parameters as needed)\n",
    "        clf = LinearSVC(penalty='l1',loss='squared_hinge',dual=False, C=.1)  \n",
    "        # clf = SVC(kernel='linear', C=1)  \n",
    "        # clf = SVC(kernel='linear')  \n",
    "        # Train the classifier\n",
    "        clf.fit(data, labels)\n",
    "        normalized_coef = normalize(clf.coef_, axis=1)\n",
    "        filters_SVM = untangent_space(normalized_coef, Frechet_Mean)\n",
    "        fkt_filters_tangent_SVM, fkt_riem_eigs_tangent_SVM, filtersA, filtersB = FKT(filters_SVM[0,:,:], Frechet_Mean, mean=metric, average=False, visualize=visualize,n=n)\n",
    "        return fkt_filters_tangent_SVM, fkt_riem_eigs_tangent_SVM, filtersA, filtersB\n",
    "\n",
    "    if TLDA:\n",
    "        # Create LDA classifier\n",
    "        lda = LDA()\n",
    "        # Train the classifier\n",
    "        lda.fit(data, labels)\n",
    "        # Get the coefficients from LDA\n",
    "        normalized_coef = normalize(lda.coef_, axis=1)\n",
    "        filters_LDA = untangent_space(normalized_coef, Frechet_Mean)\n",
    "        fkt_filters_tangent_LDA, fkt_riem_eigs_tangent_LDA, filtersA, filtersB = FKT(filters_LDA[0,:,:], Frechet_Mean, mean=metric, average=False, visualize=visualize,n=n)\n",
    "        return fkt_filters_tangent_LDA, fkt_riem_eigs_tangent_LDA, filtersA, filtersB\n",
    "\n",
    "# \n",
    "if Tangent_Class:\n",
    "    fkt_riem_eigs, filters, filtersA, filtersB = tangent_classifier(groupA_train_parcellated_covs, groupB_train_parcellated_covs, TSVM=True, tangent_calc=True, metric=metric,visualize=True,n=0)\n",
    "else:\n",
    "   fkt_riem_eigs, filters, filtersA, filtersB = FKT(groupA_train_parcellated_covs, groupB_train_parcellated_covs, mean=metric, average=True, visualize=True, n=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MIGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migp(subs, batch_size=2, m=4800):\n",
    "    W_gpu = None\n",
    "    for batch_start in range(0, len(subs), batch_size):\n",
    "        # Select the current batch of subjects\n",
    "        batch_subs = subs[batch_start:batch_start + batch_size]\n",
    "        batch_paths = [path for sublist in batch_subs for path in sublist]\n",
    "\n",
    "        concatenated_data = []\n",
    "\n",
    "        for task in batch_paths:\n",
    "            X = nib.load(task).get_fdata()\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            # print(Xn.mean(axis=0).mean())\n",
    "            # print(Xn.std(axis=0).mean())\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "            \n",
    "        try:\n",
    "            # Concatenate data along the first axis using numpy\n",
    "            batch = np.concatenate(concatenated_data, axis=0)\n",
    "            batch = hcp.normalize(batch - batch.mean(axis=1,keepdims=True))\n",
    "            del concatenated_data\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Convert to torch tensor and move to GPU\n",
    "                batch_gpu = torch.tensor(batch, dtype=torch.float32, device=\"cuda\")\n",
    "                del batch\n",
    "                if torch.isnan(batch_gpu).any():\n",
    "                    print(\"NaNs detected in the batch data. Aborting SVD operation.\")\n",
    "                    del batch_gpu\n",
    "                    torch.cuda.empty_cache()\n",
    "                    return None\n",
    "                if W_gpu is None:\n",
    "                    combined_data_gpu = batch_gpu\n",
    "                else:\n",
    "                    combined_data_gpu = torch.cat([W_gpu, batch_gpu], dim=0)\n",
    "                del batch_gpu\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # # Calculate size in GB\n",
    "                # size_in_gb = combined_data_gpu.element_size() * combined_data_gpu.nelement() / (1024**3)\n",
    "                # print(f\"Size of the array: {size_in_gb:.2f} GB\")\n",
    "                # cpu_mem()\n",
    "                # gpu_mem()\n",
    "                # Perform SVD on the GPU\n",
    "                # Check for NaNs in the data\n",
    "\n",
    "                # _, S_gpu, Vh_gpu = torch.linalg.svd(combined_data_gpu, full_matrices=False)\n",
    "                _, Q = torch.linalg.eigh(combined_data_gpu@combined_data_gpu.T)\n",
    "                # cpu_mem()\n",
    "                # gpu_mem()\n",
    "                # Compute the updated W on the GPU\n",
    "                # W_gpu = torch.diag(S_gpu[:m]) @ Vh_gpu[:m, :]\n",
    "                # Returned in Ascending order\n",
    "                W_gpu = Q[:, -m:].T@combined_data_gpu\n",
    "                del Q, combined_data_gpu  # Free up GPU memory\n",
    "                torch.cuda.empty_cache()\n",
    "                print(batch_start, \"done\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed during GPU processing: {e}\")\n",
    "            if \"combined_data_gpu\" in locals():\n",
    "                del combined_data_gpu\n",
    "            if \"Q\" in locals():\n",
    "                del Q\n",
    "            if \"W_gpu\" in locals():\n",
    "                del W_gpu\n",
    "            torch.cuda.empty_cache()\n",
    "            return None\n",
    "\n",
    "    # Transfer W back to CPU only at the end\n",
    "    W = W_gpu.cpu().numpy()\n",
    "    del W_gpu  # Free up GPU memory\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubsA = migp((groupA_train_paths))\n",
    "save_array_to_outputfolder('reducedsubsA.npy', reducedsubsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubsB = migp((groupB_train_paths))\n",
    "save_array_to_outputfolder('reducedsubsB.npy', reducedsubsB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MIGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"reducedsubsA\" in locals():\n",
    "    del reducedsubsA\n",
    "if \"reducedsubsB\" in locals():\n",
    "    del reducedsubsB\n",
    "reducedsubsA_loaded = load_array_from_outputfolder('reducedsubsA.npy')\n",
    "reducedsubsB_loaded = load_array_from_outputfolder('reducedsubsB.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubs_combined_gpu = torch.tensor(np.concatenate((reducedsubsA_loaded,reducedsubsB_loaded),axis=0),dtype=torch.float32,device=\"cuda\")\n",
    "# Returned in Descending Order\n",
    "Urc,_,_ = torch.linalg.svd(reducedsubs_combined_gpu, full_matrices=False)\n",
    "reducedsubs_combined = (Urc[:,:4800].T@reducedsubs_combined_gpu).cpu().numpy()\n",
    "del Urc, reducedsubs_combined_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedsubs_gpu = torch.tensor(reducedsubs_combined, dtype=torch.float32, device=\"cuda\")\n",
    "U,_,_ = torch.linalg.svd(reducedsubs_gpu, full_matrices=False)\n",
    "reducedsubs= (U[:,:1000].T@reducedsubs_gpu).cpu().numpy()\n",
    "del U, reducedsubs_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Haufe Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Should I pinv(F) or just multiply by 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_haufe(sub,pinv_TF):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub:\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)\n",
    "            Xn = hcp.normalize(X-X.mean(axis=1, keepdims=True))\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "\n",
    "        # Concatenate data along the first axis\n",
    "        subject = np.concatenate(concatenated_data, axis=0)\n",
    "        del concatenated_data  # Explicitly delete the concatenated data list\n",
    "\n",
    "        Xp = hcp.normalize(subject - subject.mean(axis=1, keepdims=True))\n",
    "        del subject\n",
    "        Xpf = pinv_TF@Xp\n",
    "        del Xp\n",
    "        return Xpf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None\n",
    "\n",
    "\n",
    "def haufe_transform(F, parcellated,paths):\n",
    "    \n",
    "    # Ensure the tensors are on the correct device\n",
    "    pinv_TF = np.linalg.pinv(parcellated.reshape(-1,parcellated.shape[-1]) @ F)\n",
    "    pinv_TF_list = pinv_TF.reshape(len(paths),pinv_TF.shape[0],-1)\n",
    "    with ProcessPoolExecutor(max_workers=(int(os.cpu_count()*.5))) as executor:\n",
    "        # Use map to process subjects in parallel\n",
    "        blocks = np.array(list(executor.map(process_subject_haufe, paths,pinv_TF_list)))\n",
    "        print(blocks.shape)\n",
    "        return (blocks.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtersA_transform = haufe_transform(filtersA[:,-n_filters_per_group:],groupA_train_parcellated,groupA_train_paths)\n",
    "save_array_to_outputfolder(\"filtersA_transform.npy\", filtersA_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtersB_transform = haufe_transform(filtersB[:,-n_filters_per_group:],groupB_train_parcellated,groupB_train_paths)\n",
    "save_array_to_outputfolder(\"filtersB_transform.npy\", filtersB_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Haufe Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtersA_transform = load_array_from_outputfolder('filtersA_transform.npy')\n",
    "filtersB_transform = load_array_from_outputfolder('filtersB_transform.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthonormalize Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthonormalize_filters(W1, W2):\n",
    "    # Stack the two filters into a single matrix\n",
    "    W = np.concatenate((W1, W2)).T  # shape: (features x 2)\n",
    "    print(W.shape)\n",
    "    \n",
    "    # Perform QR decomposition to orthonormalize the filters\n",
    "    Q, _ = np.linalg.qr(W)\n",
    "    \n",
    "    print(Q.shape)\n",
    "\n",
    "    # Verify that the inner product between the two orthonormalized vectors is 0 (orthogonality)\n",
    "    print(f'Inner product between Q[:, 0] and Q[:, 1]: {np.dot(Q[:, 0].T, Q[:, 1])} (should be 0)')\n",
    "    \n",
    "    # Verify that the inner product within each vector is 1 (normalization)\n",
    "    print(f'Norm of Q[:, 0]: {np.dot(Q[:, 0].T, Q[:, 0])} (should be 1)')\n",
    "    print(f'Norm of Q[:, 1]: {np.dot(Q[:, 1].T, Q[:, 1])} (should be 1)')\n",
    "    \n",
    "    return Q\n",
    "# Example usage\n",
    "\n",
    "filters = orthonormalize_filters(filtersA_transform, filtersB_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(filtersA_transform[0,:]), threshold=np.percentile(np.abs(filtersA_transform[0,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(filtersB_transform[0,:]), threshold=np.percentile(np.abs(filtersB_transform[0,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_pca_dim(Data=None,eigs=None,N=None):\n",
    "   # Start MATLAB engine\n",
    "    eng = matlab.engine.start_matlab()\n",
    "    \n",
    "    # Add the path to the MATLAB function\n",
    "    eng.addpath(\"/project/3022057.01/IFA/melodic\", nargout=0)\n",
    "    \n",
    "    if Data is not None:\n",
    "      # Call the MATLAB function\n",
    "      prob = eng.pca_dim(matlab.double(Data))\n",
    "      eig_vectors = np.array(prob['E'])\n",
    "    else:\n",
    "      prob = eng.pca_dim_eigs(matlab.double(eigs.tolist()), matlab.double([N]))\n",
    "\n",
    "    # Extract and convert each variable\n",
    "    lap = np.array(prob['lap']).flatten().reshape(-1, 1)\n",
    "    bic = np.array(prob['bic']).flatten().reshape(-1, 1)\n",
    "    rrn = np.array(prob['rrn']).flatten().reshape(-1, 1)\n",
    "    AIC = np.array(prob['AIC']).flatten().reshape(-1, 1)\n",
    "    MDL = np.array(prob['MDL']).flatten().reshape(-1, 1)\n",
    "    eig = np.array(prob['eig']).flatten()\n",
    "    orig_eig = np.array(prob['orig_eig']).flatten()\n",
    "    leig = np.array(prob['leig']).flatten()\n",
    "\n",
    "    # Stop MATLAB engine\n",
    "    eng.eval('clearvars', nargout=0)\n",
    "    eng.quit()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.arange(len(eig)),eig,label=\"Adjusted Eigenspectrum\")\n",
    "    plt.scatter(np.arange(len(orig_eig)),orig_eig,label=\"Eigenspectrum\")\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.legend()\n",
    "    plt.title('Scree Plot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Use SimpleImputer to handle any missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    lap = imputer.fit_transform(lap)\n",
    "    bic = imputer.fit_transform(bic)\n",
    "    rrn = imputer.fit_transform(rrn)\n",
    "    AIC = imputer.fit_transform(AIC)\n",
    "    MDL = imputer.fit_transform(MDL)\n",
    "    \n",
    "    # Use StandardScaler to standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    lap_std = scaler.fit_transform(lap)\n",
    "    bic_std = scaler.fit_transform(bic)\n",
    "    rrn_std = scaler.fit_transform(rrn)\n",
    "    AIC_std = scaler.fit_transform(AIC)\n",
    "    MDL_std = scaler.fit_transform(MDL)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.arange(len(lap_std)), lap_std, label='Laplacian')\n",
    "    plt.scatter(np.arange(len(bic_std)), bic_std, label='BIC')\n",
    "    plt.scatter(np.arange(len(rrn_std)), rrn_std, label='RRN')\n",
    "    plt.scatter(np.arange(len(AIC_std)), AIC_std, label='AIC')\n",
    "    plt.scatter(np.arange(len(MDL_std)), MDL_std, label='MDL')\n",
    "    \n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Standardized Value')\n",
    "    plt.legend()\n",
    "    plt.title('Scatter Plot of Standardized Eigenvalues and Model Order Selection Values')\n",
    "    plt.show()\n",
    "   \n",
    "    return np.argmax(rrn_std)+1\n",
    "\n",
    "def get_n_and_some(data):\n",
    "    # Check the shape of the data and determine the axis for mean subtraction\n",
    "\n",
    "    # Move data to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gpu = data.to(device, dtype=torch.float32)\n",
    "    groupN = data_gpu.shape[1] - 1\n",
    "\n",
    "    # Subtract the mean along the specified axis\n",
    "    data_centered = data_gpu - torch.mean(data_gpu, dim=1, keepdim=True)\n",
    "    del data_gpu  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    # Perform SVD decomposition\n",
    "    _, d, v = torch.svd(data_centered)\n",
    "    del data_centered  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert singular values to eigenvalues\n",
    "    e = (d ** 2) / groupN\n",
    "\n",
    "    # Move eigenvalues to CPU and convert to NumPy array\n",
    "    e_np = e.cpu().numpy()\n",
    "    del e, d  # Free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Determine the number of components\n",
    "    n_components = torch.tensor(call_pca_dim(eigs=e_np, N=groupN),device=device,dtype=torch.int32)\n",
    "\n",
    "    return n_components, v.T\n",
    "\n",
    "def PPCA(data, filters=None, threshold=1.6, niters=10, n=-1):\n",
    "    n_components = -1\n",
    "    n_prev = -2\n",
    "    i = 0\n",
    "\n",
    "    # Move data to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gpu = torch.tensor(data,device=device, dtype=torch.float32)\n",
    "\n",
    "    while n_components != n_prev and i < niters:\n",
    "        n_prev = n_components\n",
    "        if filters is not None:\n",
    "            basis_gpu =  torch.tensor(filters.T,device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            n_components, vt = get_n_and_some(data_gpu)\n",
    "            if n <= 0:\n",
    "                basis_gpu = vt[:n_components, :]\n",
    "            else:\n",
    "                print(n)\n",
    "                basis_gpu = vt[:n, :]\n",
    "            del vt\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(n_prev, n_components)\n",
    "\n",
    "        # Estimate noise and residual standard deviation\n",
    "        est_noise = data_gpu - (data_gpu @ torch.linalg.pinv(basis_gpu)) @ basis_gpu\n",
    "        est_residual_std = torch.std(est_noise,dim=0,correction=torch.linalg.matrix_rank(basis_gpu))\n",
    "        del est_noise\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Normalize the data\n",
    "        data_gpu = (data_gpu / est_residual_std)\n",
    "        i += 1\n",
    "\n",
    "    data = data_gpu.cpu().numpy()\n",
    "    basis = basis_gpu.cpu().numpy()\n",
    "    # del data_gpu, basis_gpu, est_residual_std\n",
    "    del data_gpu, basis_gpu\n",
    "    torch.cuda.empty_cache()\n",
    "    return data, basis\n",
    "\n",
    "subs_data_VN, vt = PPCA(reducedsubs.copy(), threshold=0.0, niters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns are samples i.e. XXT is the covariance matrix formed\n",
    "def whiten(X,n_components, method=\"SVD\", visualize=False):\n",
    "    # -1 to account for demean\n",
    "    n_samples = X.shape[-1]-1\n",
    "    X_mean = X.mean(axis=-1)\n",
    "    X -= X_mean[:, np.newaxis]\n",
    "\n",
    "    if method == \"SVD\":\n",
    "        u, d = svd(X, full_matrices=False, check_finite=False)[:2]\n",
    "        # Give consistent eigenvectors for both svd solvers\n",
    "        # u *= np.sign(u[0])\n",
    "        K = (u / d).T[:n_components]  # see (6.33) p.140\n",
    "        del u, d\n",
    "        whitening_matrix = np.sqrt(n_samples)*K\n",
    "    elif method == \"Cholesky\":\n",
    "    # Does not Orthogonalize, just has unit covariance\n",
    "        # Step 2: Perform Cholesky decomposition\n",
    "        L = np.linalg.cholesky(np.cov(X,ddof=1))\n",
    "        # Step 3:\n",
    "        whitening_matrix = np.linalg.inv(L)\n",
    "    elif method == \"InvCov\":\n",
    "        # Calculate the covariance matrix of the centered data\n",
    "        cov_matrix = np.cov(X)\n",
    "        # Perform eigenvalue decomposition of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "        # Calculate the whitening matrix\n",
    "        D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n",
    "        whitening_matrix = eigvecs @ D_inv_sqrt @ eigvecs.T\n",
    "   \n",
    "    whitened_data = whitening_matrix@X\n",
    "\n",
    "    return whitened_data, whitening_matrix\n",
    "\n",
    "# Combine Basis\n",
    "combined_spatial = np.vstack((vt,filters.T))\n",
    "\n",
    "# Whiten\n",
    "whitened_basis, whitening_matrix_pre = whiten(combined_spatial,n_components=combined_spatial.shape[0],method=\"InvCov\",visualize=True)\n",
    "subs_data_com_VN, _ = PPCA(reducedsubs_combined.copy(), filters=whitened_basis.T, threshold=0.0, niters=1)\n",
    "\n",
    "# tempbasis = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))@subs_data_com_VN\n",
    "# whitened_basis, _ = whiten(tempbasis,n_components=tempbasis.shape[0],method=\"InvCov\",visualize=True)\n",
    "\n",
    "# for i in range(0,3):\n",
    "#     # Readjust the MiGP data based on the new basis\n",
    "#     subs_data_com_VN, _ = PPCA(subs_data_com_VN.copy(), filters=whitened_basis.T, threshold=0.0, niters=1)\n",
    "\n",
    "#     # Recalculate the basis via Haufe transform based on adjusted MIGP data\n",
    "#     tempbasis = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))@subs_data_com_VN\n",
    "\n",
    "#     # Rewhiten the basis\n",
    "#     whitened_basis, whitening_matrix = whiten(tempbasis,n_components=combined_spatial.shape[0],method=\"InvCov\",visualize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scree(data, n_components=None):\n",
    "    \"\"\"\n",
    "    Perform PCA on the provided dataset and plot a scree plot.\n",
    "\n",
    "    Parameters:\n",
    "    - data: np.array or pd.DataFrame, the dataset to perform PCA on.\n",
    "    - n_components: int or None, the number of principal components to compute. \n",
    "                    If None, all components are computed.\n",
    "\n",
    "    Returns:\n",
    "    - pca: PCA object after fitting to the data.\n",
    "    \"\"\"    \n",
    "    # # Standardize the data\n",
    "    # # Initialize PCA\n",
    "    # pca = PCA()\n",
    "    \n",
    "    # # Fit PCA on the data\n",
    "    # pca.fit(data)\n",
    "    \n",
    "    # # Calculate explained variance ratio\n",
    "    # explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    _, S, _ = np.linalg.svd(data, full_matrices=False)\n",
    "    e = (S ** 2) / (data.shape[-1]-1)\n",
    "    # Create the scree plot\n",
    "    print(e)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(S) + 1), e, marker='o', linestyle='--')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.xticks(range(1, len(S) + 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "Atemp = np.linalg.pinv(subs_data_com_VN@np.linalg.pinv(whitened_basis))\n",
    "plot_scree(Atemp@subs_data_com_VN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICA(data,whitened_data):\n",
    "    ica = FastICA(whiten=False)\n",
    "    # Takes in array-like of shape (n_samples, n_features) and returns ndarray of shape (n_samples, n_components)\n",
    "    IFA_components = ica.fit_transform(whitened_data.T).T\n",
    "    A = data@np.linalg.pinv(IFA_components)\n",
    "    W = np.linalg.pinv(A)\n",
    "    print(\"The combined unmixing matrix correctly calculates the components: \", np.allclose(W@data, IFA_components))\n",
    "    print(\"The combined mixing matrix correctly reconstructs the low rank data_demean: \", np.allclose(A@IFA_components, A@(W@data)))\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    # Heat map for the combined unmixing matrix\n",
    "    sns.heatmap(W@data, cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Combined Unmixing Matrix (W @ data)')\n",
    "    axes[0].set_xlabel('Components')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "\n",
    "    # Heat map for the IFA components\n",
    "    sns.heatmap(IFA_components, cmap='viridis', ax=axes[1])\n",
    "    axes[1].set_title('IFA Components')\n",
    "    axes[1].set_xlabel('Components')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return IFA_components, A, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_components_combined, A_combined, W_combined = ICA(subs_data_com_VN,whitened_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtwhiten,_ = whiten(vt,n_components=vt.shape[0],method=\"SVD\")\n",
    "subs_data_VN, _ = PPCA(reducedsubs_combined.copy(), filters=vtwhiten.T, threshold=0.0, niters=1)\n",
    "raw_components_major, A_major, W_major = ICA(subs_data_VN,vtwhiten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_data_VN_more, vtmore = PPCA(reducedsubs.copy(), threshold=0.0, niters=1,n=vt.shape[0]+filters.shape[1])\n",
    "vtmorewhiten,_ = whiten(vtmore,n_components=vtmore.shape[0],method=\"SVD\")\n",
    "subs_data_VN_more, _ = PPCA(reducedsubs_combined.copy(), filters=vtmorewhiten.T, threshold=0.0, niters=1)\n",
    "\n",
    "raw_components_major_more, A_major_more, W_major_more = ICA(subs_data_VN_more,vtmorewhiten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_projection(W,data, visualize=True):\n",
    "\n",
    "    Signals = np.linalg.pinv(W)@(W@data)\n",
    "    Residuals = data - Signals\n",
    "    residual_std = np.std(Residuals,axis=0,ddof=np.linalg.matrix_rank(W))\n",
    "    # Trace of I-pinv(W)(W) is equal to the nullity (n-m gvien n > m) of the reconstructed matrix \n",
    "    # trace = data.shape[0] - np.linalg.matrix_rank(W)\n",
    "    # residual_std2 = (np.einsum('ij,ij->j', Residuals, Residuals)/(trace))**.5\n",
    "\n",
    "\n",
    "    if visualize:\n",
    "        n=1000\n",
    "        plt.figure()\n",
    "        plt.plot(Signals[:n,0:1])\n",
    "        plt.plot(Residuals[:n,0:1])\n",
    "        # plt.plot(data[:n,0:1])\n",
    "        # plt.plot(data[:n,0:1] - (Signals[:n,0:1]+Residuals[:n,0:1]))\n",
    "        plt.legend(['Signal','Noise', 'Data' ,'Reconstruction Error'])\n",
    "        plt.title(\"Calculations based on pinv(W)W Projection Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(range(0,residual_std.shape[0]), residual_std)\n",
    "        plt.title(\"Noise std Per Voxel based on pinv(W)W Projection Matrix\")\n",
    "        plt.show()\n",
    "    return residual_std\n",
    "\n",
    "\n",
    "def threshold_and_visualize(data, W, components,visualize=False):\n",
    "    \n",
    "    voxel_noise = noise_projection(W,data)[:, np.newaxis]\n",
    "    z_scores_array = np.zeros_like(components)\n",
    "    z_scores = np.zeros_like(components)\n",
    "\n",
    "    # Process each filter individually\n",
    "    for i in range(components.shape[1]):\n",
    "        z_score = ((components[:, i:i+1]))/voxel_noise\n",
    "        # P(Z < -z \\text{ or } Z > z) = (1 - \\text{CDF}(z)) + (1 - \\text{CDF}(z)) = 2 \\times (1 - \\text{CDF}(z))\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(z_score)))\n",
    "        # Apply multiple comparisons correction for the current filter https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "        reject, pvals_corrected, _, _ = multipletests(p_values.flatten(), alpha=0.05, method='fdr_bh')\n",
    "        masked_comp = z_score*(reject[:,np.newaxis])\n",
    "        # print(masked_comp, reject[:,np.newaxis],z_score)\n",
    "        z_scores_array[:, i:i+1] = masked_comp        \n",
    "        z_scores[:,i:i+1] = z_score\n",
    "\n",
    "       # Skip the iteration if there are no significant values\n",
    "        if not np.any(reject) and visualize:\n",
    "            print(f'Component {i} did not contain any significant values')\n",
    "            plt.figure()\n",
    "            plt.hist(z_score, bins=30, color='blue', alpha=0.7)\n",
    "            plt.title(f\"Histogram for Filter {i} NO SIGNIFICANT VALUES\")\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "        else:\n",
    "            if visualize:\n",
    "                # Create a figure and axes for subplots (1 row of 2 plots per filter)\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "                ax_hist1 = axes[0]\n",
    "                ax_img = axes[1]\n",
    "\n",
    "                # Plot the histogram of the current filter\n",
    "                ax_hist1.hist(z_score, bins=30, color='blue', alpha=0.7)\n",
    "                ax_hist1.set_title(f\"Histogram for Filter {i}\")\n",
    "                ax_hist1.set_xlabel('Value')\n",
    "                ax_hist1.set_ylabel('Frequency')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    # Heat map for the combined unmixing matrix\n",
    "    sns.heatmap(z_scores, cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('z_score')\n",
    "    axes[0].set_xlabel('Components')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "\n",
    "    # Heat map for the IFA components\n",
    "    sns.heatmap(z_scores_array, cmap='viridis', ax=axes[1])\n",
    "    axes[1].set_title('z_score thresh')\n",
    "    axes[1].set_xlabel('Components')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return z_scores, z_scores_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh, z_scores_thresh = threshold_and_visualize(subs_data_com_VN, W_combined, raw_components_combined.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh_major, z_scores_thresh_major = threshold_and_visualize(subs_data_VN, W_major, raw_components_major.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_unthresh_major_more, z_scores_thresh_major_more = threshold_and_visualize(subs_data_VN_more, W_major_more, raw_components_major_more.T, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(z_scores_thresh_major_more[:,0]), threshold=np.percentile(np.abs(z_scores_thresh_major_more[:,0]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and Save Netmats + Dual Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00115/full\n",
    "\n",
    "def calculate_netmat_and_spatial_map(Xn, z_maps):\n",
    "    \"\"\"\n",
    "    Calculate the network matrix (netmat) and spatial map for a given subject and z_maps.\n",
    "    \n",
    "    Parameters:\n",
    "    Xn (array): Time x Grayordinates normalized data matrix (Time x V)\n",
    "    z_maps (array): Grayordinates x Components map (V x C)\n",
    "\n",
    "    Returns:\n",
    "    netmat (array): Components x Components network matrix (C x C)\n",
    "    spatial_map (array): Components x Grayordinates matrix (C x V)\n",
    "    \"\"\"\n",
    "    # Time x Components\n",
    "    # Demean the regressors (z_maps)\n",
    "    z_maps_demeaned = z_maps - z_maps.mean(axis=0, keepdims=True)  # Demean the columns of z_maps (V x C)\n",
    "    \n",
    "    # Time x Components\n",
    "    A = (Xn @ np.linalg.pinv(z_maps_demeaned.T))  # A is Time x Components (T x C)\n",
    "   \n",
    "    \n",
    "    # Normalized Time x Components matrix\n",
    "    An = hcp.normalize(A)  # An is Time x Components (T x C)\n",
    "    del A\n",
    "\n",
    "    # Components x Components network matrix\n",
    "    netmat = (An.T @ An) / (Xn.shape[0] - 1)  # Netmat is Components x Components (C x C)\n",
    "\n",
    "    # Components x Grayordinates spatial map\n",
    "    spatial_map = np.linalg.pinv(An) @ Xn  # Spatial map is Components x Grayordinates (C x V)\n",
    "\n",
    "    return An, netmat, spatial_map\n",
    "\n",
    "def dual_regress_sub(sub_path, z_maps_1, z_maps_2):\n",
    "    try:\n",
    "        concatenated_data = []\n",
    "        for task in sub_path:\n",
    "            # Load and preprocess each task\n",
    "            X = nib.load(task).get_fdata(dtype=np.float32)  # Grayordinates x Time (V x T)\n",
    "            Xn = hcp.normalize(X - X.mean(axis=1, keepdims=True))  # Normalizing (V x T)\n",
    "            concatenated_data.append(Xn)\n",
    "            del X, Xn\n",
    "        \n",
    "        # Concatenate data along the first axis (all tasks into one big matrix)\n",
    "        subject = np.concatenate(concatenated_data, axis=0)  # Time x Grayordinates (T x V)\n",
    "        del concatenated_data\n",
    "        \n",
    "        # Normalize the concatenated data\n",
    "        Xn = hcp.normalize(subject - subject.mean(axis=1,keepdims=True))  # Time x Grayordinates normalized data (T x V)\n",
    "        del subject\n",
    "        \n",
    "        # Calculate netmat and spatial map for the first set of z_maps\n",
    "        An_1, netmat_1, spatial_map_1 = calculate_netmat_and_spatial_map(Xn, z_maps_1)\n",
    "\n",
    "        # Calculate netmat and spatial map for the second set of z_maps\n",
    "        An_2, netmat_2, spatial_map_2 = calculate_netmat_and_spatial_map(Xn, z_maps_2)\n",
    "\n",
    "        return (An_1, netmat_1, spatial_map_1), (An_2, netmat_2, spatial_map_2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def dual_regress(group_paths, z_maps_1, z_maps_2):\n",
    "    # Use partial to avoid duplicating z_maps in memory\n",
    "    with ProcessPoolExecutor(max_workers=int(os.cpu_count() * 0.7)) as executor:\n",
    "        # Create a partial function that \"binds\" the z_maps_1 and z_maps_2 without duplicating them\n",
    "        partial_func = partial(dual_regress_sub, z_maps_1=z_maps_1, z_maps_2=z_maps_2)\n",
    "\n",
    "        # Pass the subject paths to the executor without copying z_maps\n",
    "        results = list(executor.map(partial_func, group_paths))\n",
    "        \n",
    "        # Separate the results for the two bases, collecting An, netmat, and spatial_map\n",
    "        An_1, netmats_1, spatial_maps_1 = zip(*[(res[0][0], res[0][1], res[0][2]) for res in results if res[0] is not None])\n",
    "        An_2, netmats_2, spatial_maps_2 = zip(*[(res[1][0], res[1][1], res[1][2]) for res in results if res[1] is not None])\n",
    "\n",
    "        return (np.array(An_1), np.array(netmats_1), np.array(spatial_maps_1)), (np.array(An_2), np.array(netmats_2), np.array(spatial_maps_2))\n",
    "\n",
    "# Save function for An, netmats, and spatial maps\n",
    "def save_numpy_arrays(output_prefix, An_1, netmats_1, spatial_maps_1, An_2, netmats_2, spatial_maps_2):\n",
    "    \"\"\"\n",
    "    Saves the An arrays, netmats, and spatial maps to disk using np.save.\n",
    "    \n",
    "    Parameters:\n",
    "    output_prefix (str): Prefix for the output files.\n",
    "    An_1 (np.array): Time x Components matrix for z_maps_1.\n",
    "    netmats_1 (np.array): Network matrices for z_maps_1.\n",
    "    spatial_maps_1 (np.array): Spatial maps for z_maps_1.\n",
    "    An_2 (np.array): Time x Components matrix for z_maps_2.\n",
    "    netmats_2 (np.array): Network matrices for z_maps_2.\n",
    "    spatial_maps_2 (np.array): Spatial maps for z_maps_2.\n",
    "    \"\"\"\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_An_1.npy\", An_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_netmats_1.npy\", netmats_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_spatial_maps_1.npy\", spatial_maps_1)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_An_2.npy\", An_2)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_netmats_2.npy\", netmats_2)\n",
    "    save_array_to_outputfolder(f\"{output_prefix}_spatial_maps_2.npy\", spatial_maps_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group A - Training Set\n",
    "(groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train), (groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train) = dual_regress(groupA_train_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupA_train\", groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train, groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group A - Test Set\n",
    "(groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test), (groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test) = dual_regress(groupA_test_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupA_test\", groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test, groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group B - Training Set\n",
    "(groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train), (groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train) = dual_regress(groupB_train_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupB_train\", groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train, groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Group B - Test Set\n",
    "(groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test), (groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test) = dual_regress(groupB_test_paths, z_scores_unthresh, z_scores_unthresh_major_more)\n",
    "save_numpy_arrays(\"groupB_test\", groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test, groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Netmats + Dual Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load function for An, netmats, and spatial maps\n",
    "def load_numpy_arrays(input_prefix):\n",
    "    \"\"\"\n",
    "    Loads the An arrays, netmats, and spatial maps from disk using load_array_from_outputfolder.\n",
    "    \n",
    "    Parameters:\n",
    "    input_prefix (str): Prefix for the input files.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Six numpy arrays (An_1, netmats_1, spatial_maps_1, An_2, netmats_2, spatial_maps_2).\n",
    "    \"\"\"\n",
    "    An_1 = load_array_from_outputfolder(f\"{input_prefix}_An_1.npy\")\n",
    "    netmats_1 = load_array_from_outputfolder(f\"{input_prefix}_netmats_1.npy\")\n",
    "    spatial_maps_1 = load_array_from_outputfolder(f\"{input_prefix}_spatial_maps_1.npy\")\n",
    "    An_2 = load_array_from_outputfolder(f\"{input_prefix}_An_2.npy\")\n",
    "    netmats_2 = load_array_from_outputfolder(f\"{input_prefix}_netmats_2.npy\")\n",
    "    spatial_maps_2 = load_array_from_outputfolder(f\"{input_prefix}_spatial_maps_2.npy\")\n",
    "    \n",
    "    return An_1, netmats_1, spatial_maps_1, An_2, netmats_2, spatial_maps_2\n",
    "\n",
    "# Example usage for loading Group A train and test results\n",
    "groupA_An_1_train, groupA_netmats_1_train, groupA_spatial_maps_1_train, groupA_An_2_train, groupA_netmats_2_train, groupA_spatial_maps_2_train = load_numpy_arrays(\"groupA_train\")\n",
    "groupA_An_1_test, groupA_netmats_1_test, groupA_spatial_maps_1_test, groupA_An_2_test, groupA_netmats_2_test, groupA_spatial_maps_2_test = load_numpy_arrays(\"groupA_test\")\n",
    "\n",
    "# Example usage for loading Group B train and test results\n",
    "groupB_An_1_train, groupB_netmats_1_train, groupB_spatial_maps_1_train, groupB_An_2_train, groupB_netmats_2_train, groupB_spatial_maps_2_train = load_numpy_arrays(\"groupB_train\")\n",
    "groupB_An_1_test, groupB_netmats_1_test, groupB_spatial_maps_1_test, groupB_An_2_test, groupB_netmats_2_test, groupB_spatial_maps_2_test = load_numpy_arrays(\"groupB_test\")\n",
    "\n",
    "# Sanity check for Group A train data\n",
    "print(\"Group A Train:\")\n",
    "print(groupA_An_1_train.shape, groupA_netmats_1_train.shape, groupA_spatial_maps_1_train.shape)\n",
    "print(groupA_An_2_train.shape, groupA_netmats_2_train.shape, groupA_spatial_maps_2_train.shape)\n",
    "\n",
    "# Sanity check for Group A test data\n",
    "print(\"Group A Test:\")\n",
    "print(groupA_An_1_test.shape, groupA_netmats_1_test.shape, groupA_spatial_maps_1_test.shape)\n",
    "print(groupA_An_2_test.shape, groupA_netmats_2_test.shape, groupA_spatial_maps_2_test.shape)\n",
    "\n",
    "# Sanity check for Group B train data\n",
    "print(\"Group B Train:\")\n",
    "print(groupB_An_1_train.shape, groupB_netmats_1_train.shape, groupB_spatial_maps_1_train.shape)\n",
    "print(groupB_An_2_train.shape, groupB_netmats_2_train.shape, groupB_spatial_maps_2_train.shape)\n",
    "\n",
    "# Sanity check for Group B test data\n",
    "print(\"Group B Test:\")\n",
    "print(groupB_An_1_test.shape, groupB_netmats_1_test.shape, groupB_spatial_maps_1_test.shape)\n",
    "print(groupB_An_2_test.shape, groupB_netmats_2_test.shape, groupB_spatial_maps_2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Discriminant Information via Netmats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netmat(group_data,basis):\n",
    "    A = ((group_data@np.linalg.pinv(basis.T)))\n",
    "    # Normalized Time x Components matrix\n",
    "    An = hcp.normalize(A)  # An is Time x Components (T x C)\n",
    "    del A\n",
    "\n",
    "    timepoints = An.shape[0]\n",
    "    group_netmat = (An.T@An)/(timepoints-1)\n",
    "    return group_netmat\n",
    "\n",
    "def group_dist(group_data1,group_data2,basis,metric=\"riemann\"):\n",
    "    netmat1 = netmat(group_data1,basis)\n",
    "    netmat2 = netmat(group_data2,basis)\n",
    "    print(\"Distance between Group Netmats:\", distance(netmat1,netmat2,metric=metric))\n",
    "\n",
    "group_dist(reducedsubsA_loaded,reducedsubsB_loaded,z_scores_unthresh_major,metric=metric)\n",
    "group_dist(reducedsubsA_loaded,reducedsubsB_loaded,z_scores_unthresh_major_more,metric=metric)\n",
    "group_dist(reducedsubsA_loaded,reducedsubsB_loaded,z_scores_unthresh,metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyriemann.classification import TSclassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5662067\n",
    "def var_diff(group1_red, group2_red, group1_covs_red, group2_covs_red, metric):\n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    # Split the reduced data and covariances consistently using indices\n",
    "    n_group1 = group1_red.shape[0]\n",
    "    n_group2 = group2_red.shape[0]\n",
    "\n",
    "    # Generate indices for splitting\n",
    "    group1_indices = np.arange(n_group1)\n",
    "    group2_indices = np.arange(n_group2)\n",
    "\n",
    "    # Perform a consistent split on the indices\n",
    "    group1_train_idx, group1_test_idx = train_test_split(group1_indices, test_size=0.3, random_state=42)\n",
    "    group2_train_idx, group2_test_idx = train_test_split(group2_indices, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Apply the split to both the reduced data and the covariances using the indices\n",
    "    group1_train = group1_red[group1_train_idx, :]\n",
    "    group1_test = group1_red[group1_test_idx, :]\n",
    "    group1_cov_train = group1_covs_red[group1_train_idx, :, :]\n",
    "    group1_cov_test = group1_covs_red[group1_test_idx, :, :]\n",
    "\n",
    "    group2_train = group2_red[group2_train_idx, :]\n",
    "    group2_test = group2_red[group2_test_idx, :]\n",
    "    group2_cov_train = group2_covs_red[group2_train_idx, :, :]\n",
    "    group2_cov_test = group2_covs_red[group2_test_idx, :, :]\n",
    "\n",
    "    # Compute the mean covariances using the training data only\n",
    "    group1_mean = mean_covariance(group1_cov_train, metric=metric)\n",
    "    group2_mean = mean_covariance(group2_cov_train, metric=metric)\n",
    "\n",
    "    # Continue with the rest of the logic...\n",
    "    _, feature_all = eigh(group1_mean, group2_mean + group2_mean, eigvals_only=False)\n",
    "\n",
    "    # Initialize list to store results (accuracy and distance)\n",
    "    results = []\n",
    "\n",
    "    # Loop from n=1 to n=15 for selecting top and bottom eigenvectors\n",
    "    for n in range(1, 15):\n",
    "        # Perform eigen decomposition based on top and bottom n eigenvectors\n",
    "        features = np.hstack([feature_all[:, :n], feature_all[:, -n:]])  # Select top and bottom n eigenvectors\n",
    "        group1_train_transformed = group1_train @ features\n",
    "        group2_train_transformed = group2_train @ features\n",
    "        group1_test_transformed = group1_test @ features\n",
    "        group2_test_transformed = group2_test @ features\n",
    "\n",
    "        # Calculate log variance for both groups\n",
    "        group1_train_logvar = np.log(np.var(group1_train_transformed, axis=1))\n",
    "        group2_train_logvar = np.log(np.var(group2_train_transformed, axis=1))\n",
    "        group1_test_logvar = np.log(np.var(group1_test_transformed, axis=1))\n",
    "        group2_test_logvar = np.log(np.var(group2_test_transformed, axis=1))\n",
    "\n",
    "        # Prepare the dataset for classification\n",
    "        X_train = np.vstack([group1_train_logvar, group2_train_logvar])\n",
    "        y_train = np.hstack([np.zeros(group1_train_logvar.shape[0]), np.ones(group2_train_logvar.shape[0])])\n",
    "        X_test = np.vstack([group1_test_logvar, group2_test_logvar])\n",
    "        y_test = np.hstack([np.zeros(group1_test_logvar.shape[0]), np.ones(group2_test_logvar.shape[0])])\n",
    "\n",
    "        # Train logistic regression classifier on training data\n",
    "        clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test data and calculate accuracy\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Calculate class means for distance (using the training data)\n",
    "        mean_group1_train = np.mean(group1_train_logvar, axis=0)\n",
    "        mean_group2_train = np.mean(group2_train_logvar, axis=0)\n",
    "\n",
    "        # Calculate the distance between the two class means\n",
    "        distance_vars = np.linalg.norm(mean_group1_train - mean_group2_train)\n",
    "\n",
    "        # Store accuracy and Riemannian distance for this n\n",
    "        results.append((n, distance_vars, accuracy))\n",
    "\n",
    "        # Plot when n=1\n",
    "        if n == 1:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(group1_test_logvar[:, 0], group1_test_logvar[:, 1], label='Group 1 Log Variance (Test)', color='blue')\n",
    "            plt.scatter(group2_test_logvar[:, 0], group2_test_logvar[:, 1], label='Group 2 Log Variance (Test)', color='red')\n",
    "\n",
    "            # Plot the line connecting the two means\n",
    "            plt.plot([mean_group1_train[0], mean_group2_train[0]], [mean_group1_train[1], mean_group2_train[1]], 'k--', label=f'Mean Distance: {distance_vars:.2f}')\n",
    "\n",
    "            # Decision boundary\n",
    "            x_values = np.array([X_train[:, 0].min(), X_train[:, 0].max()])\n",
    "            y_values = -(clf.intercept_ + clf.coef_[0][0] * x_values) / clf.coef_[0][1]\n",
    "            plt.plot(x_values, y_values, 'g-', label='Decision Boundary')\n",
    "\n",
    "            # Display plot\n",
    "            plt.xlabel('Log Variance Feature 1')\n",
    "            plt.ylabel('Log Variance Feature 2')\n",
    "            plt.title('Log Variance Comparison and Logistic Regression Decision Boundary')\n",
    "\n",
    "            # Display classification accuracy on the plot\n",
    "            plt.text(0.05, 0.95, f'Accuracy: {accuracy:.2f}', transform=plt.gca().transAxes, fontsize=12,\n",
    "                     verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='lightgrey'))\n",
    "\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    # Return the list of accuracies and distances for each n\n",
    "    return results\n",
    "\n",
    "def mean_diff(group1_covs_red,group2_covs_red,metric):\n",
    "    group_1 = mean_covariance(group1_covs_red, metric=metric)\n",
    "    group_2 = mean_covariance(group2_covs_red, metric=metric)\n",
    "    return distance(group_1,group_2,metric=metric)\n",
    "\n",
    "def tangent_class_test(group1_covs_red, group2_covs_red, metric):\n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    # Generate indices for splitting\n",
    "    n_group1 = group1_covs_red.shape[0]\n",
    "    n_group2 = group2_covs_red.shape[0]\n",
    "    \n",
    "    group1_indices = np.arange(n_group1)\n",
    "    group2_indices = np.arange(n_group2)\n",
    "    \n",
    "    # Perform consistent split on the indices\n",
    "    group1_train_idx, group1_test_idx = train_test_split(group1_indices, test_size=0.35, random_state=42)\n",
    "    group2_train_idx, group2_test_idx = train_test_split(group2_indices, test_size=0.35, random_state=42)\n",
    "\n",
    "    # Apply the split to the covariances using the indices\n",
    "    group1_cov_train = group1_covs_red[group1_train_idx, :, :]\n",
    "    group1_cov_test = group1_covs_red[group1_test_idx, :, :]\n",
    "    group2_cov_train = group2_covs_red[group2_train_idx, :, :]\n",
    "    group2_cov_test = group2_covs_red[group2_test_idx, :, :]\n",
    "\n",
    "    # Compute the mean covariance using ONLY the training data\n",
    "    group_Mean = mean_covariance(np.concatenate((group1_cov_train, group2_cov_train)), metric=metric)\n",
    "\n",
    "    # Project the training covariances into the tangent space\n",
    "    tangent_projected_1_train = tangent_space(group1_cov_train, group_Mean, metric=metric)\n",
    "    tangent_projected_2_train = tangent_space(group2_cov_train, group_Mean, metric=metric)\n",
    "\n",
    "    # Project the test covariances into the tangent space\n",
    "    tangent_projected_1_test = tangent_space(group1_cov_test, group_Mean, metric=metric)\n",
    "    tangent_projected_2_test = tangent_space(group2_cov_test, group_Mean, metric=metric)\n",
    "\n",
    "    # Combine the tangent projections for training and testing\n",
    "    X_train = np.vstack((tangent_projected_1_train, tangent_projected_2_train))\n",
    "    X_test = np.vstack((tangent_projected_1_test, tangent_projected_2_test))\n",
    "    y_train = np.hstack((np.zeros(tangent_projected_1_train.shape[0]), np.ones(tangent_projected_2_train.shape[0])))\n",
    "    y_test = np.hstack((np.zeros(tangent_projected_1_test.shape[0]), np.ones(tangent_projected_2_test.shape[0])))\n",
    "\n",
    "    # Set up dimensionality reduction\n",
    "    max_dim = np.min((X_train.shape[0], X_train.shape[1]))\n",
    "    dims = [2, 3,  int((max_dim-1)/20), int((max_dim-1)/17), int((max_dim-1)/15),\n",
    "            int((max_dim-1)/13), int((max_dim-1)/12), int((max_dim-1)/10), \n",
    "            int((max_dim-1)/7), int((max_dim-1)/5), int((max_dim-1)/3), \n",
    "            int((max_dim-1)/2), int((max_dim-1)/1.7), int((max_dim-1)/1.5), \n",
    "            int((max_dim-1)/1.3), int((max_dim-1)/1.1), max_dim-1]\n",
    "\n",
    "    logistic_accuracies = []\n",
    "\n",
    "    for i in dims:\n",
    "        # Reduce dimensionality using PCA\n",
    "        pca = PCA(n_components=i)\n",
    "        X_train_reduced = pca.fit_transform(X_train)\n",
    "        X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "        # Train logistic regression classifier\n",
    "        clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "        # Test accuracy\n",
    "        y_pred = clf.predict(X_test_reduced)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        logistic_accuracies.append(test_accuracy)\n",
    "\n",
    "    return dims, logistic_accuracies\n",
    "   \n",
    "def PSD_diff_all(group1_red, group1_covs_red, group2_red, group2_covs_red,metric=None):\n",
    "\n",
    "    # group1_covs_red = cov_est.transform(np.transpose(group1_red, (0, 2, 1)))\n",
    "    # group2_covs_red = cov_est.transform(np.transpose(group2_red, (0, 2, 1)))\n",
    "    psd_mean_distance = mean_diff(group1_covs_red, group2_covs_red, metric)\n",
    "    dims, logistic_accuracies = tangent_class_test(group1_covs_red, group2_covs_red, metric)\n",
    "    fkt_results = var_diff(group1_red, group2_red, group1_covs_red, group2_covs_red, metric)\n",
    "    # plt.scatter(dims,logistic_accuracies)\n",
    "    # plt.xlabel(\"Dimension\")\n",
    "    # plt.ylabel(\"Logistic Regression Accuracy\")\n",
    "    # plt.title(\"Logistic Regression Accuracy (65/35 Train Test Splot) on Data Reduced via PCA\")\n",
    "\n",
    "    # plt.scatter(dims,logistic_accuracies)\n",
    "    # plt.xlabel(\"Dimension\")\n",
    "    # plt.ylabel(\"Logistic Regression Accuracy\")\n",
    "    # plt.title(\"Logistic Regression Accuracy (65/35 Train Test Splot) on Data Reduced via PCA\")\n",
    "\n",
    "\n",
    "    result = {\n",
    "        \"psd_mean_distance\": psd_mean_distance,\n",
    "        \"dims\": dims,\n",
    "        \"logistic_accuracies\": logistic_accuracies,\n",
    "        \"var_diff_n_distance_accuracy\": fkt_results\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFA_result = PSD_diff_all(groupA_An_1, groupA_netmats_1, groupB_An_1, groupB_netmats_1, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_result = PSD_diff_all(groupA_An_2, groupA_netmats_2, groupB_An_2, groupB_netmats_2, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(IFA_result[\"dims\"],IFA_result[\"logistic_accuracies\"],label='IFA Components')\n",
    "plt.scatter(major_result[\"dims\"],major_result[\"logistic_accuracies\"],label='ICA Components')\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Logistic Regression Accuracy\")\n",
    "plt.title(\"Logistic Regression Accuracy (65/35 Train Test Splot) on Tangent Netmats Reduced via PCA\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter([tup[0]*2 for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],[tup[2] for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],label='IFA Components')\n",
    "plt.scatter([tup[0]*2 for tup in major_result[\"var_diff_n_distance_accuracy\"]],[tup[2] for tup in major_result[\"var_diff_n_distance_accuracy\"]],label='ICA Components')\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Logistic Regression Accuracy of Netmats Transformed via FKT Filters\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter([tup[0]*2 for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],[tup[1] for tup in IFA_result[\"var_diff_n_distance_accuracy\"]],label='IFA Components')\n",
    "plt.scatter([tup[0]*2 for tup in major_result[\"var_diff_n_distance_accuracy\"]],[tup[1] for tup in major_result[\"var_diff_n_distance_accuracy\"]],label='ICA Components')\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.title(\"Distance Between Means of Group Netmats Reduced via FKT\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z Scores on Dual Regressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(groupA_spatial_maps_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Assuming group1 and group2 are numpy arrays with shapes:\n",
    "# group1.shape = (subjects1, components, grayordinates)\n",
    "# group2.shape = (subjects2, components, grayordinates)\n",
    "\n",
    "# Define the number of permutations\n",
    "n_permutations = 100  # Adjust based on computational resources\n",
    "\n",
    "# Function to perform t-test for a specific component\n",
    "def perm_ttest_for_component(component_idx, data_group1, data_group2, n_permutations):\n",
    "    # Extract data for this component from both groups\n",
    "    group1_comp_data = data_group1[:, component_idx, :]  # Shape: (subjects1, grayordinates)\n",
    "    group2_comp_data = data_group2[:, component_idx, :]  # Shape: (subjects2, grayordinates)\n",
    "\n",
    "    # Perform the permutation t-test for each grayordinate\n",
    "    t_stat, p_val = ttest_ind(\n",
    "        group1_comp_data, group2_comp_data, axis=0, equal_var=True, nan_policy='omit',\n",
    "        permutations=n_permutations, random_state=None, alternative='two-sided'\n",
    "    )\n",
    "    return t_stat, p_val\n",
    "\n",
    "# Use Parallel to parallelize over components\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(perm_ttest_for_component)(component_idx, groupA_spatial_maps_1, groupB_spatial_maps_1, n_permutations)\n",
    "    for component_idx in range(groupA_spatial_maps_1.shape[1])\n",
    ")\n",
    "\n",
    "# Unpack results\n",
    "t_statistics, p_values = zip(*results)\n",
    "t_statistics = np.array(t_statistics)  # Shape: (components, grayordinates)\n",
    "p_values = np.array(p_values)          # Shape: (components, grayordinates)\n",
    "\n",
    "# Flatten p-values for multiple comparison correction\n",
    "p_values_flat = p_values.flatten()\n",
    "\n",
    "# Apply FDR correction\n",
    "_, p_values_corrected, _, _ = multipletests(p_values_flat, method='fdr_bh')\n",
    "\n",
    "# Reshape the corrected p-values back to the original (components, grayordinates) shape\n",
    "p_values_corrected = p_values_corrected.reshape(p_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupB_spatial_maps_1[10,20,:]), threshold=np.percentile(np.abs(groupB_spatial_maps_1[10,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupB_spatial_maps_1[20,20,:]), threshold=np.percentile(np.abs(groupB_spatial_maps_1[20,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupA_spatial_maps_1[11,20,:]), threshold=np.percentile(np.abs(groupA_spatial_maps_1[11,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(groupA_spatial_maps_1[1,20,:]), threshold=np.percentile(np.abs(groupA_spatial_maps_1[1,20,:]), 95), bg_map=hcp.mesh.sulc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf(hcp.mesh.inflated, hcp.cortex_data(z_scores_unthresh[:,20]), threshold=np.percentile(np.abs(z_scores_unthresh[:,20]), 0), bg_map=hcp.mesh.sulc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
